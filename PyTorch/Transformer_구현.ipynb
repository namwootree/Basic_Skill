{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_구현.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNOzfQHSNeZdRfWpEsdA4eU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namwootree/Basic_Skill/blob/main/PyTorch/Transformer_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "qqUiMXR3MRyO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0q_BaIiLiFr",
        "outputId": "9936bd45-385c-483a-96a7-5e67d4790e21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 57 kB 5.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 26.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 50.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 18.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 44.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install Korpora sentencepiece einops wandb torch-summary -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset\n",
        "from Korpora import Korpora\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from einops import rearrange, reduce, repeat\n",
        "\n",
        "# Tokenizer\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Training\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 시스템\n",
        "import wandb\n",
        "from torch.cuda import amp\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# 기타\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "aUSa4olXMPy-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 32000 + 7\n",
        "SEQ_LEN = 100\n",
        "PAD_IDX = 0\n",
        "\n",
        "# Trainig Set 모집단의 크기\n",
        "TRAINSET_SIZE = 120000\n",
        "\n",
        "# 실제로 사용할 Training Set의 크기. 이 수만큼 전체 Training Set에서 Random Sampling\n",
        "TRAIN_LEN = 100000\n",
        "VALID_LEN = 10000\n",
        "BATCH_SIZE = 2\n",
        "WANDB_SAVED_PATH = ''\n",
        "\n",
        "if 'device' not in globals():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f'Using {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWbNDDjGM4aE",
        "outputId": "7c71b4ff-fce7-4813-f35f-a5992041320d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"10a106cc0c288b92e67a1540a17ddbeb973957b3\"\n",
        "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
        "wandb.init(project=\"Transformer\", entity=\"namwootree\")\n",
        "RUN_PATH = ''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "313o3l6HNAP4",
        "outputId": "0e4d6571-d921-453c-b955-4553ccb6cbc8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.19"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Set"
      ],
      "metadata": {
        "id": "kutmJ-9ORxwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Data"
      ],
      "metadata": {
        "id": "zY5eXHboQDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Korpora.load(\"open_subtitles\", root_dir='./')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3JXuY5KQEiw",
        "outputId": "1efcbbc0-e748-42f8-cf90-0e6b4c2e7968"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : TRAC (https://trac.edgewall.org/)\n",
            "    Repository : http://opus.nlpl.eu/OpenSubtitles-v2018.php\n",
            "    References :\n",
            "        - P. Lison and J. Tiedemann, 2016, OpenSubtitles2016: Extracting Large Parallel Corpora\n",
            "          from Movie and TV Subtitles. In Proceedings of the 10th International Conference on\n",
            "          Language Resources and Evaluation (LREC 2016)\n",
            "\n",
            "    This is a new collection of translated movie subtitles from http://www.opensubtitles.org/.\n",
            "\n",
            "    [[ IMPORTANT ]]\n",
            "    If you use the OpenSubtitle corpus: Please, add a link to http://www.opensubtitles.org/\n",
            "    to your website and to your reports and publications produced with the data!\n",
            "    I promised this when I got the data from the providers of that website!\n",
            "\n",
            "    This is a slightly cleaner version of the subtitle collection using improved sentence alignment\n",
            "    and better language checking.\n",
            "\n",
            "    62 languages, 1,782 bitexts\n",
            "    total number of files: 3,735,070\n",
            "    total number of tokens: 22.10G\n",
            "    total number of sentence fragments: 3.35G\n",
            "\n",
            "    [[ NOTICE ]]\n",
            "    In original data, the source language is `en` and target language is `ko`. However in Korpora,\n",
            "    we change the language pair so that source language is `ko` and target language is `en`.\n",
            "\n",
            "    # License\n",
            "    Open Data. Details in https://opendefinition.org/od/2.1/en/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[open_subtitles] download en-ko.tmx.gz: 48.1MB [00:01, 32.0MB/s]                            \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decompress /content/open_subtitles/en-ko.tmx.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame([corpus.train.pairs, corpus.train.texts], index = ['src', 'trg'])\n",
        "data = data.transpose()\n",
        "data.to_csv('data.txt', index=False)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "tkD-3xYCQFWq",
        "outputId": "3af0a912-c79f-4f99-ed6d-a7c38da871ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 src  \\\n",
              "0  Through the snow and sleet and hail, through t...   \n",
              "1  ever faithful, ever true, nothing stops him, h...   \n",
              "2        Look out for Mr Stork That persevering chap   \n",
              "3     He'll come along and drop a bundle in your lap   \n",
              "4    You may be poor or rich It doesn't matter which   \n",
              "\n",
              "                                                 trg  \n",
              "0  폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...  \n",
              "1               우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!  \n",
              "2                                      황새 아저씨를 기다리세요  \n",
              "3                                     찾아와 선물을 주실 거예요  \n",
              "4                                 가난하든 부자이든 상관이 없답니다  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e77c2683-55f8-4fe5-bb64-a1ce84358d72\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Through the snow and sleet and hail, through t...</td>\n",
              "      <td>폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ever faithful, ever true, nothing stops him, h...</td>\n",
              "      <td>우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Look out for Mr Stork That persevering chap</td>\n",
              "      <td>황새 아저씨를 기다리세요</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>He'll come along and drop a bundle in your lap</td>\n",
              "      <td>찾아와 선물을 주실 거예요</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You may be poor or rich It doesn't matter which</td>\n",
              "      <td>가난하든 부자이든 상관이 없답니다</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e77c2683-55f8-4fe5-bb64-a1ce84358d72')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e77c2683-55f8-4fe5-bb64-a1ce84358d72 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e77c2683-55f8-4fe5-bb64-a1ce84358d72');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Yv_fcn2PQXjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('src.txt', mode = 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(data['src']))\n",
        "with open('trg.txt', mode= 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(data['trg']))"
      ],
      "metadata": {
        "id": "b4uTGH_OQHUu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"src.txt\"\n",
        "prefix = \"src\"\n",
        "vocab_size = VOCAB_SIZE - 7\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +\n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + \n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + \n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" +  \n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" +  \n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" +  \n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\")  \n",
        "\n",
        "corpus = \"trg.txt\"\n",
        "prefix = \"trg\"\n",
        "vocab_size = VOCAB_SIZE - 7\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" +\n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + \n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" +  \n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" +  \n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" +  \n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" +  \n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") "
      ],
      "metadata": {
        "id": "piPxcqFTQelS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def en_encode(tmpstr:str) -> np.array :\n",
        "    tmpstr = np.array(sp_src.EncodeAsIds(tmpstr))\n",
        "\n",
        "    # SEQ_LEN보다 길면 짜른다 \n",
        "    if len(tmpstr) > SEQ_LEN :\n",
        "        tmpstr = tmpstr[:SEQ_LEN]\n",
        "\n",
        "    # SEQ_LEN보다 작으면 padding\n",
        "    else :\n",
        "        tmpstr = np.pad(tmpstr, (0, SEQ_LEN - len(tmpstr)), 'constant', constant_values = sp_src.pad_id())\n",
        "    \n",
        "    return tmpstr"
      ],
      "metadata": {
        "id": "25DlZ_ovQqtw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_src = spm.SentencePieceProcessor()\n",
        "sp_src.Load('src.model')\n",
        "\n",
        "src_data = data['src']\n",
        "\n",
        "src_list = []\n",
        "\n",
        "for idx in tqdm(range(len(src_data))):\n",
        "    src_list.append(en_encode(src_data[idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLx9fpbpQ8xq",
        "outputId": "0f066cd9-f6ce-4a4e-9cfb-881e7bd6502b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1269683/1269683 [02:00<00:00, 10560.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ko_encode(tmpstr: str) -> np.array:\n",
        "    tmpstr = np.array(sp_trg.EncodeAsIds(tmpstr))\n",
        "    tmpstr = np.insert(tmpstr, 0, sp_trg.bos_id())\n",
        "\n",
        "    if len(tmpstr) >= SEQ_LEN:\n",
        "        # SEQ_LEN -1의 길이로 자른다\n",
        "        tmpstr = tmpstr[:SEQ_LEN-1]\n",
        "        # 마지막에 <eos> 토큰을 넣어줌으로써, 길이를 SEQ_LEN으로 맞춘다\n",
        "        tmpstr = np.pad(tmpstr, (0, 1),\n",
        "                        'constant', constant_values=sp_trg.eos_id())\n",
        "\n",
        "\n",
        "    else:\n",
        "        tmpstr = np.pad(tmpstr, (0, 1),\n",
        "                        'constant', constant_values=sp_trg.eos_id())\n",
        "        tmpstr = np.pad(tmpstr, (0, SEQ_LEN - len(tmpstr)),\n",
        "                        'constant', constant_values=sp_trg.pad_id())\n",
        "\n",
        "    return tmpstr"
      ],
      "metadata": {
        "id": "oDoXv8BeRJ1y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_trg = spm.SentencePieceProcessor()\n",
        "sp_trg.Load('trg.model')\n",
        "\n",
        "trg_data = data['trg']\n",
        "\n",
        "trg_list = []\n",
        "\n",
        "for idx in tqdm(range(len(trg_data))):\n",
        "    trg_list.append(ko_encode(trg_data[idx])) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJoRDI7MROye",
        "outputId": "01364afb-96d3-43dd-9b1b-d674943dd5c6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1269683/1269683 [02:28<00:00, 8574.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Data"
      ],
      "metadata": {
        "id": "rKqWUAiPRgOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mask = np.random.choice(len(src_list[:TRAINSET_SIZE]), size = TRAIN_LEN, replace = False)\n",
        "valid_mask = np.random.choice(len(trg_list[TRAINSET_SIZE:]), size = VALID_LEN, replace = False)\n",
        "\n",
        "src_train = np.take(src_list, train_mask, axis = 0)\n",
        "trg_train = np.take(trg_list, train_mask, axis = 0)\n",
        "\n",
        "src_valid = np.take(src_list, valid_mask, axis = 0)\n",
        "trg_valid = np.take(trg_list, valid_mask, axis = 0)"
      ],
      "metadata": {
        "id": "j9RhwPEARRs0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(src_train.shape)\n",
        "print(trg_train.shape)\n",
        "print(src_valid.shape)\n",
        "print(trg_valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M131eWURi28",
        "outputId": "db2359f5-59fc-4cf9-e155-14b1bfe6d4c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 100)\n",
            "(100000, 100)\n",
            "(10000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b4vlZqXRkK4",
        "outputId": "1a66b94c-9050-47d0-9b78-7cd82cbe5572"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "452"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataSet"
      ],
      "metadata": {
        "id": "HV-usTiYRpan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, src_data, trg_data):\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(src_data) == len(trg_data)\n",
        "\n",
        "        self.src_data = src_data\n",
        "        self.trg_data = trg_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "        \n",
        "    def __getitem__ (self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        trg_input = self.trg_data[idx]\n",
        "        trg_output = trg_input[1:SEQ_LEN]\n",
        "        trg_output = np.pad(trg_output, (0,1), 'constant', constant_values =0)\n",
        "        # (seq_len,)\n",
        "        return src, trg_input, trg_output\n",
        "\n",
        "train_dataset = TrainDataset(src_train, trg_train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle= True, pin_memory=True)"
      ],
      "metadata": {
        "id": "Bq4-Z8TJRlaG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidDataset(Dataset):\n",
        "    def __init__(self, src_data, trg_data):\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(src_data) == len(trg_data)\n",
        "\n",
        "        self.src_data = src_data\n",
        "        self.trg_data = trg_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "        \n",
        "    def __getitem__ (self, idx):\n",
        "        src = self.src_data[idx]\n",
        "        trg_input = self.trg_data[idx]\n",
        "        trg_output = trg_input[1:SEQ_LEN]\n",
        "        trg_output = np.pad(trg_output, (0,1), 'constant',constant_values= 0)\n",
        "\n",
        "        return src, trg_input, trg_output\n",
        "\n",
        "valid_dataset = ValidDataset(src_valid, trg_valid)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle= False, pin_memory=True)"
      ],
      "metadata": {
        "id": "diuRDNh7Rs9f"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for src, trg_input, trg_output in train_dataloader:\n",
        "    print(src.shape)\n",
        "    print(trg_input.shape)\n",
        "    print(trg_output.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_ajpjJwRuGL",
        "outputId": "03ca6f54-cf7e-4f93-f27a-bb7970854431"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 100])\n",
            "torch.Size([2, 100])\n",
            "torch.Size([2, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "TX3L9zeDR42s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mask function"
      ],
      "metadata": {
        "id": "Vi6vw_AyR-WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeMask(tensor, option: str) -> torch.Tensor:\n",
        "\n",
        "    if option == 'padding':\n",
        "        tmp = torch.full_like(tensor, fill_value=PAD_IDX).to(device)\n",
        "        # tmp : (bs,seq_len)\n",
        "        mask = (tensor != tmp).float()\n",
        "        # mask : (bs, seq_len)\n",
        "        mask = repeat(mask, 'bs seq_len -> bs new_axis seq_len ',\n",
        "                      new_axis=mask.shape[1])\n",
        "        # mask(bs,seq_len,seq_len)\n",
        "\n",
        "    elif option == 'lookahead':\n",
        "        padding_mask = makeMask(tensor, 'padding')\n",
        "\n",
        "        mask = torch.ones_like(padding_mask)\n",
        "        mask = torch.tril(mask)\n",
        "\n",
        "        mask = mask * padding_mask\n",
        "\n",
        "    return mask"
      ],
      "metadata": {
        "id": "qVIoLeiyRvgM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention"
      ],
      "metadata": {
        "id": "z-Qo3vAISIej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multiheadattention(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, num_head: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # embedding_dim, d_model, 512 in paper\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # 8 in paper\n",
        "        self.num_head = num_head\n",
        "        # head_dim, d_key, d_query, d_value, 64 in paper (= 512 / 8)\n",
        "        self.head_dim = hidden_dim // num_head\n",
        "        self.scale = torch.sqrt(torch.FloatTensor()).to(device)\n",
        "\n",
        "        self.fcQ = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fcK = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fcV = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fcOut = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, srcQ, srcK, srcV, mask=None):\n",
        "\n",
        "        ##### SCALED DOT PRODUCT ATTENTION ######\n",
        "\n",
        "        # input : (bs, seq_len, hidden_dim)\n",
        "        Q = self.fcQ(srcQ)\n",
        "        K = self.fcK(srcK)\n",
        "        V = self.fcV(srcV)\n",
        "\n",
        "        Q = rearrange(\n",
        "            Q, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
        "        K_T = rearrange(\n",
        "            K, 'bs seq_len (num_head head_dim) -> bs num_head head_dim seq_len', num_head=self.num_head)\n",
        "        V = rearrange(\n",
        "            V, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
        "\n",
        "        attention_energy = torch.matmul(Q, K_T)\n",
        "        # attention_energy : (bs, num_head, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None :\n",
        "            attention_energy : torch.masked_fill(attention_energy, (mask==0), -1e10)\n",
        "\n",
        "        attention_energy = torch.softmax(attention_energy, dim = -1)\n",
        "        # print(attention_energy[0,0,0,:])\n",
        "\n",
        "        result = torch.matmul(attention_energy,V)\n",
        "        # result (bs, num_head, seq_len, head_dim)\n",
        "\n",
        "        ##### END OF SCALED DOT PRODUCT ATTENTION ######\n",
        "\n",
        "        # CONCAT\n",
        "        result = rearrange(result, 'bs num_head seq_len head_dim -> bs seq_len (num_head head_dim)')\n",
        "        # result : (bs, seq_len, hidden_dim)\n",
        "\n",
        "        # LINEAR\n",
        "\n",
        "        result = self.fcOut(result)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "qUK7r79oSIHs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Poistionwise Feedforward Network"
      ],
      "metadata": {
        "id": "FM-mr93aST7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__ (self, hidden_dim, inner_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # 512 in paper \n",
        "        self.hidden_dim = hidden_dim\n",
        "        # 2048 in paper\n",
        "        self.inner_dim = inner_dim \n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim, inner_dim)\n",
        "        self.fc2 = nn.Linear(inner_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "        \n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "        output = self.fc1(output)\n",
        "        output2 = self.relu(output)\n",
        "        output3 = self.fc2(output2)\n",
        "\n",
        "        return output3"
      ],
      "metadata": {
        "id": "lvPfmf-GSRYi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Layer"
      ],
      "metadata": {
        "id": "9JW1caa9SYJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_head, inner_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_head = num_head\n",
        "        self.inner_dim = inner_dim\n",
        "        \n",
        "        self.multiheadattention = Multiheadattention(hidden_dim, num_head)\n",
        "        self.ffn = FFN(hidden_dim, inner_dim)\n",
        "        self.layerNorm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=0.1)\n",
        "        self.dropout2 = nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "    def forward(self, input, mask = None):\n",
        "\n",
        "        # input : (bs, seq_len, hidden_dim)\n",
        "        \n",
        "        # encoder attention\n",
        "        # uses only padding mask\n",
        "        output = self.multiheadattention(srcQ= input, srcK = input, srcV = input, mask = mask)\n",
        "        output = self.dropout1(output)\n",
        "        output = input + output\n",
        "        output = self.layerNorm(output)\n",
        "\n",
        "        output_ = self.ffn(output)\n",
        "        output_ = self.dropout2(output_)\n",
        "        output = output + output_\n",
        "        output = self.layerNorm(output)\n",
        "\n",
        "        # output : (bs, seq_len, hidden_dim)\n",
        "        return output"
      ],
      "metadata": {
        "id": "iWDngvjXSW35"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder Architecture"
      ],
      "metadata": {
        "id": "m5LcI_4TScQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__ (self, N, hidden_dim, num_head, inner_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # N : number of encoder layer repeated \n",
        "        self.N = N\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_head = num_head\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE, embedding_dim=hidden_dim, padding_idx=0)\n",
        "        self.enc_layers = nn.ModuleList(EncoderLayer(hidden_dim, num_head, inner_dim) for _ in range(N))\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # input : (bs, seq_len)\n",
        "        mask = makeMask(input, option='padding')\n",
        "\n",
        "        # embedding layer\n",
        "        output = self.embedding(input)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # N encoder layer\n",
        "        for layer in self.enc_layers:\n",
        "            output = layer(output, mask)\n",
        "\n",
        "        # output : (bs, seq_len, hidden_dim)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "bpEALwV0SbSo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Layer"
      ],
      "metadata": {
        "id": "pkyfBMBvSjGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_head, inner_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_head = num_head\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "        self.multiheadattention1 = Multiheadattention(hidden_dim, num_head)\n",
        "        self.layerNorm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.multiheadattention2 = Multiheadattention(hidden_dim, num_head)\n",
        "        self.layerNorm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.ffn = FFN(hidden_dim, inner_dim)\n",
        "        self.layerNorm3 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=0.1)\n",
        "        self.dropout2 = nn.Dropout(p=0.1)\n",
        "        self.dropout3 = nn.Dropout(p=0.1)\n",
        "\n",
        "    \n",
        "    def forward(self, input, enc_output, paddingMask, lookaheadMask):\n",
        "        # input : (bs, seq_len, hidden_dim)\n",
        "\n",
        "        # first multiheadattention\n",
        "        output = self.multiheadattention1(input, input, input, lookaheadMask)\n",
        "        output = self.dropout1(output)\n",
        "        output = output + input\n",
        "        output = self.layerNorm1(output)\n",
        "\n",
        "        # second multiheadattention\n",
        "        output_ = self.multiheadattention2(output, enc_output, enc_output, paddingMask)\n",
        "        output_ = self.dropout2(output_)\n",
        "        output = output_ + output\n",
        "        output = self.layerNorm2(output)\n",
        "\n",
        "        # Feedforward Network\n",
        "        output_ = self.ffn(output)\n",
        "        output_ = self.dropout3(output_)\n",
        "        output = output + output_\n",
        "        output = self.layerNorm3(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "9_eX229QSh84"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Architecture"
      ],
      "metadata": {
        "id": "0z80sexVSmnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__ (self, N, hidden_dim, num_head, inner_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # N : number of encoder layer repeated \n",
        "        self.N = N\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_head = num_head\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE, embedding_dim=hidden_dim, padding_idx=0)\n",
        "\n",
        "        self.dec_layers = nn.ModuleList(DecoderLayer(hidden_dim, num_head, inner_dim) for _ in range(N))\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        \n",
        "        self.finalFc = nn.Linear(hidden_dim, VOCAB_SIZE)\n",
        "\n",
        "\n",
        "    def forward(self, input, enc_src, enc_output):\n",
        "\n",
        "        # input : (bs, seq_len)\n",
        "        # enc_src : (bs, seq_len)\n",
        "        # enc_output : (bs, seq_len,hidden_dim)\n",
        "\n",
        "        lookaheadMask = makeMask(input, option='lookahead')\n",
        "        paddingMask = makeMask(enc_src, option = 'padding')\n",
        "\n",
        "        # embedding layer\n",
        "        output = self.embedding(input)\n",
        "\n",
        "        # Dropout\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # N decoder layer\n",
        "        for layer in self.dec_layers:\n",
        "            output = layer(output, enc_output, paddingMask, lookaheadMask)\n",
        "        # output : (bs, seq_len, hidden_dim)\n",
        "\n",
        "        logits = self.finalFc(output)\n",
        "        # logits : (bs, seq_len, VOCAB_SIZE)\n",
        "        output = torch.softmax(logits, dim = -1)\n",
        "\n",
        "        output = torch.argmax(output, dim = -1)\n",
        "        # output : (bs, seq_len)\n",
        "\n",
        "\n",
        "\n",
        "        return output, logits\n"
      ],
      "metadata": {
        "id": "eBfJ2chUSl-G"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Model"
      ],
      "metadata": {
        "id": "m_X1z1VoSsio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, N = 6, hidden_dim = 512, num_head = 8, inner_dim = 2048):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(N, hidden_dim, num_head, inner_dim)\n",
        "        self.decoder = Decoder(N, hidden_dim, num_head, inner_dim)\n",
        "\n",
        "    def forward(self, enc_src, dec_src):\n",
        "        # enc_src : (bs, seq_len)\n",
        "        # dec_src : (bs, seq_len)\n",
        "\n",
        "        # print(f'enc_src : {enc_src.shape}')\n",
        "        # print(f'dec_src : {dec_src.shape}')\n",
        "\n",
        "        enc_output = self.encoder(enc_src)\n",
        "        output, logits = self.decoder(dec_src, enc_src, enc_output.detach())\n",
        "        # logits = (bs, seq_len, VOCAB_SIZE) \n",
        "\n",
        "        return output, logits"
      ],
      "metadata": {
        "id": "xUHLCjpRSrXl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Train"
      ],
      "metadata": {
        "id": "vXFphYHQS6DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer().to(device)"
      ],
      "metadata": {
        "id": "1AIeX51GSy_j"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "test1 = torch.randint(low = 0, high = 1000, size = (SEQ_LEN,))\n",
        "test2 = torch.randint(low = 0, high = 1000, size = (SEQ_LEN,))\n",
        "summary(model, [(SEQ_LEN,), (SEQ_LEN,)], dtypes = [torch.int, torch.int])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG7NUVBUS77q",
        "outputId": "439962d0-2d59-43e4-8d18-037422e37597"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================================================\n",
            "Layer (type:depth-idx)                        Output Shape              Param #\n",
            "===============================================================================================\n",
            "├─Encoder: 1-1                                [-1, 100, 512]            --\n",
            "|    └─Embedding: 2-1                         [-1, 100, 512]            16,387,584\n",
            "|    └─Dropout: 2-2                           [-1, 100, 512]            --\n",
            "|    └─ModuleList: 2                          []                        --\n",
            "|    |    └─EncoderLayer: 3-1                 [-1, 100, 512]            3,151,360\n",
            "|    |    └─EncoderLayer: 3-2                 [-1, 100, 512]            3,151,360\n",
            "|    |    └─EncoderLayer: 3-3                 [-1, 100, 512]            3,151,360\n",
            "|    |    └─EncoderLayer: 3-4                 [-1, 100, 512]            3,151,360\n",
            "|    |    └─EncoderLayer: 3-5                 [-1, 100, 512]            3,151,360\n",
            "|    |    └─EncoderLayer: 3-6                 [-1, 100, 512]            3,151,360\n",
            "├─Decoder: 1-2                                [-1, 100]                 --\n",
            "|    └─Embedding: 2-3                         [-1, 100, 512]            16,387,584\n",
            "|    └─Dropout: 2-4                           [-1, 100, 512]            --\n",
            "|    └─ModuleList: 2                          []                        --\n",
            "|    |    └─DecoderLayer: 3-7                 [-1, 100, 512]            4,204,032\n",
            "|    |    └─DecoderLayer: 3-8                 [-1, 100, 512]            4,204,032\n",
            "|    |    └─DecoderLayer: 3-9                 [-1, 100, 512]            4,204,032\n",
            "|    |    └─DecoderLayer: 3-10                [-1, 100, 512]            4,204,032\n",
            "|    |    └─DecoderLayer: 3-11                [-1, 100, 512]            4,204,032\n",
            "|    |    └─DecoderLayer: 3-12                [-1, 100, 512]            4,204,032\n",
            "|    └─Linear: 2-5                            [-1, 100, 32007]          16,419,591\n",
            "===============================================================================================\n",
            "Total params: 93,327,111\n",
            "Trainable params: 93,327,111\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 230.49\n",
            "===============================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 34.58\n",
            "Params size (MB): 356.01\n",
            "Estimated Total Size (MB): 390.59\n",
            "===============================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "├─Encoder: 1-1                                [-1, 100, 512]            --\n",
              "|    └─Embedding: 2-1                         [-1, 100, 512]            16,387,584\n",
              "|    └─Dropout: 2-2                           [-1, 100, 512]            --\n",
              "|    └─ModuleList: 2                          []                        --\n",
              "|    |    └─EncoderLayer: 3-1                 [-1, 100, 512]            3,151,360\n",
              "|    |    └─EncoderLayer: 3-2                 [-1, 100, 512]            3,151,360\n",
              "|    |    └─EncoderLayer: 3-3                 [-1, 100, 512]            3,151,360\n",
              "|    |    └─EncoderLayer: 3-4                 [-1, 100, 512]            3,151,360\n",
              "|    |    └─EncoderLayer: 3-5                 [-1, 100, 512]            3,151,360\n",
              "|    |    └─EncoderLayer: 3-6                 [-1, 100, 512]            3,151,360\n",
              "├─Decoder: 1-2                                [-1, 100]                 --\n",
              "|    └─Embedding: 2-3                         [-1, 100, 512]            16,387,584\n",
              "|    └─Dropout: 2-4                           [-1, 100, 512]            --\n",
              "|    └─ModuleList: 2                          []                        --\n",
              "|    |    └─DecoderLayer: 3-7                 [-1, 100, 512]            4,204,032\n",
              "|    |    └─DecoderLayer: 3-8                 [-1, 100, 512]            4,204,032\n",
              "|    |    └─DecoderLayer: 3-9                 [-1, 100, 512]            4,204,032\n",
              "|    |    └─DecoderLayer: 3-10                [-1, 100, 512]            4,204,032\n",
              "|    |    └─DecoderLayer: 3-11                [-1, 100, 512]            4,204,032\n",
              "|    |    └─DecoderLayer: 3-12                [-1, 100, 512]            4,204,032\n",
              "|    └─Linear: 2-5                            [-1, 100, 32007]          16,419,591\n",
              "===============================================================================================\n",
              "Total params: 93,327,111\n",
              "Trainable params: 93,327,111\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 230.49\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 34.58\n",
              "Params size (MB): 356.01\n",
              "Estimated Total Size (MB): 390.59\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.named_parameters():\n",
        "    if 'weight' in param[0] and 'layerNorm' not in param[0] :\n",
        "        torch.nn.init.xavier_uniform_(param[1])"
      ],
      "metadata": {
        "id": "xN-AXlMOS9Rl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params = model.parameters())"
      ],
      "metadata": {
        "id": "XSMwTuldS_CV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(logits: torch.tensor, targets: torch.tensor):\n",
        "    return nn.CrossEntropyLoss(ignore_index=PAD_IDX)(logits.view(-1,VOCAB_SIZE), targets.view(-1))"
      ],
      "metadata": {
        "id": "LpZy23vQS_6-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
        "    # train 모드로 변경\n",
        "    model.train()\n",
        "\n",
        "    # for the Mixed Precision\n",
        "    # Pytorch 예제 : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "    \n",
        "    for step, (src, trg_input, trg_output) in bar:\n",
        "        src = src.to(device)\n",
        "        trg_input = trg_input.to(device)\n",
        "        trg_output = trg_output.to(device)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        with amp.autocast(enabled=True):\n",
        "            output, logits = model(enc_src = src, dec_src = trg_input)\n",
        "            loss = criterion(logits, trg_output)\n",
        "            \n",
        "        # logits (bs, seq_len, VOCAB_SIZE)\n",
        "        # trg_output (bs, seq_len)\n",
        "\n",
        "        # loss를 Scale\n",
        "        # Scaled Grdients를 계산(call)하기 위해 scaled loss를 backward()\n",
        "        scaler.scale(loss).backward()\n",
        "        # loss.backward()\n",
        "\n",
        "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
        "        # otherwise, optimizer.step() is skipped.\n",
        "        scaler.step(optimizer)\n",
        "        # optimizer.step()\n",
        "        \n",
        "        # Updates the scale for next iteration.\n",
        "        scaler.update()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # change learning rate by Scheduler\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # loss.item()은 loss를 Python Float으로 반환\n",
        "        # loss.item()은 batch data의 average loss이므로, sum of loss를 구하기 위해 batch_size를 곱해준다\n",
        "        running_loss += loss.item() * batch_size\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        epoch_loss = running_loss / dataset_size\n",
        "\n",
        "        bar.set_postfix(\n",
        "            Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"]\n",
        "        )\n",
        "\n",
        "    # Garbage Collector\n",
        "    gc.collect()\n",
        "\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "-lB7vdF8TBNA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def valid_one_epoch(model, dataloader, device, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    dataset_size = 0\n",
        "    running_loss = 0\n",
        "\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "    for step, (src, trg_input, trg_output) in bar:\n",
        "        src = src.to(device)\n",
        "        trg_input = trg_input.to(device)\n",
        "        trg_output = trg_output.to(device)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        output, logits = model(enc_src = src, dec_src = trg_input)\n",
        "        loss = criterion(logits, trg_output)\n",
        "\n",
        "        running_loss += loss.item() * batch_size\n",
        "        dataset_size += batch_size\n",
        "\n",
        "        # 실시간으로 정보를 표시하기 위한 epoch loss\n",
        "        val_loss = running_loss / dataset_size\n",
        "\n",
        "        bar.set_postfix(\n",
        "            Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[0][\"lr\"]\n",
        "        )\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "SHFF51w4TKHK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    metric_prefix=\"\",\n",
        "    file_prefix=\"\",\n",
        "    early_stopping=True,\n",
        "    early_stopping_step=10,\n",
        "):\n",
        "    # To automatically log graidents\n",
        "    wandb.watch(model, log_freq=100)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n",
        "\n",
        "    start = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "    history = defaultdict(list)\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    # num_epochs만큼, train과 val을 실행한다\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        gc.collect()\n",
        "\n",
        "        train_epoch_loss = train_one_epoch(\n",
        "            model,\n",
        "            optimizer,\n",
        "            scheduler,\n",
        "            dataloader= train_dataloader,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "        )\n",
        "\n",
        "        val_loss = valid_one_epoch(\n",
        "            model, valid_dataloader, device=device, epoch=epoch\n",
        "        )\n",
        "\n",
        "        history[f\"{metric_prefix}Train Loss\"].append(train_epoch_loss)\n",
        "        history[f\"{metric_prefix}Valid Loss\"].append(val_loss)\n",
        "\n",
        "        # Log the metrics\n",
        "        wandb.log(\n",
        "            {\n",
        "                f\"{metric_prefix}Train Loss\": train_epoch_loss,\n",
        "                f\"{metric_prefix}Valid Loss\": val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"Valid Loss : {val_loss}\")\n",
        "\n",
        "        # deep copy the model\n",
        "        if val_loss <= best_loss:\n",
        "            early_stop_counter = 0\n",
        "\n",
        "            print(\n",
        "                f\"Validation Loss improved( {best_loss} ---> {val_loss}  )\"\n",
        "            )\n",
        "\n",
        "            # Update Best Loss\n",
        "            best_loss = val_loss\n",
        "            \n",
        "            # Update Best Model Weight\n",
        "            # run.summary['Best RMSE'] = best_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(file_prefix, epoch, best_loss)\n",
        "            torch.save(model.state_dict(), PATH)\n",
        "            torch.save(model.state_dict(), f\"{file_prefix}best_{epoch}epoch.bin\")\n",
        "            # Save a model file from the current directory\n",
        "            wandb.save(PATH)\n",
        "\n",
        "            print(f\"Model Saved\")\n",
        "\n",
        "        elif early_stopping:\n",
        "            early_stop_counter += 1\n",
        "            if early_stop_counter > early_stopping_step:\n",
        "                break\n",
        "\n",
        "        print()\n",
        "\n",
        "    end = time.time()\n",
        "    time_elapsed = end - start\n",
        "    print(\n",
        "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
        "            time_elapsed // 3600,\n",
        "            (time_elapsed % 3600) // 60,\n",
        "            (time_elapsed % 3600) % 60,\n",
        "        )\n",
        "    )\n",
        "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "TLZRy2WgTLe7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training(\n",
        "    model = model,\n",
        "    optimizer = optimizer,\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=100, eta_min=1e-6),\n",
        "    device = device,\n",
        "    num_epochs = 1,\n",
        "    metric_prefix=\"\",\n",
        "    file_prefix=\"\",\n",
        "    early_stopping=True,\n",
        "    early_stopping_step=10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpg6VVweTM6A",
        "outputId": "68576573-fc98-45de-a233-149e649c87c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using GPU:Tesla V100-SXM2-16GB\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1079/50000 [01:20<55:30, 14.69it/s, Epoch=1, LR=0.000106, Train_Loss=8.27]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'final.bin')\n",
        "wandb.save('final.bin')\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "swgAZMGqTPUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}