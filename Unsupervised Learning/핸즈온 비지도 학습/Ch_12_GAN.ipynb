{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9rvWDsx/YmgN/0dZEq2VP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namwootree/Basic_Skill/blob/main/Unsupervised%20Learning/%ED%95%B8%EC%A6%88%EC%98%A8%20%EB%B9%84%EC%A7%80%EB%8F%84%20%ED%95%99%EC%8A%B5/Ch_12_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN"
      ],
      "metadata": {
        "id": "VILwe6Lvqp1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 생성자 : 입력 받은 실제 데이터의 샘플을 사용해 생성한 모델을 기반으로 데이터를 생성\n",
        "\n",
        "* 감별자 : 생성자가 생성한 데이터와 실제 분포의 데이터 (실제 데이터)를 판별\n",
        "\n",
        "* GAN은 레이블이 없는 경우에도 생성자가 실제 분포의 내재된 구조를 학습할 수 있으므로 비지도 학습 알고리즘에 속한다\n",
        "\n",
        "* 생성자는 훈련된 데이터 양보다 훨씬 적은 수의 매개변수를 사용해 데이터의 내재된 구조를 학습\n",
        "\n",
        "* 이 제약 조건으로, 생성자는 실제 데이터 분포의 가장 핵심적인 특징을 효율적으로 추출해야 한다\n",
        "\n",
        "* 생성자의 신경망에 있는 각 은닉층은 데이터의 내재된 표현을 추출한다\n",
        "\n",
        "* 그리고 후행 계층은 더 단순한 선행 계층 위에 구축함으로써 더 복잡한 표현을 추출한다\n",
        "\n",
        "* 생성자는 이 모든 계층을 함께 사용해 데이터의 내재된 구조를 학습하고 실제 데이터와 거의 동일한 합성 데이터를 생성하려고 시도한다\n",
        "\n",
        "* 생성자가 실제 데이터의 핵심을 성공적으로 추출했다면 합성 데이터는 실제 데이터처럼 보일 것"
      ],
      "metadata": {
        "id": "as2_GgSNqxgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "8ZbDyE_ysX93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library"
      ],
      "metadata": {
        "id": "rv9ppDdRsZJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip, datetime"
      ],
      "metadata": {
        "id": "WRLaPI3ysbVu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import Grid\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()"
      ],
      "metadata": {
        "id": "njqMOXTgsdHr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "h0M5aP6esgvT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "0qAKGTJ1snz7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
        "from keras.layers import BatchNormalization, Input, Lambda\n",
        "from keras.layers import Embedding, Flatten, dot\n",
        "from keras import regularizers\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "metadata": {
        "id": "r-qQ7kbAspOd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data Set"
      ],
      "metadata": {
        "id": "ZaIvNMWptKPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive Mount"
      ],
      "metadata": {
        "id": "XVIzPN_ltMcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzio3_7itQaL",
        "outputId": "d3da158a-4ff8-43c9-e6d6-f1505c0ad1f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Mnist Data Set"
      ],
      "metadata": {
        "id": "UUbRjBwTtgmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_mnist = '/content/drive/MyDrive/머신러닝 엔지니어링/핸즈온 비지도 학습/data/mnist.pkl.gz'\n",
        "\n",
        "f = gzip.open(path_mnist, 'rb')\n",
        "\n",
        "train_set, val_set, test_set = pickle.load(f, encoding='latin1')\n",
        "f.close()"
      ],
      "metadata": {
        "id": "JMgQdCMPtVPp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_set[0], train_set[1]\n",
        "X_val, y_val = val_set[0], val_set[1]\n",
        "X_test, y_test = test_set[0], test_set[1]"
      ],
      "metadata": {
        "id": "pmvYLZD2tsrO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesing"
      ],
      "metadata": {
        "id": "HtfuIUnLt7eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_keras = X_train.reshape(50000,28,28,1)\n",
        "X_validation_keras = X_val.reshape(10000,28,28,1)\n",
        "X_test_keras = X_test.reshape(10000,28,28,1)\n",
        "\n",
        "y_train_keras = to_categorical(y_train)\n",
        "y_validation_keras = to_categorical(y_val)\n",
        "y_test_keras = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "2tHO57lGt9cx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_index = range(0,len(X_train))\n",
        "val_index = range(len(X_train), len(X_train)+len(X_val))\n",
        "test_index = range(len(X_train)+len(X_val),\n",
        "                   len(X_train)+len(X_val)+len(X_test))\n",
        "\n",
        "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
        "y_train = pd.Series(data=y_train,index=train_index)\n",
        "\n",
        "X_validation = pd.DataFrame(data=X_val, index=val_index)\n",
        "y_validation = pd.Series(data=y_val, index=val_index)\n",
        "\n",
        "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
        "y_test = pd.Series(data=y_test,index=test_index)"
      ],
      "metadata": {
        "id": "0t9FFnhGuAPU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view_digit(X, y, example):\n",
        "    label = y.loc[example]\n",
        "    image = X.loc[example,:].values.reshape([28,28])\n",
        "    plt.title('Example: %d  Label: %d' % (example, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NViTRpHNuTyX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_digit(X_train, y_train, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "GUiEUsNJuVqC",
        "outputId": "6cf92485-7f46-48ff-e629-579791367327"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU 사용 확인"
      ],
      "metadata": {
        "id": "YygfvT7-uj4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else: print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_ClDdaLuePx",
        "outputId": "b285883c-6f52-448e-9fdf-9679ed33cc9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "hQVHDsKHsGeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 생성"
      ],
      "metadata": {
        "id": "Er9pIgHtvrzk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KeYao3Wrqni1"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32,\n",
        "                 kernel_size=(5,5),\n",
        "                 padding='Same',\n",
        "                 activation='relu',\n",
        "                 input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(Conv2D(filters = 32,\n",
        "                 kernel_size=(5,5), \n",
        "                 padding='Same', \n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters=64,\n",
        "                 kernel_size=(3,3),\n",
        "                 padding='Same', \n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(Conv2D(filters=64, \n",
        "                 kernel_size=(3,3), \n",
        "                 padding='Same', \n",
        "                 activation='relu'))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2),\n",
        "                    strides=(2,2)))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation =\"relu\"))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(10, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 컴파일"
      ],
      "metadata": {
        "id": "g2uqvt0zvu0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R_zO5zXLvtOV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "Aoe6KYz6v1GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
        "                        validation_data=(X_validation_keras,\n",
        "                                         y_validation_keras),\n",
        "                        epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwzYNj25vy2c",
        "outputId": "7bc747c8-c244-477f-dfb4-3e81b099c490"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1563/1563 [==============================] - 18s 6ms/step - loss: 0.1900 - accuracy: 0.9413 - val_loss: 0.0409 - val_accuracy: 0.9884\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0728 - accuracy: 0.9785 - val_loss: 0.0402 - val_accuracy: 0.9898\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0546 - accuracy: 0.9838 - val_loss: 0.0295 - val_accuracy: 0.9917\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.0284 - val_accuracy: 0.9924\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0383 - accuracy: 0.9884 - val_loss: 0.0322 - val_accuracy: 0.9920\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0366 - accuracy: 0.9890 - val_loss: 0.0292 - val_accuracy: 0.9921\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0332 - accuracy: 0.9905 - val_loss: 0.0276 - val_accuracy: 0.9916\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.0297 - accuracy: 0.9911 - val_loss: 0.0308 - val_accuracy: 0.9925\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 0.0356 - val_accuracy: 0.9912\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 0.0447 - val_accuracy: 0.9917\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.0289 - val_accuracy: 0.9936\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.0300 - val_accuracy: 0.9926\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.0268 - val_accuracy: 0.9940\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0234 - accuracy: 0.9932 - val_loss: 0.0335 - val_accuracy: 0.9924\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0202 - accuracy: 0.9935 - val_loss: 0.0369 - val_accuracy: 0.9928\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.0326 - val_accuracy: 0.9937\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0197 - accuracy: 0.9938 - val_loss: 0.0349 - val_accuracy: 0.9928\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0297 - val_accuracy: 0.9930\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.0334 - val_accuracy: 0.9943\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.0356 - val_accuracy: 0.9926\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.0456 - val_accuracy: 0.9906\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0437 - val_accuracy: 0.9920\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0199 - accuracy: 0.9944 - val_loss: 0.0386 - val_accuracy: 0.9939\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0494 - val_accuracy: 0.9912\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0186 - accuracy: 0.9943 - val_loss: 0.0342 - val_accuracy: 0.9939\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0175 - accuracy: 0.9949 - val_loss: 0.0405 - val_accuracy: 0.9922\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0185 - accuracy: 0.9948 - val_loss: 0.0387 - val_accuracy: 0.9931\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0204 - accuracy: 0.9942 - val_loss: 0.0317 - val_accuracy: 0.9941\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0383 - val_accuracy: 0.9934\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0210 - accuracy: 0.9942 - val_loss: 0.0384 - val_accuracy: 0.9928\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0199 - accuracy: 0.9944 - val_loss: 0.0370 - val_accuracy: 0.9934\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0188 - accuracy: 0.9948 - val_loss: 0.0478 - val_accuracy: 0.9928\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.0371 - val_accuracy: 0.9943\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.0513 - val_accuracy: 0.9931\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0210 - accuracy: 0.9944 - val_loss: 0.0345 - val_accuracy: 0.9935\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0177 - accuracy: 0.9948 - val_loss: 0.0380 - val_accuracy: 0.9933\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.0436 - val_accuracy: 0.9939\n",
            "Epoch 38/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0190 - accuracy: 0.9949 - val_loss: 0.0424 - val_accuracy: 0.9933\n",
            "Epoch 39/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0175 - accuracy: 0.9952 - val_loss: 0.0348 - val_accuracy: 0.9939\n",
            "Epoch 40/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0191 - accuracy: 0.9948 - val_loss: 0.0416 - val_accuracy: 0.9933\n",
            "Epoch 41/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0196 - accuracy: 0.9948 - val_loss: 0.0349 - val_accuracy: 0.9932\n",
            "Epoch 42/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0197 - accuracy: 0.9952 - val_loss: 0.0346 - val_accuracy: 0.9943\n",
            "Epoch 43/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0187 - accuracy: 0.9952 - val_loss: 0.0437 - val_accuracy: 0.9935\n",
            "Epoch 44/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0219 - accuracy: 0.9946 - val_loss: 0.0456 - val_accuracy: 0.9932\n",
            "Epoch 45/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0214 - accuracy: 0.9946 - val_loss: 0.0421 - val_accuracy: 0.9939\n",
            "Epoch 46/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.0445 - val_accuracy: 0.9945\n",
            "Epoch 47/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0175 - accuracy: 0.9952 - val_loss: 0.0407 - val_accuracy: 0.9933\n",
            "Epoch 48/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0202 - accuracy: 0.9949 - val_loss: 0.0543 - val_accuracy: 0.9938\n",
            "Epoch 49/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.0480 - val_accuracy: 0.9938\n",
            "Epoch 50/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0407 - val_accuracy: 0.9939\n",
            "Epoch 51/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0181 - accuracy: 0.9951 - val_loss: 0.0429 - val_accuracy: 0.9945\n",
            "Epoch 52/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0199 - accuracy: 0.9949 - val_loss: 0.0409 - val_accuracy: 0.9946\n",
            "Epoch 53/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0194 - accuracy: 0.9956 - val_loss: 0.0429 - val_accuracy: 0.9944\n",
            "Epoch 54/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0183 - accuracy: 0.9954 - val_loss: 0.0597 - val_accuracy: 0.9926\n",
            "Epoch 55/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0219 - accuracy: 0.9945 - val_loss: 0.0444 - val_accuracy: 0.9941\n",
            "Epoch 56/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0145 - accuracy: 0.9960 - val_loss: 0.0512 - val_accuracy: 0.9939\n",
            "Epoch 57/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0193 - accuracy: 0.9947 - val_loss: 0.0453 - val_accuracy: 0.9934\n",
            "Epoch 58/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0220 - accuracy: 0.9950 - val_loss: 0.0506 - val_accuracy: 0.9939\n",
            "Epoch 59/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0196 - accuracy: 0.9953 - val_loss: 0.0625 - val_accuracy: 0.9931\n",
            "Epoch 60/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0162 - accuracy: 0.9958 - val_loss: 0.0454 - val_accuracy: 0.9938\n",
            "Epoch 61/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0196 - accuracy: 0.9953 - val_loss: 0.0619 - val_accuracy: 0.9925\n",
            "Epoch 62/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0203 - accuracy: 0.9950 - val_loss: 0.0432 - val_accuracy: 0.9948\n",
            "Epoch 63/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0180 - accuracy: 0.9957 - val_loss: 0.0539 - val_accuracy: 0.9944\n",
            "Epoch 64/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.0534 - val_accuracy: 0.9936\n",
            "Epoch 65/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0209 - accuracy: 0.9951 - val_loss: 0.0508 - val_accuracy: 0.9951\n",
            "Epoch 66/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0231 - accuracy: 0.9947 - val_loss: 0.0480 - val_accuracy: 0.9944\n",
            "Epoch 67/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0179 - accuracy: 0.9957 - val_loss: 0.0596 - val_accuracy: 0.9937\n",
            "Epoch 68/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0183 - accuracy: 0.9956 - val_loss: 0.0638 - val_accuracy: 0.9931\n",
            "Epoch 69/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0188 - accuracy: 0.9956 - val_loss: 0.0537 - val_accuracy: 0.9946\n",
            "Epoch 70/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.0495 - val_accuracy: 0.9945\n",
            "Epoch 71/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0218 - accuracy: 0.9950 - val_loss: 0.0528 - val_accuracy: 0.9934\n",
            "Epoch 72/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0226 - accuracy: 0.9951 - val_loss: 0.0432 - val_accuracy: 0.9949\n",
            "Epoch 73/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0222 - accuracy: 0.9952 - val_loss: 0.0488 - val_accuracy: 0.9938\n",
            "Epoch 74/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.0490 - val_accuracy: 0.9941\n",
            "Epoch 75/100\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.0186 - accuracy: 0.9954 - val_loss: 0.0495 - val_accuracy: 0.9944\n",
            "Epoch 76/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0229 - accuracy: 0.9949 - val_loss: 0.0538 - val_accuracy: 0.9935\n",
            "Epoch 77/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0214 - accuracy: 0.9953 - val_loss: 0.0429 - val_accuracy: 0.9950\n",
            "Epoch 78/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0228 - accuracy: 0.9945 - val_loss: 0.0471 - val_accuracy: 0.9944\n",
            "Epoch 79/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0168 - accuracy: 0.9954 - val_loss: 0.0551 - val_accuracy: 0.9940\n",
            "Epoch 80/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0231 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9936\n",
            "Epoch 81/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0176 - accuracy: 0.9955 - val_loss: 0.0616 - val_accuracy: 0.9939\n",
            "Epoch 82/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0218 - accuracy: 0.9953 - val_loss: 0.0510 - val_accuracy: 0.9942\n",
            "Epoch 83/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.0520 - val_accuracy: 0.9944\n",
            "Epoch 84/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0192 - accuracy: 0.9957 - val_loss: 0.0566 - val_accuracy: 0.9926\n",
            "Epoch 85/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0195 - accuracy: 0.9951 - val_loss: 0.0610 - val_accuracy: 0.9934\n",
            "Epoch 86/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0212 - accuracy: 0.9952 - val_loss: 0.0764 - val_accuracy: 0.9944\n",
            "Epoch 87/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0213 - accuracy: 0.9954 - val_loss: 0.0656 - val_accuracy: 0.9940\n",
            "Epoch 88/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0199 - accuracy: 0.9957 - val_loss: 0.0696 - val_accuracy: 0.9944\n",
            "Epoch 89/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0224 - accuracy: 0.9954 - val_loss: 0.0542 - val_accuracy: 0.9942\n",
            "Epoch 90/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0200 - accuracy: 0.9949 - val_loss: 0.0640 - val_accuracy: 0.9941\n",
            "Epoch 91/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0222 - accuracy: 0.9953 - val_loss: 0.0653 - val_accuracy: 0.9937\n",
            "Epoch 92/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0210 - accuracy: 0.9955 - val_loss: 0.0528 - val_accuracy: 0.9941\n",
            "Epoch 93/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0206 - accuracy: 0.9951 - val_loss: 0.0552 - val_accuracy: 0.9938\n",
            "Epoch 94/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0209 - accuracy: 0.9953 - val_loss: 0.0494 - val_accuracy: 0.9944\n",
            "Epoch 95/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0178 - accuracy: 0.9962 - val_loss: 0.0676 - val_accuracy: 0.9938\n",
            "Epoch 96/100\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.0553 - val_accuracy: 0.9954\n",
            "Epoch 97/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.0804 - val_accuracy: 0.9939\n",
            "Epoch 98/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0203 - accuracy: 0.9952 - val_loss: 0.0864 - val_accuracy: 0.9940\n",
            "Epoch 99/100\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0262 - accuracy: 0.9947 - val_loss: 0.0575 - val_accuracy: 0.9947\n",
            "Epoch 100/100\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0250 - accuracy: 0.9951 - val_loss: 0.0580 - val_accuracy: 0.9934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 결과"
      ],
      "metadata": {
        "id": "B93Y0m62wp9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_history.history.keys())"
      ],
      "metadata": {
        "id": "CdZzPIJkwDoF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CNN Final Val Accuracy\", cnn_history.history['val_accuracy'][-1])\n",
        "pd.Series(cnn_history.history['val_accuracy']).plot(logy=False)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Val Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zAi5TX61wttN",
        "outputId": "4221b96b-1cdd-45cf-9837-43d654c56d35"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Final Val Accuracy 0.993399977684021\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eXxdZ33n//7e/eou2hdb3nc7wbFDSAgkJIHSmtIhk0DLUtrOTPujdErbKZO2MO3Q3zCTAVpo+2NKh8m06Y90KDSkw5DS0KSQQAwJJA5OnMVLbMebLNvadXWv7v7MH+c855676kqWLFl63q+XXpbOPefoubJ0vs93+3xFKYXBYDAYDM3iWewFGAwGg+HqwhgOg8FgMMwKYzgMBoPBMCuM4TAYDAbDrDCGw2AwGAyzwrfYC7gSdHV1qQ0bNiz2MgwGg+Gq4rnnnhtWSnVXHl8RhmPDhg0cOHBgsZdhMBgMVxUicrrWcROqMhgMBsOsMIbDYDAYDLPCGA6DwWAwzApjOAwGg8EwK4zhMBgMBsOsMIbDYDAYDLPCGA6DwWAwzApjOAwGg+Eq4PilBE+fGFnsZQDGcBgMBsNVwZ99+1V+56EXFnsZgDEcBoPBcFUwlsoylswu9jIAYzgMBoPhqmBiOkcyWyCbLy72UozhMBgMhquBiekcAOPTi+91GMNhMBgMVwHjqVzZv4uJMRwGg8GwxCkUFYl0HmBJ5DmM4TAYDIYlzuR0ycsYnzYeh8FgMBhmYMJtOFLL3OMQkX0iclREjovIx2q8vl5EviMih0TkuyKyxvXaZ0TkJfvjva7j/7+IvCYiz9sfexbyPRgMhhJ3fuEHPPD0qcVexoqj3HAsY49DRLzAF4B3ALuA94vIrorTPgs8oJTaDXwS+JR97TuB64E9wE3APSISd133O0qpPfbH8wv1HgwGQ4l8ocgLZ8f5wfHhxV7KisMdnhpbzoYDuBE4rpQ6qZTKAl8F7qw4ZxfwuP35E67XdwFPKqXySqkkcAjYt4BrNRgMM6B3vSeGkou8knKGEhkKRbXYy1hQVlKoqh846/r6nH3MzQvA3fbndwExEem0j+8TkRYR6QLuANa6rrvXDm/9qYgEF2b5BoPBjd71nh5JkissfhMawHS2wG1//ARfffbMYi9lQZmwjUV7i395h6qa5B7gNhE5CNwGDAAFpdRjwCPAU8BXgKeBgn3Nx4EdwBuADuD3at1YRD4kIgdE5MDQ0NDCvguDYQWgd7q5guLsaGqRV2MxPJUhlS3w3KmxxV7KgqI9jnWdEcaWuccxQLmXsMY+5qCUOq+UulsptRf4ffvYuP3vvXYO4+2AAMfs44PKIgP8NVZIrAql1H1KqRuUUjd0d3fP93szGFYcY8nSTnc+w1WpbJ5nT43O6Vq9+z58ITFv61mKjKdyhP1eemPBsrDVYrGQhuNZYKuIbBSRAPA+4GH3CSLSJSJ6DR8H7rePe+2QFSKyG9gNPGZ/vcr+V4B/Cby0gO/BYDDYuBO0J4em5u2+f//cOX7ufzzNyFRm1tfq3ffxS4kloeG0UExM52gN+2lr8S9vj0MplQc+AjwKHAYeVEq9LCKfFJF32afdDhwVkWNAL3CvfdwP7BeRV4D7gA/a9wP4soi8CLwIdAH/ZaHeg8FgKKFDVS0BLyfm0XBcnMygFAxOpGe/JtuY5QqKk8Pzt6alxsR0jrYWP+0tAcZSOZRa3GIA30LeXCn1CFauwn3sE67PHwIeqnFdGquyqtY93zrPyzQYVjzFouKvnzrFz92whljIX/Oc8VQOr0d4XX/rvIaqtGjfUGL2Hoe7wujw4CQ7+uINzr56GZ/OEQ/7aWsJkM0XSeeKhAPeRVvPYifHDQbDEuDoxQT/+Zuv8HfPnq17zlgqS2vYz+aeKMcvTc3brlf3JVxKzN7j0HmXgNfDkcHlm+eYnM7RZoeqgEUPVxnDYTAYmMpYkeD9r9Zv7hu3wyWbu6NMTOcYnSexPe01XJycg8cxnSUW9LG1N7qsE+TjKSvH0W4bjsUuyTWGw2AwkLQNx49eGyGTL9Q8ZzyVpb0lwObuCDB/lVXjl+FxjKdytLb42dEX5/Dg5LysZymik+Ot4QBQvwnw+KUE//H/vLTgDZHGcBgMBlJZy1ikc8W6PRFjSStcsrk7CsxfZZVjOObicdjGbOeqGEOJDMNzqMxa6mTyBaZzBSs5HtGhqtoex8PPn+dvfnia8+PTC7omYzgMBoPjcQA8WSdcZVX2BOhvCxP0eeatssoJVc0hOT6WssJnO1dZSfHlmOfQfRtWqMr2OOpMATwxbHmB8xVGrIcxHAbDEuHSZJoP/M8f8qWnTl3x7609jq09Ub5/vLbSwlgqS1uLH49H2NQdbSpU9Zf7T/KL9z9TN7SSzRdJ2t97aHIuoaosbS0BdvTFADhyYfmFq/QsjtaWAK3hxjmOE5csY24Mh8GwAjg9kuTdX3yKp06M8OlvHeHSHB6il4NOju+7to+XBiarmvEy+QKpbMFJzm7qjjTlcTx1YoQnjw3xs198mgs1+jT0zjkW9DE0laE4y9j8+HSO9hY/ndEgPbEgryzDPIc2Eq1hPyG/l7DfW9MQF4qK12yPY6FDdsZwGAzzwGwfeG5ePj/Bu//700yl83z+/XvJF4v86bdfncfVzUwqm8frEd62sxeA71dIp0+kSrtegM3dUc6Opuom0jVjqSz9bWEGJ9K8+78/VZUX0Q/FbX0xcgU1qzLTQlFZ4TN7F75zVXxZh6r0+7S6x6s9jvPj02Ts7nnjcRgMS5zvvzrMtf/vo3P6Yy0UFb90/zP4vcLXPnwz77puNT9/03oePHCW45euXCd0MlOgJeDldf2ttIb9VWW5ukNbexybuyMUFZweaSx2OJHKsWddG1/90BtJ5wr8ygMHyu9rPwC326GmS7PIcyTSOZSCNtuY7VgV4/ilqSWj3DtfuHMcYL3fWh7HcZdRHjGGw2BY2vzgxDCpbIFTI7MvT31tOMnwVJaPvn0bW3qsh+dvvHULYb+XP/qnI/O91LqksnkiAR9ej3DLli6+/+pwWYPfWFLLepc8DijF1OsxlsrS3uLn2v5WPvjG9ZwcKpdk1x7Gth7rfrMxHHrXrZvidvbFyRaKnFxi80IuF3eoCupLq+v3HQl4GZkyhsNgWNIcsePqw3OoCtLJXF0VBNAZDfLh2zbx2CsXOTBH1djZkswWiAQtCYtbtnZxYTJd5vGMV+x6Nzm9HPUNR9EJJVnGpjceAsrj7xOuUBXAxQa5ncpu9bFUuTHTP8O59nMkM/kZy1jdeYRKjl1MzEs3/fBUpsyj0B5HvCxUVW0YTgxN0dbiZ1N3lJGkyXEYDEuaw3ZcfXgOu7zDg5N4PcIWe8et+Te3bKS9xc//+uHpeVnjTKQyeSJBS7ruzZu7APjRayWjpR9k7RHrId0S8NHfFm4YTkuk8xRVySPoiVkz19wd4voBuL3XMhz19Kp+dHKEn/iT7/GD4yPOsYkKj2NTd4SAz8OhcxNNvedK/u2Xf8zP/+WPGp7zzUPnedvnvsupCuPx9IkRfvJPn+TvfzxQ58rm+dADB7jnay84X09M54iFLG8QrFBVLWn1E5em2NwdpTMaMDkOg2EpM5bMcsHeJc9FpO/IYILN3RFC/nLBupaAj/WdkQWPVWuSWSvHAbCm3erTcD8cnbBQuCSAuKk7wsk6u2/rGmvtOgfRE7cMh7tibCyVw+8VOiIBYiFf3Wqyrz13DrA6o+vd3+/1cOOGjjnNRN//6hDfOzbU0OMBKxxUVPDkq+Uly08cvQTA5x47SjrXuGCgEblCkZcGJnn+bMn4aWVcTVvYClVVejcnhpJs7o7QEQmYUJVhYUjnCnz7lYuLvYymyOaL/NNLF66YlPTpkSSHzo03de5hV9/AXEogGym6xkI+JtP5mq/NNzrHAeDxCOs6WjjtmvI3nsoR8Hoc4wJWnuNEA7HDyoS6DlW58xgT01YfhojQGw/V1Kuazhb41ouDAAy4Qkk6zt/ueqjesrWLoxcTMxoAN8Wi4tPfsvJJqWyhYXJdlxQ/eazcOD15bIi+eIjBiTR//YNTTX/vSk4OJckWigxPZZyNyLgtLqlpbwmQLyoSrqbNiekcw1MZNnVH6YoGTajKsDD8wwvn+ZUHDiyZEaCN+Pbhi3z4fz3HkSskYveZfzrCR/72YFPn6vLP9hb/rA3HRCrH+Yl0WX7DTTzsJ5G+MmJ2yUyBlmBpysL6zhbOjLgNR5bWFj/W/DSLzd0RktlC3YR2pUfQGQkgUuFxJHPOg78nFqypV/XYKxdIZgv4PFJhOLKIUCYDf+tWK8z2/QZijZU8/MJ5Xj4/yd51bQBMNTDWg/baf3hyxDEwlxJpjlxI8ItvWs9bd/TwF9897hQTzBZ3A6P+3J0nglJobiJVPVhrc3eUjkiAdK5IKrtwmw5jOFYo+o99oWOh84F+IF+4Qk1x58fTnBtLNTVR7siFSbqiAbb3xWZtOPSDYceqWM3X4yEfk9NXxuNIZvJEXN7Euo4IZ0ZTjjcxnsqV7exh5sqqyhyEz+uhMxIsMzTj09my5HktI/T1gwP0t4W5aVMHA2MlwzFmK8bq2D9YlVVd0QD7X63d/V5JJl/gjx89yrX9cT5w4zoAJhsY6wsT04T9XqYyeZ4/a3mlOjT2lq3d/N6+HSQzef78ieNNff9KXrFzXlDalGiBQ402xO4Eue7i39wdodPOQy1kuMoYjhWK/qVaCvOLZ0LPXJhL1dJcGEpkKCo4NzazN3Z4MMHOVXG6osFZ5zh09c+ueh5H6Mp5HKlsgZZAuccxnSswZBvDMVvaw80mbTjqVFZVVj0B9MaDZWGk8VSuLHl+aTJTFvoaSmTY/+owd+5Zzdr2lnKPw9X8p/HocuLjw001Zf7N06cZGJ/mY/t2Og/nRCOPYyLNvmv78AjsP2YZp/3HhumIBNi1Ks72vhjvef0a/ubp03PqwzkymGBbb4zeeND5/Ziwhzhpakmrnxiawu8V1na00Bm1DccCbgqN4Vih6Bhoo93VUsGZEHcFlE+VUk645PQMYbx8ocixiwl29MXoigZnXVV15EKC9ha/U21USSzkI5MvztidfbkopUhm8045LsC6jhYAJ1w1UeMh3RsPEgl462pW6QdbPFQySFY4KlN2jjYc3bEg2UKxbDPz8AvnKRQVd+3tp78tzPBU1kk+j9cwZgC3bO1meCpbln+q974fePo0N23s4JatXU7Ia7LOZiqZyZNI59nWG+O6tW08afe67D8+zJu3dOGxPYWPvn070ZCP9933NC8NzK7C68iFSXauirFzVZzDF6zy3qrkeI1hTicuTbG+M4Lf66EjYv0+zWWGe7MYw7FC0SGqKxUKuRz0A2g4sfBhtbFUjlzB2qnOlP85NZIkky+yc1Wc7liQqUx+VhU1hwcn2bkqXpY3cBNvYgc8H6RzRZTCKccFWNdpGQ7dGT5my5e7ERE290TrehzjqSzxkA+ft/SY6YmVh6Pc962VPP/6wXNc2x9na2+M/vYwUEqQ1wqfQfN5judOj3FmNMXP3rAWgHjYev/1ChJ0qHRVa4hbt3Zz6Nw4z7w2ylAi43xPgL7WEF/78M0EfV7ef98P+eHJkZr3q2Q0meXiZIadfXF29MU5finB5HSeXEHVDFW5PY6Tw0k2dVm9NU6oyngchvnmSoeqHnz2LP/jeyfmdK3uIbgSsxbcydmZ5DR0/8aOvjjdUWuX12y4qlBUHL2YaDgjO2bv1Ct3wA8+e5Y/+edjTX2fZkjaSVR3jmNNexiRktfl9gzc6MqqWoylck7fh6Y3HmR4KkO+UCSdK5DJF2mt6vOw/g9evZjgpYFJ7tq7BoD+Nttw2HmOWuEz63uE2N4bazjNEOB/Hxwg5Pew79o+wAoNQn0vXFdU9bWGuHVrF0UFf/ToUYAywwHWz+WhX7uZ3tYQv3j/Mxw8Uz3j5A+/8RL/7Kps1I2kO1fF2bnK0u76sX2d29trq1DIzRWKnB5JstnuBXJCVVdrjkNE9onIURE5LiIfq/H6ehH5jogcEpHvisga12ufEZGX7I/31rj28yJy5cR8lhlXOlT14IGz3P+D1+Z0re4huBKGQ5eDijRjOCbx2c17XbHArNZ4aiRJOldkZ53EOJQeZJUexzdeGODz33m15sNoLqQylpfkznEEfV5Wt4Y5M5JkOms94Gs9pDd3Rzg/ka5ZwVMrB9EdD6GUtRuuzIH0aI/D/j/4+sEBvB7hXdetBqjpcdQyZmCV5T5zarSuB5jJF/jHQ4P81DV9RG1Pq97PWzM4UfI49qxtIxr08dzpMbb0RFnVGq46f1VrmK/96s34PcLf//hc2WsD49N86enT/Nm3SxsArey7ww5VAfzwNctbcXscPq+HWNDn/PzOjqbIFZRTrNAS8BH2exldwJLcBTMcIuIFvgC8A9gFvF9EdlWc9lngAaXUbuCTwKfsa98JXA/sAW4C7hGRuOveNwDtC7X25Y5SyglVXSmPY3AizcXJzJySvXqNV8TjsHe723tjnBltrHl05EKCLT1RAj4PXbP0OHTFTL1SXCiVmVYad73T/NS3jsxLb4vjcQTLmxB1L0fpAV/9kN7kTAOs/lnVykFor+LSZMYpemiv8DguJSx59W88f55bt3bRbR/vi4fweoSBsWlyhSJTmXxZmaqbW7d2kc0Xeea12pItTxy5xMR0jrv29jvHonU8PM2FCctg9cZD+L0ebt7c6XyverRHAty8ubPK+/m+XfX18vlJjl20fheOXEjQFQ3SFQ2ysStCwOvhhyet9bdW/OzbIn7n78JdUaXpiASu2lDVjcBxpdRJpVQW+CpwZ8U5u4DH7c+fcL2+C3hSKZVXSiWBQ8A+cAzSHwO/u4BrX7J87O8P8fnvXJ7k9mQ678Tx6/2RzCfFonLCD3MRoNMPrrl0Zs8WHV+/YUN7WTlqLazmPctj0Iaj2QT5kQu1pUbc6Jh75Q54PJUjFvTxzGujPH7kUlPfrxF6+p/b4wCrsursaMoxVPVCVVC7sqqWR6DzGBcn007Rg56jHQn6iAZ9XJxM86PXRhkYny57sPu8HvriIQbGp0vNf5HaHsdNGzsJeD08eax2We7XDw7QFQ1yy5bSQ9/rEaJBX0OPoyMScLr8tcFoZDis17s5PZIq64t58lWrEsvrEf63LVNi5bys3ye/18PW3qiTXG+t8NzawgHGUlmKRcWjL18ASkYcoCu6sN3jC2k4+oGzrq/P2cfcvADcbX9+FxATkU77+D4RaRGRLuAOYK193keAh5VSg42+uYh8SEQOiMiBoaHmarqvBva/OsyPXmsu2VYPd+/GlehMHk5myNulkSeHZxdd1DMXvB6xE9cLK5l9aTJNLORjW2+MdK5Yt7ltPJVl0NW8p+PKzXpFhwcn2dRVLTXipl6Vz1gqy7tfv4aNXRE+/a0j5C/zZ6In8FV6HGs7WhieyjqhoVqhqvWdLXiEmpVVtRLqbq+i1sO/J2aVNX/94DkiAS8/uauv7Pr+tjAD49NMOEantuEIB7y8YWN71VwRsP7vHj9yiTv3rC5L3IPu1q+f49CGD+A9r1/Df77zGm7b1lPzfM0ttmHZb09WLBQVPzg+zFt39HDbtm6+8fwA2XyRVy9OlXmgO/riFOy/myrDYTec/s5Dh3jouXP88i0by87piCysXtViJ8fvAW4TkYPAbcAAUFBKPQY8AjwFfAV4GiiIyGrgZ4H/NtONlVL3KaVuUErd0N3dvWBv4EozlsrWHRvZLLpMz+eRWYWqprOFmjvwdK7Q8OF1caL0MD1xaXYeh565sMGu8mn2jyGZydes40/nGktKXEpk6I2HSuWodSqrdBf7DvsPPejz0hpuvntc9380QpexunfAehJfVzTA7/7Udl69NFUVPwfLy3PPEW9EqoHHATjyK7U8jpDfy9qOliqPI18okkjnq67RntmlRLrkybjCTT3xIGdGU3zrxQvsu3YV4UC5MetvDzMwNu3kvSoNk5tbtnRz5EKiSv/qm4cGyRVUmTejadQ7MziRZlVryXC0BHz8ws0byhoQa7GpK0J/W5j9tkzJy+cnGE/luHVrF3ft7WdwIs1XnjlDtlCe83J/Xmm021sCvDQwyd//+By//RPb+IN37ix7vTMavGrLcQcoeQkAa+xjDkqp80qpu5VSe4Hft4+N2//eq5Tao5R6OyDAMWAvsAU4LiKngBYRmVuL5lVIOmc9NC43L6Fjn2s7Wkg0ea9UNs+N//Xb/OOL1Y7eu/78+/zFd+tXTA3asWFrZzo7j0M/ILb2NFZPdZPOFXjTpx/nK8+eqXrtrr94ytElqsWlRIaeWJD1nVa8uF6C3KmA6Sv9cXdFA02t75uHzjMwPs01qxsbjkjAh0h5jqPUjR1g37V97F3XxheeqP7Z/+0zZ3jTpx9vqjzY8TgqDUeH9TN4wVabrfeQrlVZVTm1ThPweeiMBLg4mXFJkrg9jhAvDkyQyOS5+/rqB3t/W5gLk2nnodjIcDhluRVex9cPDrCtN1rz5x9r0K1/YTJNn8twNIuIcOvWLn5wYph8oejkO968pYu37+olGvQ54Wd3lZ3eWHg9UlbxBlbPiwj85zuv4bd+YmtVSXenneNYKH23hTQczwJbRWSjiASA9wEPu08QkS4R0Wv4OHC/fdxrh6wQkd3AbuAxpdQ/KqX6lFIblFIbgJRSassCvoclhf5Dm7hMj0Pv2jd2RZo2QiNTWRLpPGdHq+cVnBlN1Z1RAKX699etaZu14dCluDoX0MyO/uiFBBPTuarE6MR0jsODk3UTpmDF3ntiQfrbwngEztQZznRiKEk85HMSt4DdBNh4fX/z9Cl+4ysHuXFDBx+4aV3Dcz0eIVYRc9fCgW22btTP7F7NmdFU1a76yWNDTEznmtIiS9VLjtsexwu2tEa9sNCmrgivDSfLPDzHI4hUP9i7Y0GGEmkmpnOE/d6ycF2PKxH+xk2dVdf2t4cpFJXj8dWrqgKrI78zEijr5zg9kuS502PctXdNzf6ZeNhPIlP9N5HOFRhNZlkVn73hACvPkUjneeHcBE8eG+Ka1ZbaQMjv5R3X9jGSzOL3ipMzApz8WVvYX7XWX79jC9/49TfzCzdvqPn9OiIBMvmisymYbxbMcCil8lj5iEeBw8CDSqmXReSTIvIu+7TbgaMicgzoBe61j/uB/SLyCnAf8EH7fisa/cBPZPJO7HMu6N3ahs4Ik+lqeeZa6IfXdEXZZbGoSOeKDQ3Q4EQan0e4aWMHp4ZTs4rJ63DG1l7rD6qZHb3WgKqcP33Uftgcu5iouQara9wKVQV8Hla3het2j58YmmJzT7TsD7o7Vt49rpRi/6tDfOvFQb714iCfeuQw//EbL/O2HT088Ms3lonz1SMW8pflOCon8e1ZawnzHTxbUvNVSjlfV3pMxaKqUv5NZnSOo9zjaA37aWvx13zAu9ncEyWTL5bJgTTKQWhNqrFktm7y/M69q2uGgHQvx8vnrf/jRobD4xHevKWL/cdL0wy/fnAAEbhzz+qa18RCtZPjukR4Lh4HwJs2dyICj718gR+fGXPyHgB32Z7V5m6rQk/TGQ3SEwvW/Bl2RALsXtNW9/t12iHB0QVKkPtmPmXuKKUewcpVuI99wvX5Q8BDNa5LY1VWzXT/+iUpyxBdvghWwrTWbq4ZRpJZokFrt5wrWA/+ylhyJTruO10R+kjbchiNqrN0UnFLT5Rsoci5sWk2dEXqnu9GV97onVgzVUu6Me/E0BSZfIGgz2sftx42mXyRUyOpqoqmyek82XzR8SLWd7bUDVWdGJri1q3lubOuaLBMT+vA6TF+4a+eKTvn7uv7+cy7d+P3Nrdni4f9ZQUMleNSr1kdx+8VDp4Z56eusRLJ5yfSjoGtzNH88+GL/OrfPMe3P3qb8/5T2TwegaCvek3rOloYT03ULMXVuCur1tq5oVKpbfXvaE8syJELk/TGQ1Wx+y29Ufxe4T3Xr6m6Dkq9HC8PTOCzq6AacevWLh5+4TxHLybY3hvj6wcHeOPGTla3VfddgJXjqPW7rMOttfo1mqE9EmB3fytfevoUuYLiLa7fnTdu7GRjV4QbNlR3GLxhY4eTg5oNunt8OJlxPMf5ZEENh2F+cWvTjF+O4ZjK0hkNODuZielcE4bD9jgqDMe07Qo3aiQcnJhmVWvIqTM/OTzVtOHQD6D+tjAtAW9ToSptIPJFxfFLU1yzuhUol6w+PDhZZTgu2l3juhFtXUfEKXV0k0jnuDiZKQsrgOVxJGzZkZDfy4FTVoPe3//azUSCPgJeDxu7InUlRmpRWeWjd/L6gRvye9m1urWsGdD9eaXh0D+b8+PTzvtPZgp2PqV6Xes6Wjh0boLWBrmEzc4Y2SS3b7eOlWZx1DAcccszG01mq3Igt2/r5pn/8BN1f7e1x3F+Ik1XNDDjz1Ib9/3HhklmCpweSfHrd9SPbmuPQylVdm8dbp2rx6HX8sK5CUJ+D69fXzISHo/wD79xC35v9Xv53M9ex1zSFLrKb6E8jsWuqjLMArfhaBQaOjWcbCgrPZrM0hEJuLR5Zs5zTNm7nlRFzFR/3Wg9FyaspOKmLi3D3XxllZ65EA/7m8ohKGXFv9+4qQMoeR/689evb8fnkTIjotHhiF7b41jX0cJoMltVZaN7UTZ1lxu/LvuPVe/2D54ZY0NnC69f38GOvjibuqOzMhqgq3yqPQ63B7B3bRsvDkw44beDZ8YJ+jxs641yuiJHo8tm3YN+kpk8LcHaGwddWdXI4+iIBGhr8Zflr3RuqrJxDaxwVKGoODE0VdWHISINN0Qhv9f5OdfLubjpaw2xtSfKk68O8fWD5wj6PLzj2r6658fDfvJFVbVBGpy4fMOhw1M3buysCvtFgz7HM3YT8ntn3NTVosP+GS5USa4xHFcR7l8C9zD7Sv78ieP82y//uG7uYiSZpTMSdCQWmkmQ64dnZZWO/rpeJYpSyiljbI8E6IwEZpUgH5/OEQ9ZMxe6ooEZDcfghJV03XdNH0Gfx6l+KhQVRy8k2L2mlc3d0TKDorlU4XHoh2blrv2Ea2iOGx3iGp7KOHmGvesuT+AgHvKVGS49iS/sevDsXddGKlvg2EVrXR70kYIAACAASURBVAfPjPG6/lY2dUWrcjS6+sndHJZ0Tf+rRFdWNcoliAhbuqO8erH0Mx1PWb03bmVcjU6Aj6dyTvPfbNBeR6OKKje3bO3imddG+eahQX7ymr6GuaVYjRJosDY/sZBvxtBYI65f18723hh37a2dX5lPOm2F3OEFkh0xhuMqwj1VrNHD/uxoikQ6X7e5b2QqQ2ekFKpqpnt80kmOV4SqbMMxnSvUHHw0MZ0jky/SZ8eGN3fXV1OtxZhLAbWZmRfak7i2v5XtfTFHWvvMaIrpXIGdq+LsWBVzDIobrVPV4/I4gLKOX7A8Dp9HHMOicXePD4xPM5TIOFPl5ko8XB5zt2Q8yqts9q61jNPBs2Nk80VesqfZre9s4dzotFNIUSwqp/rNvQlJZQt1PQ4dH6/V/Odmx6oYR2wZcLC849Ya1UAA3bHSrr2RJ1MPneeYaU2at2ztJpMvMp7KcXeN3g038TpNl4MT0/TNsaJKE/B5ePS33+KINi4k4YCXloDXhKpWGv/zyZO8eK5cy3/UlpqAxoZDV7e4p6VptE5VRzTgyHY343HoUFW9HAfUDnk5Lr79R7e5J1J3fkMt3HpHlVVLtdCexPa+GDv74hwetB5mh52+izg7V8U5P5Gu8touJdJEgz6nukgbhqpd+9AU6zpbqhLcJcORcabD6aqnuRIL+ZhyNTPWkvFY2xGmMxLg4JlxDg9Oks0X2buunXWdLWQLRUfuZXAy7fz/lXkcmXxV85+mmVAVWP0HiXS+KQHC3niphLlZr8GN9jgaeUFubtrUgd9reawzyYM4isQ1PI7LCVMtBgupV2UMxxJEKcWnvnWYvztQ3sA2lsw6SeV6vRyFonLkn93lkZrJ6Tz5opq1x+FUVVXmOFyGpNZ9LlTEhjd3RxlNZpueyex+AHVFg4ylsg3LeQ8PTrK2I0ws5GfHqhijySxDiQxHBifxiFXWq+vjK2eY6+Y/TSzkpyMSqKqsOjE05eRr3HS6chw6z9BINr0Z4iE/RVUSIqwlJS4i7FnbxvNnx53E+J61bU6YSa9fh6lEymc1pLKFuiGY3liIu/f2c/v2xrIaullNl0CPT1fLjWjcvS+1ciAzUQpVNXdtS8DH/3PrJv7dT2yrkhipRG+mKjdBFybLu8avBjqjQWM4VhKpbIGiKj10NaPJLD2xIC0Bb10v4eJk2tGFGqgx+lQnRTujgbq7q1rUq6pKuwxJrTW5paihlFBuVrPK/QDqigVRqnHCzxIetB5i+mF2+EKCVwYTbOqOWlVIzkOuPFx1aTJd9lADK1zlbqLLF4qcGk6xuae6KswtO6LzDIEaJa6zoTLmPp6qlioHK89x/NIU3zs2RG88yKrWkCtHY3l4OkS4vTdWnhzP5mmpk4D1eIQ/ee8e3rCho+E6t9vGWHt2Y8na6wTr56Q3A3PyONqbC5+5+d19O/jgG9fPeF4tmZdcwdIs65tjKe5i0RkJLJjsiDEcSxD9SztYYTjGUlnaIwHawn6n3LESt5dRy+PQO5DOSBC/19PQCLmZqleO6/Y4ahigCxPTeKS0y3Rq/pusrBpP5hzPqFvv6PUc7GSW9933NC+ft0J66VyB14aTjsHYaRuQw4OT9khO6+vuWJCOSKAqQa6b/9ys72wp64o/NzZNtlCsSoxruqIBzo+nnTzD5VK5A663k9dJ+O8eG2Lv2nZEhFWtIXweKXkcQ1PEQz629sbKcxx2Oe7lEA36WNfR4nhx1rjT+g/2XjvP0Wy4yc1sQ1WzoVaOYyiRQSmuPo9jAYUOjeFYguiw0MXJao/DKqP1133Y67xGwOupbTjs2LYu12sN1254ql5T7eS4uzy3duOUtYvX+YA17S0EvJ6mEuS5QpFEJl/yOCpmXnznyCV+eHKU//QPr6CU4tjFBEVV0o9qbfGzujXEM6+Ncm5s2glRiQg7V8XKSnKVUlyazFTN/75hfTsD49O8Yncq16uo0nRFg/zo5AjZfJE9ay9/ZIzb41BKMZbK0VZDSnz3mlZEQCkcg+XzeuhvL3W/n7hkTYnrjATKkqbJbP1y3Nmwc1Ws5HGkqrvC3fTYeY65JMe398X4zbdt5e27eue20AbEagxzmo9S3MWgI7pwelXGcCxBEnYiengqS8buzNZT2NpbrNxEvRyHNhbXrW2tmRzXOxD9EI6H6hshN5N1chzu8txa97GE4UouvtcjbOyKNGU49P10rX+p3NV6D7pXRc+lqDUcacequDOTYVeFZPXRiwmn4iiRyTOdKzgPNM07d6/G5xG+ftBSoD1ZY2iOmy67CRCYH4/DtQNO54pk88Waw4tiIT9b7YY+d0LeHWo7OTzF5u4oXdEAiUyeTN5SO05m6pfjzoYdfXFeG0kykcqRyhYaGgX9fzmXclyvR/jo27fRE5v/B3nI78HvlbIcx4WKcOvVQlckSDZfdApb5hNjOJYg7t2ObkobTWlPoaQfVItzY9N0RAJs6YnW8ThsVVH7Ydwa9s+qAXA6Vy6t3kxVVaUw3OaeCMdrzKn+xvMD3PHZ7zplvU4TWbiUHAeraqlYVHz/1WHeuXuVM5fipfMTtAS8ThktWLtgnfPZUSZZHSedK3LKbpBzmv8q1toRCXD79h6+8fx5p2mtMxKoG4bRs8d1nuFycXscjSbxgdUn4PMIr1vT6hzTsinubvcOu8Z/NJklky9SVMyTxxFHKXjmlCUi2ShUtbo1jNcjCxJuuhxEhFiFtLqWG7ncctwrzUI2ARrDsQQp/6W1djtucbvWsN/RcKpkYHya/rYw/W1hhqeyVQ17I8ksMVeXajxcX0a6fE3WOUVlaT1pUrkCfq8Q8HlqGrOLNcoYt/XGOD2aqppT/b2jQ7w2nHQS5+MVMxciQWuW8nAiw+ELk4wks7x1e48zl+Lvnj3L9r4YHpc4nk6Ut7X4y/7wncoq20vRzX+VyXGw9KUuJTI8dWLYEjesE6ZyX6/zDJeLO8fRaBIfwG++bSt/9a/eUFZau77DUkA+eMYqD97cHXGqv0amss7MjvnwOPT8iKdODDdcJ8AvvWkDf/VLNzSt2XUliVdIq58dTREL+prqVF9KXLe2ld/bt+OymhbrsfT+1wxOIhpKux29a+iwd7v1cxwpy3DYTVKVXsdIMus8OKC5UJVSiqlM3pkJ4DZG01lLl8kShys3BIl0jkQmX2U4dvRZO1Pd6aw5bCdWS3Hy6gdlV8zqHtczDW7d2uXMpcjki1Xlr/phtqMvVvYg39ITxesR53tdcpr/qneVb93RQyzk4+s/HuDEULJmRZWzPvtnu2cewlRQ7nGMO/Mrau/kV7eFuW1bufCibuDTI2Y3dUcdAbyRZNbJUdWrqpoNa9tbiAS8PH3CmlDZqGKqOxacscR3saj0OE6PpljX2TIvG4EryZaeGL92+2ZHKXc+MYZjCeIOVen4qhOmsPsv0rlilTehlLI8jvYw/W3WA6MyzzGazDguLGj11caGYzpXoFBUzm7aXUmVzhUI+72W51JxH53crwzZ6Ie5uxw2my9y/FLCPm73AqRKXpamKxpkaCrD/leH2N4boyceQkT4Dz9tTUC7zhWmAUs6PhbycV1FI17I73XUUk8NJx2Pozde/UcW8nv5md2r+OaLg4wmsw09Dq0Oe+PGxuWrzRL0eQn6PExO52oa0pnQYbsnjl5yut31g2RkKuP0h8zHrtTjEbb3xZzKqqtth66xfpdLf4NnRlJVKgErHWM4liCJdA4RiAS8TqjK8ThaSh3flVVMo8ks6VyxsccxlS3bgcTDloheo/ke2pBpDSd3JVUqW6Al4K1ZnVXZNa7RO9PDLsNxcniKXMFawyv28Vqhme5okHNj0zx7aqysC/gNGzp49N+9hXe/vlzOwef18M3fuIXffOvWqvf16Xe/jlQ2z3u++DQ/OD5C2O+t+wC9a+8aJ/dSKW7o5uZNnXzrt27l+svUqHITC1nS6jo8OZveB204To+knG53d+xbz+JomadwhrswYa7qzYtNLFjyOApFxdmxlLMhMFgYw7EESWTyRAM+VreFSx5HMovHVoltqyMVoo1Ef3uY3lgQr0eqPA5L4LD0B613hVMNmgD1H5EuVXUnxKdz7lBVbcNROcNA70wPX3Ar11rG4ro1rc6OdSyVrZq50BULcnokRTZfLBuGA1aZZq2Y+frOSNWQIoDda9r42offRMArfO/YED3xYN1wxA3r21nTXtLbqodV6nt53eKVaG9OG9LZ7OQjQZ9TVKDXHQ/58HvFDlXpHMflh6qgNIMd5lZquxRw5/0GJ6bJFZTThW+wMIZjCZJI54mFfPS1hhi0wz2jttSE1yPOg6OyCVAbif62MD6vh754qMzjKBYVY1U5jpml1bXHoUNV6cpQVcBbs7dEG73KElewdqaHByedCq0jgwkCXg8//bpVDCUyDE9lGJ/OVQn66YdgwOvhpo3Vo0Vny5aeKA/92pvY2hN15prXwuMRPnDTOrpjQda0X9ndZ8yWVh9PZRtO4quHDrNowyEilo7RVMZJjtfTqpotu+wwZKWC79WEO8ehlZFNqKocYziWIIl0jljIqgK6YCfHx5IlzSb9b2UvhzYSemfc3x4u8zgm0znyReWUYwJNCR1qw9FbJ1QV9ntprYgLg+VxdEYCNR90O1ZZonjnbePyyuAkW3ujXNtvD10aTDBuK6y60d3jb9jYPqc5BbVY3Rbmkd+6lb/4+esbnvdrt23m+793R82RpguJVeWTK1MKng3r7TCLO8TWEQmWhaoq543Ple2uKrarLZmsiYf8JLMF8oWio4y8zoSqyjCGYwkylckTDflY1RpiKJEhXyhaXeMt5QNsKj2Oc2PTROx8A8CatnCZx1GSG6kOVTXqHndyHDWS49N2jkOHqtw9Hhcmput22+6qSJAfuZBgR1/cKZM9PDjJeCpXFc/XHsctW8qrhy4Xv9czo66UiNQctrPQxO0d8Hgq13ASXz10fN4dYrNmm5RCVfPlcWjpkaXWnzEbdCXbVCbP6dEUfq/UHTW7UllQwyEi+0TkqIgcF5GP1Xh9vYh8R0QOich3RWSN67XPiMhL9sd7Xcf/SkResK95SESW3dzxUqgqTFFZ2kxapwooG/nqRldU6Z1ef3uYC5NpR01Wy41UluPWupebqYzOcVhGoDJUFfJbxipfVGXeiB7gVIttvSUDMTyVYSiRYeeqGJ3RID2xIIcvTFryGhUPoNetaWVbb5Sffl39KW7LDV3lM57KzsnjeMu2Ll7X3+oYZbDKukeTWZLZ+fU4AN513eqymdpXG6XikzxnRlKsaW+54l7mUmfBDIeIeIEvAO8AdgHvF5FdFad9FnhAKbUb+CTwKfvadwLXA3uAm4B7RERn3X5bKXWdfc0Z4CML9R4WC8tw+J2H7uBEmrFUyeOIhfyI1DAcY9OOABxYuY5CUTnzkkdtRVR3Oa6WtZ5NjqNWqKqWHHXletzEQn7WdoQ5fCFRJRWyY5U1R2OihoT4mvYWHvvt21jfuXKSlTrmPpP+Uz1ev76Df/iNW8oKBDojQUamMqQyeUSY13zEPT+1nT/4mco/9auHmCvvd3o0aSqqarCQHseNwHGl1EmlVBb4KnBnxTm7gMftz59wvb4LeFIplVdKJYFDwD4ApdQkgFjb6jAw/wpei0winSca9DlhnsHxNGPJnONxeD1CLOhjomIQkfY4NNq91nkOLQ7Y5S7HDc08GGqywnBUVlXpUJX7PhPTVvOfez2VWIOWJh2xQb0j3rkqxvFLCUaSc9thLzfiIR/pXJGhRGZWUuKN6IwGSGYLjKayRAK+qzYfsRA4+mDpHKdHUk6OyFBiIQ1HP3DW9fU5+5ibF4C77c/vAmIi0mkf3yciLSLSBdwBrNUXichfAxeAHcB/q/XNReRDInJARA4MDQ3Nx/u5YiTSOeJ2jgMsRdZsoUiHSxW1snt8KpNnYjrnNP4BZb0cxaLiq8+eZU17uMxwRAI+PFJ/ZjhYpbrWZDxrV1qW48gVCLnyKvo+pQqv+n90O1bFOTWc5OCZcXpiQae/ZGdfnFxBkckX5+1BeTUTcx5k+bozLmaLznOdHZ2el67x5YT2OM6NTpNI501FVQ1mNBwi8jkRuWaBvv89wG0ichC4DRgACkqpx4BHgKeArwBPA87TSin1r4HVwGHgvZU3tc+5Tyl1g1Lqhu7uqyfems0XyeSLxEKWNk7I73Ekvd2J4taKmRzOg7q9PFSlX3v4hfO8fH6S3/mp7WXxWo9HZuweT6RzRIM+Al4PHil5HIWiIpsvOp3jUEqyu3tK6rFrVYyigm8fvljW++D+/GpOss4X+mcLcxt8VIsOx3Ckava4rGT0Jugle86LqaiqphmP4zBwn4j8SEQ+LCKtM15hMYDLSwDW2McclFLnlVJ3K6X2Ar9vHxu3/71XKbVHKfV2QIBjFdcWsMJf725yPVcFWoU2GrTCB33xkNNJ7c5NVCrkDoxbZYPunELI76UrGuC14SR//OhRru2P8y92r676njPpVelkvYgQ9nsdj0P/WytUpacP1stxQEmAMJMvlinXbuqO4Pdaxq2WhPhKIxZ0e5rz5HHYBRLnxozHUYn2OF4csAzHSsqnNcuMhkMp9ZdKqTcDvwhsAA6JyN+KyB0zXPossFVENopIAHgf8LD7BBHpEhG9ho8D99vHvXbIChHZDewGHhOLLfZxAd4FHGnurV4d6MYjHZ7oaw05TUjtFRpT7j6OgXErAb6mYoff3xbm4RfOMzA+zcffsbNMObZ0L1/DctypTN75YwoHfCXDYXseYb8rVJUueRxBn8cR/avFuo4W56G10yVO6Pd62GI345kcR6nKB2Y3LrURnXYvT7ZQnBdl3OWEVirQagbG46imqRyHXSG1w/4YxspBfFREvlrvGqVUHqvi6VEsr+VBpdTLIvJJEXmXfdrtwFEROQb0Avfax/3AfhF5BbgP+KB9PwG+JCIvAi8Cq7CqsZYNuoJJP6jdch0dFaGqiYpQVcDrceZBaPrbw+SLirds6+bNW8olOurdq3pNOaK2IQsHPM6ccV2WG/J7S5UoOsdhy7s3Srpq6RGgSqZDCyGaHEfpdwHmz5B2uAz6fMziWE74vB4iAS/pXJHuWHDeGk2XEzNuNUTkT4Gfwap++q9KqWfslz4jIkcbXauUegQrV+E+9gnX5w8BD9W4Lo1VWVV5vAi8eaY1X81owxG1HxbuBjq3x9FmP+yVUogIp0eSrGoLVXkU6zoiiMDH9u2o+z3jIT8XJ+sPtU+k86yxd11hv9cpxy1JcvucP7ZSqGq6YX5Dc83qOK+cn6wSDrxmdSv/+8cDDT2WlUK5xzE/hiNm56yMx1GbeNjqHjcVVbVp5jfmEPAHdllsJTfO83pWPDpUpXMGurLK6xGndBZwGu6S2QItfi8/em2U27dVFwH8yq0buX17N7tW1xfem2nueCKTd753WajK/jcc8JTu4wpVNSP291tv28Zde9dUiRP+/E3r2NYbdRR5VzJuj2O+PDCtV3VhMm1yHDWIhXwMTpTmmRjKacZwjLvPE5E24Hal1P9RSk0s2MpWKJWhKi1J3l6h/ePoVU3nODWcZDSZrVKLBatno2uGQS61BArL15Rz4r5hv6cqx6G1qPR90rkCw1PZpmQaumPBmlP3Qn4vt17F3cfzSTTgQwSUmt8ZF51Ry3CYqqpq9MbNqOLWppkcxx+6DYRd9fSHC7eklY27qgpKOY7KMkxHryqVdabh3VInhzET8ZCPTL56MBRArlAknSs6yfqw3+sYjOlcuc5R3PZczo+XVHoNl4/HlpaPBX3zOmpVV+nNp9zIckFv3EwPR22a+S2sdY7ZoiwQtaqqoHoojlvVdv+rQ+zoi805rFNZEeVmqsIDaimrqrI0sLRcRdweONRMD4dhdsRDftoi81thppsA50vgcDmh/76M3EhtmjEcB0TkT0Rks/3xJ8BzC72wlUoinSfoKym1dkYC+L1SVlEFpf6GCxNpDlRMw5st8Yqu78r1QMmQhco8jlI5rnUfq6zXPRfEMD/EQr5572nRnfrzNcRpOWE8jsY0Yzh+A8gCf2d/ZIBfX8hFrWQSrp4JsMIUe9e1c01FcluLE/7zKxfJFoqXlQ9wa/NUoo85OY6AO8eRt49ZDx6dZB8Yn8Yj1JVUN8yea1a3cm3//E4W1KGq+Robu5zY0RdnS0+0bASBocSMvzF2NVWVJLphYdDKuG4e/NWbq87TmkWPH7lEwOfhxo0dc/6euvS31vhYnXOJu0NVlR5HoBSqSmTynBlN0RcPzWs8fqXzuZ+7bt7vqUudTTluNR9843o++Mb1i72MJUszWlXdIvLHIvKIiDyuP67E4pYjuUKRz3/nVSeXUYk1/W/mP+SWgBefR8jki9y0sWPW40TdaG9CG4ny9dQIVeUKKKWqcxy2MTsymDD5jasAPQnSNAAaZkszW8IvY8l6bAT+E3AKS07EMAcOnhnnT/75GI8fuVTzda1EOxMipdnjc62m0jiGo4bHoQ2c9kq0kUjniqRyeQI+jyOaqNdzYmjK5DeuAq5b08oN69u5pol+G4PBTTOGo1Mp9VdATin1PaXUvwHeusDrWrZoMcLRZLbm61pQsBl0nuNy+x3cozIr0cccrSq/9SsznSuQtoc4aXQ4K19UxuO4CuiJh3jo195kmiwNs6YZw6FjKoMi8k4R2QvMPaC+zHj21Ci/+9ALFIvNzZPSFUd6jGslVqiqubLL1rCfrmiwbCToXIg0Faoq5TjAMhzTuXLD4W5OazSHw2AwXN00s7X9L7aU+r/HGpoUB357QVd1FfHw8+d58MA5fulNG7hm9cyK87rHYaSex5FpLlQF8K/etIFcQdVUvJ0Nfq+HkN9T03BMpnMEvB6CPstAhOxE+HQ2TypbKJOrcGsqGY/DYFi+NHxC2aq4W5VS3wQmsCbxGVycGJoCYP+rw00ZjnOOx1EtKlgsKqZculAzceeeyoGKcyca9DvehZupitCZ9jCms1anuTspX2Y4TI7DYFi2NAxV2cOS3n+F1nJVcnLI0n78vi37MRPa46iV40hm8yhF06Gq+SQW8tUNVbkNh/YwnFBVoF6oyhgOg2G50szW9gci8udYzX+OQq5S6scLtqqrhKlMnguTaUJ+D8+cGmU6W2io3a+UcnScahkOR6eqSY9jPokGfUzVKBG2ZnGU1qM9jJQdqnKH1SIBLx6xdLXMDAODYfnSzBNqj/2ve2CSwlRWcdIOU921dw1feeYMz5wa5bYa0uaakWSWdK6I3ysM1whVVSairyTRYG2PYyqTLxtdWirHLTCdLZQp74pY88tNfsNgWN40Mzr2jhofK95oQCm/8YEb1xHweth/bKjh+bqiakdfnMl0nmy+WPZ6pcDhlSQa8tXMcTQKVaUrqqoAemJBNpgZzQbDsqaZCYCfqHVcKbWsRrbOhROXknjt8adv2NjO9483znPo/MbuNa28ODDBWCpLr6uG3pn+twjaQbE6HkelBIoOQaWyhaqqKoAvfvD1ixJqMxgMV45m+jiSro8C8A5gwwKu6arh5PAU6zpaCPg83Lq1myMXElyaTAOQzRc5cmGy7HztcexeY1VfVfZyaMPRbFXVfBKtmxwvl0AJOVVVVnK8UupkU3eUnphpKDMYljPNhKo+5/q4F7gd2NTMzUVkn4gcFZHjIlIllCgi60XkOyJySES+KyJrXK99RkResj/e6zr+ZfueL4nI/SJy5eM6NicuJdlsz8rWsh/7Xx0mmcnzy196ln1/tp/DgyXjMTA+TTToc0I5I8nyPEelLtSVxEqO51Gq1MiolFUeXCtUldahKpMENxhWHHORL20B1sx0kt0D8gUsD2UX8H4R2VVx2meBB5RSu7GS75+yr30ncD1WYv4m4B4R0YI6XwZ2AK8DwsCvzOE9XDaFouK14SSbu6MA7FoVpzMS4B9fHOQDf/kjJ2x14NSoc825sWn628LOHITKyqqpTLku1JUkGvKRLyoyrrxLMlugqMqT9X6vB59HSKTz5AqKlssQVzQYDFcnzajjvmh7BIdE5GXgKPBnTdz7RuC4UuqkUioLfBW4s+KcXYBW2n3C9fou4EmlVN6WdT8E7ANQSj2ibIBnaMKILQTnxlJkC0XHcHg8wi1bu3j8yCUOD05y3y/cQHcsyMEz4841A+PT9LeHHTnr4RqhKo8szmAdnVdxJ8innJxLuQcU9nsdo2c8DoNh5dGMx/EzwL+wP34SWK2U+vMmrusHzrq+Pmcfc/MCcLf9+V1ATEQ67eP7RKRFRLqwOtbXui+0Q1S/APxTrW8uIh8SkQMicmBoqHG101zQFVWbe0oVRHdfv4b+tjAP/JsbefuuXvasbeP5sy7DMZaivy1MPOTH6xFGa4SqokEfIpcnITIXakmrT0xbHlA8XO4BhQMlw3E5cu4Gg+HqpBnDsQoYVUqdVkoNAGERuWmevv89wG0ichC4DRgACkqpx4BHgKeArwBPYyXm3fwFlleyv9aNlVL3KaVuUErd0N19eeqxtdAd45u6os6x27Z184OPvZU3buoEYO+6Nk4OJxlLZkmkc0ym8/S3h/F4hI5IoCpUVWuI05WilrT6WMpaX+XY2nDAy6j9WmVVlcFgWP40Yzj+OzDl+jppH5uJAcq9hDX2MQel1Hml1N1Kqb3A79vHxu1/71VK7VFKvR0Q4Ji+TkT+EOgGPtrEOhaEE0NTdEQCtDcYLbl3bTsAz58bd0pxtRRHZyRQI1TV3BCnhSBaQ1p9zDZsle+xLFRlPA6DYcXRjOEQ5Sq1UUoVaa7j/Flgq4hsFJEA8D7g4bIbi3SJiF7Dx4H77eNeO2SFiOwGdgOP2V//CvBTwPvttSwK7oqqeuxe04pHrOFNuhRXd1XX9zgWx3Do7nC34dBeRUeF4Qj5vYzaRi9kPA6DYcXRjOE4KSK/KSJ+++O3gJMzXaSUygMfAR4FDgMPKqVeFpFPisi77NNu9IyNlAAAFehJREFUB46KyDGgF7jXPu4H9ovIK8B9wAft+wF80T73aRF5vl6D4kJzYmjKSYzXIxL0sa03xvNnSx7HGu1xRINVCrlW6esihaocj6OkV6U9jraW8jW1BLwkbANjqqoMhpVHM9vbDwOfB/4AS6PqO8CHmrm5UuoRrFyF+9gnXJ8/BDxU47o0VmVVrXsuelvyeCrLSDI7o+EA2LuunX88dJ4dfTECXo+j7dQZCVTN5Eikc2zsWhy5jlo5jtFkjmjQ58zi0LjDU6aqymBYecz4EFZKXcIKMxlsTtiJcXdFVT32rmvjK8+cYf+rw6xuCzlDlzojARLpPJl8wXkwL2qoyv6+iUx5crw9Uu0BucNTJsdhMKw8munj+JKItLm+bheR+xd2WUsbXYrrrqiqx/XrrB/d4cHJMtXYDruXYyxZCg0lFjFUFfRZjX3lHke2qqIKysNTxuMwGFYezeQ4dutKJwCl1Biwd+GWtPQ5OZTE7xXWNCEfvqkr6uzm3cONOu2Es5Ydmc4WyOaLi+ZxiEiVXtV4KktbDcMRNh6HwbCiacZweESkXX8hIh00lxtZtowls7S3BPB5Z/7xeTzCnrWW19Hf1uIc17IjWujwFVvTamvPzF7MQqH1qjSjqWxVRRWYHIfBsNJpxgB8DquC6WtY/RTvAf7rgq5qiVMp/DcTe9e2sf/V4fJQlf1A1iW5B8+MAbBnXVv1Da4Q0aCvPMeRzNE+g8cR8hnDYTCsNJpJjj8gIgcoTfy7Wyn1ysIua2kzmc7NKhdx48ZO4HhZ30dXxPI49CTAg2fH6W8LL6okeSxU8jgy+QJTmTwdNZLj2uMI+T1Ost9gMKwcmto224biFRHZDHxARL6mlLpmYZe2dJlt9dObt3Tyj795C9esbnWOxcM+fB5xPI7nz4yzdxG9DbA8Dt3NPp6ykva1OuO1x2HyGwbDyqSZqqrVIvLbIvIs8LJ9zYouz51tqEpEyoyGPtYRCTAyleXSZJqB8WknF7JYREN+JzmuDVqtqiptMIzhMBhWJnUNh60u+wTwXaAT+GVgUCn1n5RSL16h9S1JEumcI9FxOXTYTYAHbQXdvevaZ7hiYYkGvY6sej2dKnB5HCYxbjCsSBptm/8cS5X2A0qpAwAiohqcv2JIpPPzMmypMxpgNJnh4Jlx/F7hmtXxmS9aQKJBnyM5Uk+nClwehzEcBsOKpNHTbxXws8DnRKQPeBBLQ2pFUygqUtnCvPRbdEaCvDA2zsEzY+xa3brosy2iQT/pXJF8oVjyOBpUVZlQlcGwMqkbqlJKjSilvqiUug14GzAOXBSRwyKyYstxp+ZxLnhHJMBwIsOhcxPsXeT8BpSEDpOZAmN2crxS4BDcHseKbucxGFYsTc0cV0qdU0p9Til1A9Z41/TCLmvpMpm2Hqix4OU/NLuiAZLZAtO5wqJXVEHpPSUyOUaTWeIhH/4aTY4lj2MuI+sNBsPVzqyffkqpY8AnF2AtVwW66mg+QlUddi8HlIY+LSbuYU6WwGHtIVWmqspgWNmYLeMsScxjqKrTFjrsjARY2zGz7tVC45ZWH7VlVWpRqqoyoSqDYSViDMcsSdihqnmpqrJ39HvWtiGy+B3YUZe0+lgdnSowHofBsNKp+/QTkesbXaiU+vH8L2fpM5+hKi10uBTyG1DKcUyl84wlc2zvrV0erA1GiynHNRhWJI2efp9r8JqipF21ophMz5/h2NDZwsffsYO7r19z2feaD9w5jtFktqZOFYDP6+HTd7+ON27qvJLLMxgMS4S6Tz+l1B1XciFXCwmnqurycxwiwq/etvmy7zNf6BzHUCLDdK5QNzkO8L4b112pZRkMhiVGUzkOEblWRH5ORH5RfzR53T4ROSoix0XkYzVeXy8i3xGRQyLyXRFZ43rtMyLykv3xXtfxj9j3UyLS1cw65pOpdB6fRwgtw1LUiJ3sPjuaAmrrVBkMBkMzIod/CPw3++MO4I+AdzVxnRf4AvAOYBfwfhHZVXHaZ4EHlFK7sUp8P2Vf+07gemAPcBNwj4jogPsPgJ8ATs+0hoVAy40shWT2fOPxCNGgj7NjluFo5HEYDIaVSzPb5vdgdY5fUEr9a+A6oLXxJQDcCBxXSp1USmWBr2I1D7rZBTxuf/6E6/VdwJNKqbxSKgkcAvYBKKUOKqVONfH9F4REOrdo412vBNGgj7Oj00BtnSqDwWBoxnBMK6WKQN7e9V8C1jZxXT9w1vX1OfuYmxeAu+3P7wJiItJpH98nIi12OOqOJr+ng63ue0BEDgwNDc3m0oZMZfLzkt9YqkRDPs5PWIajXh+HwWBY2TRjOA6ISBvwP4HngB9jqebOB/cAt4nIQeA2YAAoKKUeAx4BngK+Yn+/wmxurJS6Tyl1g1Lqhu7u7nlarlVVNR89HEuVSNCHsjWQ22voVBkMBkOjPo4vAH+rlPq39qEvisg/AXGl1KEm7j1AuZewxj7moJQ6j+1xiEgUeLdSatx+7V7gXvu1vwWONfWOFpipdJ7VbYs33nWh0b0cItAaNobDYDBU08jjOAZ8VkROicgfichepdSpJo0GwLPAVhHZKCIBrKmBD7tPEJEuEdFr+Dhwv33ca4esEJHdwG7gsebf1sKRyMxu3vjVhi7JbQ378dUQODQYDIZGsur/n1LqZqwQ0ghwv4gcEZE/FJFtM91YKZUHPgI8ChwGHlRKvSwinxQRXZV1O3BURI4BvdgeBtbcj/0i8gpwH/BB+36IyG+KyDksD+aQiPzl7N/23Emk887DdTmiw3CmFNdgMNRjxiegUuo08BngMyKyF8sr+AQwo96EUuoRrFyF+9gnXJ8/BDxU47o0VmVVrXt+Hvj8TN97IVBKMZWe3bzxqw1tFE0prsFgqEczfRw+EfkXIvJl4FvAUUqVUCuKdK5IvqiWdahKG0VTUWUwGOrRKDn+duD9wE8Dz2D1YXzI7qtYkcynMu5SRXsc9XSqDAaDodET8OPA3wL/Xik1doXWs6RJ2Mq48eVsOEImVGUwGBrTSORwRarfNiIxj8q4SxXH4zChKoPBUAdTbzkLnFDVMu4cjxmPw2AwzIAxHLNgagV4HHE78d9pDIfBYKiDMRyzYCWEqvaua+eTd17DLVuvuGK9wWC4Sli+T8AFYHIehzgtVbwe4Rdv3rDYyzAYDEsY43HMAj1vfDmX4xoMBsNMGMMxCxLpPJGAF69n+Q1xMhgMhmYxhmMWJNI5420YDIYVjzEcs2Aqk1/WciMGg8HQDMZwzILEMhc4NBgMhmYwhmMWTC5zSXWDwWBoBmM4ZsFUOuc0yBkMBsNKxRiOWWBCVQaDwWAMx6yYyphQlcFgMBjD0ST5QpFUtmCqqgwGw4rHGI4m0V3jJlRlMBhWOsZwNIkWODQNgAaDYaWzoIZDRPaJyFEROS4iH6vx+noR+Y6IHBKR74rIGtdrnxGRl+yP97qObxSRH9n3/DsRuSL639pwLOfpfwaDwdAMC2Y4RMQLfAF4B7ALeL+I7Ko47bPAA0qp3cAngU/Z174TuB7YA9wE3CMicfuazwB/qpTaAowBv7xQ78GNHuJkchwGg2Gls5Aex43AcaXUSaVUFvgqcGfFObuAx+3Pn3C9vgt4UimVV0olgUPAPhER4K3AQ/Z5XwL+5QK+BwdHGddUVRkMhhXOQhqOfuCs6+tz9jE3LwB325/fBcREpNM+vk9EWkSkC7gDWAt0AuNKqXyDewIgIh8SkQMicmBoaOiy38xKGOJkMBgMzbDYyfF7gNtE5CBwGzAAFJRSjwGPAE8BXwGeBgqzubFS6j6l1A1KqRu6u7sve6EmVGUwGAwWC2k4BrC8BM0a+5iDUuq8UupupdRe4PftY+P2v/cqpfYopd4OCHAMGAHaRMRX754LRcKU4xoMBgOwsIbjWWCrXQUVAN4HPOw+QUS6RESv4ePA/fZxrx2yQkR2A7uBx5RSCisX8h77ml8CvrGA78Ehkc7j9wpB32I7aQaDwbC4LNhT0M5DfAR4FDgMPKiUellEPiki77JPux04KiLHgF7gXvu4H9gvIq8A9wEfdOU1fg/4qIgcx8p5/NVCvQc3iXSOaNCHlZ83GAyGlcuCxl2UUo9g5Srcxz7h+vwhShVS7nPSWJVVte55Eqti64oyOZ0nHjb5DYPBYDBxlyYZS2Vpb7kivYYGg8GwpDGGo0nGUlk6IsZwGAwGgzEcTTKWzNHWYkJVBoPBYAxHk4wms3SYUJXBYDAYw9EM6VyB6VyBdhOqMhgMBmM4mmEs9X/bu/sYuao6jOPfp7td21JpS1srdoGWtEELlpdUwosKFv8AJYBiLA1GQlAMoqIRFfxDI5EoSBSqDQlvCgnhJRW1GASxVG0UkGJ5KxWotUJrgSW7RdkStl1+/nHOboftznQundtpZ59PMpm5Z+7MnNPTzLPn3Dvn9gH4GIeZGQ6OunT3puDwWVVmZg6OuvT0pnWqPOIwM3Nw1KV7cKrKZ1WZmTk46tDjqSozs0EOjjp09/YhwQQvOWJm5uCoR8+WPiaMHU17m/+5zMz8TVgH//jPzGw7B0cderb0ebkRM7PMwVGH7t6tPhXXzCxzcNShp9dLqpuZDXBw7EREeEl1M7MKDo6deH1rP29se9MLHJqZZQ6OnRhYp8pnVZmZJaUGh6STJT0jaa2kS4Z5/iBJyyQ9IemPkjornrtS0mpJayQtkqRcviDvv1rSFWXWH7avU+URh5lZUlpwSGoDFgOnAHOAhZLmDNntKuCWiJgLXAb8IL/2OOB4YC5wGPAB4ARJk4EfASdFxKHAuyWdVFYbwOtUmZkNVeaI42hgbUSsi4g+4Hbg9CH7zAEeyI+XVzwfwBigA3gHMBp4CTgYeC4iuvJ+fwDOLK0FeJ0qM7OhygyO6cALFdsbclmlx4FP5sefAN4paXJEPEgKkk35dl9ErAHWAodImiGpHTgDOGC4D5d0vqSVklZ2dXUNt0tdBo9xeKrKzAxo/sHxi0lTUKuAE4CNQL+kWcD7gE5S2MyX9KGI6AEuAO4AVgDrgf7h3jgirouIeRExb+rUqW+7gj1b+hgl2HeMp6rMzADaS3zvjbx1NNCZywZFxH/IIw5J44EzI2KzpM8DD0XEa/m53wHHAisi4m7g7lx+PlWCo1G6e/uYOK6DUaNU5seYme01yhxxPALMljRTUgdwFrC0cgdJUyQN1OFS4Kb8+HnSSKRd0mjSaGRNfs278v0k4IvADSW2gZ4tfUzyOlVmZoNKC46I2AZ8CbiP9KV/Z0SslnSZpNPybicCz0h6FpgGXJ7LlwD/BJ4kHQd5PI80AK6R9DTwF+CHEfFsWW2AvDKuj2+YmQ0qc6qKiLgHuGdI2XcqHi8hhcTQ1/UDX6jyngsbXM2aNm/ZyoH7jdudH2lmtkdr9sHxPZ5HHGZmb+XgqGFggUP/atzMbDsHRw2vvbGNrf3hdarMzCo4OGrwOlVmZjtycNTgdarMzHbk4KjB61SZme3IwVFDt4PDzGwHDo4aevJUlY9xmJlt5+Coobu3j7ZRYt8xpf5O0sxsr+LgqCGtU9VBvvigmZnh4Kgp/WrcZ1SZmVXyHEwNczsnMnPK+GZXw8xsj+LgqOHCj8xqdhXMzPY4nqoyM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkVoohodh1KJ6kL+PfbfPkU4JUGVmdvMRLbPRLbDCOz3W5zfQ6KiKlDC0dEcOwKSSsjYl6z67G7jcR2j8Q2w8hst9u8azxVZWZmhTg4zMysEAfHzl3X7Ao0yUhs90hsM4zMdrvNu8DHOMzMrBCPOMzMrBAHh5mZFeLgqEHSyZKekbRW0iXNrk8ZJB0gabmkpyWtlnRRLt9P0v2Snsv3k5pd10aT1CZplaTf5u2Zkh7O/X2HpI5m17HRJE2UtETSPyStkXRsq/e1pK/l/9tPSbpN0phW7GtJN0l6WdJTFWXD9q2SRbn9T0g6qshnOTiqkNQGLAZOAeYACyXNaW6tSrEN+HpEzAGOAS7M7bwEWBYRs4FlebvVXASsqdi+AvhJRMwCeoDzmlKrcl0D3BsR7wUOJ7W/Zfta0nTgK8C8iDgMaAPOojX7+hfAyUPKqvXtKcDsfDsfuLbIBzk4qjsaWBsR6yKiD7gdOL3JdWq4iNgUEX/Pj/9H+iKZTmrrzXm3m4EzmlPDckjqBD4O3JC3BcwHluRdWrHNE4APAzcCRERfRGymxfuadInssZLagXHAJlqwryPiz0D3kOJqfXs6cEskDwETJe1f72c5OKqbDrxQsb0hl7UsSTOAI4GHgWkRsSk/9SIwrUnVKsvVwDeBN/P2ZGBzRGzL263Y3zOBLuDneYruBkn70MJ9HREbgauA50mB8SrwKK3f1wOq9e0ufb85OAwASeOBXwJfjYj/Vj4X6ZztljlvW9KpwMsR8Wiz67KbtQNHAddGxJFAL0OmpVqwryeR/rqeCbwH2Icdp3NGhEb2rYOjuo3AARXbnbms5UgaTQqNWyPirlz80sDQNd+/3Kz6leB44DRJ60lTkPNJc/8T83QGtGZ/bwA2RMTDeXsJKUhaua8/CvwrIroiYitwF6n/W72vB1Tr2136fnNwVPcIMDuffdFBOqC2tMl1arg8t38jsCYiflzx1FLgnPz4HOA3u7tuZYmISyOiMyJmkPr1gYg4G1gOfCrv1lJtBoiIF4EXJB2Si04CnqaF+5o0RXWMpHH5//pAm1u6rytU69ulwGfz2VXHAK9WTGntlH85XoOkj5HmwtuAmyLi8iZXqeEkfRBYATzJ9vn+b5OOc9wJHEhakv7TETH0wNteT9KJwMURcaqkg0kjkP2AVcBnIuKNZtav0SQdQTohoANYB5xL+gOyZfta0veABaQzCFcBnyPN57dUX0u6DTiRtHz6S8B3gV8zTN/mEP0ZadpuC3BuRKys+7McHGZmVoSnqszMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYNYCkfkmPVdwatlCgpBmVK56aNVv7zncxszq8HhFHNLsSZruDRxxmJZK0XtKVkp6U9DdJs3L5DEkP5GshLJN0YC6fJulXkh7Pt+PyW7VJuj5fV+L3ksY2rVE24jk4zBpj7JCpqgUVz70aEe8n/VL36lz2U+DmiJgL3AosyuWLgD9FxOGkdaRW5/LZwOKIOBTYDJxZcnvMqvIvx80aQNJrETF+mPL1wPyIWJcXk3wxIiZLegXYPyK25vJNETFFUhfQWbn8RV7u/v58MR4kfQsYHRHfL79lZjvyiMOsfFHlcRGV6yj14+OT1kQODrPyLai4fzA//itpZV6As0kLTUK6vOcFMHhN9Am7q5Jm9fJfLWaNMVbSYxXb90bEwCm5kyQ9QRo1LMxlXyZdie8bpKvynZvLLwKuk3QeaWRxAenKdWZ7DB/jMCtRPsYxLyJeaXZdzBrFU1VmZlaIRxxmZlaIRxxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhfwfkOpbsawD7y4AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DCGAN"
      ],
      "metadata": {
        "id": "xM6PIrMgz9WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ElapsedTimer(object):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "    def elapsed(self,sec):\n",
        "        if sec < 60:\n",
        "            return str(sec) + \" sec\"\n",
        "        elif sec < (60 * 60):\n",
        "            return str(sec / 60) + \" min\"\n",
        "        else:\n",
        "            return str(sec / (60 * 60)) + \" hr\"\n",
        "    def elapsed_time(self):\n",
        "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
      ],
      "metadata": {
        "id": "fF715jzAzwYT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGAN(object):\n",
        "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "        self.channel = channel\n",
        "        self.D = None   # 감별자\n",
        "        self.G = None   # 생성자\n",
        "        self.AM = None  # 적대 모델\n",
        "        self.DM = None  # 판별 모델\n",
        "        \n",
        "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, window=5, input_dim=100, output_depth=1):\n",
        "\n",
        "        if self.G:\n",
        "            return self.G\n",
        "\n",
        "        self.G = Sequential()\n",
        "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "        self.G.add(Reshape((dim, dim, depth)))\n",
        "        self.G.add(Dropout(dropout))\n",
        "        \n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(UpSampling2D())\n",
        "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
        "        self.G.add(BatchNormalization(momentum=momentum))\n",
        "        self.G.add(Activation('relu'))\n",
        "\n",
        "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
        "        self.G.add(Activation('sigmoid'))\n",
        "        self.G.summary()\n",
        "\n",
        "        return self.G\n",
        "\n",
        "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
        "\n",
        "        if self.D:\n",
        "            return self.D\n",
        "\n",
        "        self.D = Sequential()\n",
        "\n",
        "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
        "\n",
        "        self.D.add(Conv2D(depth*1,\n",
        "                          5,\n",
        "                          strides=2,\n",
        "                          input_shape=input_shape,\n",
        "                          padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*2,\n",
        "                          5,\n",
        "                          strides=2,\n",
        "                          padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*4,\n",
        "                          5,\n",
        "                          strides=2,\n",
        "                          padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Conv2D(depth*8,\n",
        "                          5,\n",
        "                          strides=1,\n",
        "                          padding='same'))\n",
        "        self.D.add(LeakyReLU(alpha=alpha))\n",
        "        self.D.add(Dropout(dropout))\n",
        "\n",
        "        self.D.add(Flatten())\n",
        "        self.D.add(Dense(1))\n",
        "        self.D.add(Activation('sigmoid'))\n",
        "        self.D.summary()\n",
        "\n",
        "        return self.D\n",
        "\n",
        "    def discriminator_model(self):\n",
        "\n",
        "        if self.DM:\n",
        "            return self.DM\n",
        "\n",
        "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
        "        self.DM = Sequential()\n",
        "        self.DM.add(self.discriminator())\n",
        "        self.DM.compile(loss='binary_crossentropy', \n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "        \n",
        "        return self.DM\n",
        "\n",
        "    def adversarial_model(self):\n",
        "\n",
        "        if self.AM:\n",
        "            return self.AM\n",
        "\n",
        "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
        "\n",
        "        self.AM = Sequential()\n",
        "        self.AM.add(self.generator())\n",
        "        self.AM.add(self.discriminator())\n",
        "        self.AM.compile(loss='binary_crossentropy',\n",
        "                        optimizer=optimizer,\n",
        "                        metrics=['accuracy'])\n",
        "        \n",
        "        return self.AM"
      ],
      "metadata": {
        "id": "h2pfsY3-0Jb7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCGAN 초기화 및 훈련"
      ],
      "metadata": {
        "id": "DfsbZ8L41DHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_DCGAN(object):\n",
        "    def __init__(self, x_train):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channel = 1\n",
        "\n",
        "        self.x_train = x_train\n",
        "\n",
        "        self.DCGAN = DCGAN()\n",
        "        self.discriminator =  self.DCGAN.discriminator_model()\n",
        "        self.adversarial = self.DCGAN.adversarial_model()\n",
        "        self.generator = self.DCGAN.generator()\n",
        "\n",
        "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
        "\n",
        "        noise_input = None\n",
        "\n",
        "        if save_interval>0:\n",
        "\n",
        "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
        "\n",
        "        for i in range(train_steps):\n",
        "\n",
        "            images_train = self.x_train[np.random.randint(0,\n",
        "                                                          self.x_train.shape[0],\n",
        "                                                          size=batch_size),\n",
        "                                         :, :, :]\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            images_fake = self.generator.predict(noise)\n",
        "            x = np.concatenate((images_train, images_fake))\n",
        "            y = np.ones([2*batch_size, 1])\n",
        "            y[batch_size:, :] = 0\n",
        "            \n",
        "            d_loss = self.discriminator.train_on_batch(x, y)\n",
        "\n",
        "            y = np.ones([batch_size, 1])\n",
        "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
        "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
        "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
        "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
        "            print(log_mesg)\n",
        "            if save_interval>0:\n",
        "                if (i+1)%save_interval==0:\n",
        "                    self.plot_images(save2file=True,\n",
        "                                     samples=noise_input.shape[0],\n",
        "                                     noise=noise_input, step=(i+1))\n",
        "\n",
        "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
        "        current_path = os.getcwd()\n",
        "        file = os.path.sep.join(['', 'images', ''])\n",
        "        filename = 'mnist.png'\n",
        "        if fake:\n",
        "            if noise is None:\n",
        "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
        "            else:\n",
        "                filename = \"mnist_%d.png\" % step\n",
        "            images = self.generator.predict(noise)\n",
        "        else:\n",
        "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
        "            images = self.x_train[i, :, :, :]\n",
        "\n",
        "        plt.figure(figsize=(10,10))\n",
        "        for i in range(images.shape[0]):\n",
        "            plt.subplot(4, 4, i+1)\n",
        "            image = images[i, :, :, :]\n",
        "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
        "            plt.imshow(image, cmap='gray')\n",
        "            plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        if save2file:\n",
        "            plt.savefig(current_path+file+filename)\n",
        "            plt.close('all')\n",
        "        else:\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "f3BDLYlI0oWG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PhVTF0H2Je2",
        "outputId": "d732ffc1-9ece-42c8-f51d-6edd7246770a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘images/chapter12/synthetic_mnist’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
        "timer = ElapsedTimer()\n",
        "mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bAG87nh1BeT",
        "outputId": "7492e993-81c2-4112-ab88-cd7c4d3e94e7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_8 (Conv2D)           (None, 14, 14, 64)        1664      \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 7, 7, 128)         204928    \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 4, 4, 256)         819456    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 512)         3277312   \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 8193      \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,311,553\n",
            "Trainable params: 4,311,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 12544)             1266944   \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 12544)            50176     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 12544)             0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 7, 7, 256)         0         \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 14, 14, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 14, 14, 128)      819328    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 28, 28, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 28, 28, 64)       204864    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_6 (Conv2DT  (None, 28, 28, 32)       51232     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 28, 28, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        801       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,394,241\n",
            "Trainable params: 2,368,705\n",
            "Non-trainable params: 25,536\n",
            "_________________________________________________________________\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "0: [D loss: 0.691956, acc: 0.517578]  [A loss: 1.189875, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "1: [D loss: 0.631155, acc: 0.982422]  [A loss: 1.086515, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "2: [D loss: 0.490482, acc: 1.000000]  [A loss: 1.119576, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "3: [D loss: 0.294395, acc: 1.000000]  [A loss: 1.335925, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "4: [D loss: 0.249263, acc: 0.974609]  [A loss: 0.011500, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "5: [D loss: 0.164545, acc: 0.998047]  [A loss: 0.026889, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "6: [D loss: 0.130861, acc: 0.992188]  [A loss: 0.001474, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "7: [D loss: 0.108056, acc: 0.992188]  [A loss: 0.000529, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "8: [D loss: 0.078190, acc: 0.998047]  [A loss: 0.000242, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "9: [D loss: 0.056740, acc: 1.000000]  [A loss: 0.000172, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "10: [D loss: 0.041478, acc: 0.998047]  [A loss: 0.000080, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "11: [D loss: 0.029574, acc: 1.000000]  [A loss: 0.000161, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "12: [D loss: 0.040166, acc: 0.994141]  [A loss: 0.000001, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "13: [D loss: 0.025755, acc: 1.000000]  [A loss: 0.000010, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "14: [D loss: 0.014434, acc: 1.000000]  [A loss: 0.000028, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "15: [D loss: 0.009916, acc: 1.000000]  [A loss: 0.000031, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "16: [D loss: 0.009322, acc: 1.000000]  [A loss: 0.000018, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "17: [D loss: 0.007940, acc: 1.000000]  [A loss: 0.000013, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "18: [D loss: 0.008072, acc: 1.000000]  [A loss: 0.000010, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "19: [D loss: 0.008685, acc: 1.000000]  [A loss: 0.000259, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "20: [D loss: 0.011402, acc: 1.000000]  [A loss: 0.008092, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "21: [D loss: 0.875917, acc: 0.697266]  [A loss: 29.680759, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "22: [D loss: 1.327914, acc: 0.544922]  [A loss: 0.000009, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "23: [D loss: 1.014650, acc: 0.568359]  [A loss: 0.006708, acc: 1.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "24: [D loss: 0.492453, acc: 0.699219]  [A loss: 0.408734, acc: 0.859375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "25: [D loss: 2.432544, acc: 0.490234]  [A loss: 6.518207, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "26: [D loss: 0.717729, acc: 0.589844]  [A loss: 0.400122, acc: 0.949219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "27: [D loss: 1.343800, acc: 0.476562]  [A loss: 4.833598, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "28: [D loss: 0.584066, acc: 0.679688]  [A loss: 0.949478, acc: 0.164062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "29: [D loss: 1.168684, acc: 0.472656]  [A loss: 4.683341, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "30: [D loss: 0.602266, acc: 0.644531]  [A loss: 1.124135, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "31: [D loss: 1.056173, acc: 0.472656]  [A loss: 4.191999, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "32: [D loss: 0.564917, acc: 0.710938]  [A loss: 1.285654, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "33: [D loss: 0.880124, acc: 0.478516]  [A loss: 3.715076, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "34: [D loss: 0.519494, acc: 0.791016]  [A loss: 1.544868, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "35: [D loss: 0.795817, acc: 0.488281]  [A loss: 3.624133, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "36: [D loss: 0.484820, acc: 0.835938]  [A loss: 1.539083, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "37: [D loss: 0.736096, acc: 0.494141]  [A loss: 3.445308, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "38: [D loss: 0.458903, acc: 0.876953]  [A loss: 1.509146, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "39: [D loss: 0.682882, acc: 0.501953]  [A loss: 3.450137, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "40: [D loss: 0.432620, acc: 0.871094]  [A loss: 1.486104, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "41: [D loss: 0.635979, acc: 0.501953]  [A loss: 3.307724, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "42: [D loss: 0.396541, acc: 0.927734]  [A loss: 1.496917, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "43: [D loss: 0.579168, acc: 0.513672]  [A loss: 3.025312, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "44: [D loss: 0.361107, acc: 0.951172]  [A loss: 1.475039, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "45: [D loss: 0.538090, acc: 0.533203]  [A loss: 2.932558, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "46: [D loss: 0.346386, acc: 0.984375]  [A loss: 1.508295, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "47: [D loss: 0.501306, acc: 0.560547]  [A loss: 2.750002, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "48: [D loss: 0.328177, acc: 0.976562]  [A loss: 1.448897, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "49: [D loss: 0.503587, acc: 0.568359]  [A loss: 2.812593, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "50: [D loss: 0.336722, acc: 0.976562]  [A loss: 1.205076, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "51: [D loss: 0.569884, acc: 0.521484]  [A loss: 3.122397, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "52: [D loss: 0.369674, acc: 0.941406]  [A loss: 0.981258, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "53: [D loss: 0.640204, acc: 0.503906]  [A loss: 2.918557, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "54: [D loss: 0.375584, acc: 0.931641]  [A loss: 1.050085, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "55: [D loss: 0.573378, acc: 0.515625]  [A loss: 2.578886, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "56: [D loss: 0.352120, acc: 0.978516]  [A loss: 1.209948, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "57: [D loss: 0.530832, acc: 0.546875]  [A loss: 2.630950, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "58: [D loss: 0.353828, acc: 0.962891]  [A loss: 1.119239, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "59: [D loss: 0.595832, acc: 0.531250]  [A loss: 2.838348, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "60: [D loss: 0.414056, acc: 0.888672]  [A loss: 0.963916, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "61: [D loss: 0.661257, acc: 0.515625]  [A loss: 2.565460, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "62: [D loss: 0.416569, acc: 0.908203]  [A loss: 1.059555, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "63: [D loss: 0.582741, acc: 0.558594]  [A loss: 2.336451, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "64: [D loss: 0.392234, acc: 0.941406]  [A loss: 1.082096, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "65: [D loss: 0.537975, acc: 0.607422]  [A loss: 2.379496, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "66: [D loss: 0.393786, acc: 0.927734]  [A loss: 0.970348, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "67: [D loss: 0.661469, acc: 0.533203]  [A loss: 2.627299, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "68: [D loss: 0.473111, acc: 0.839844]  [A loss: 0.762066, acc: 0.406250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "69: [D loss: 0.701531, acc: 0.515625]  [A loss: 2.129973, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "70: [D loss: 0.424379, acc: 0.910156]  [A loss: 0.946874, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "71: [D loss: 0.587681, acc: 0.570312]  [A loss: 2.078786, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "72: [D loss: 0.420318, acc: 0.912109]  [A loss: 0.999688, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "73: [D loss: 0.585467, acc: 0.562500]  [A loss: 2.269885, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "74: [D loss: 0.441868, acc: 0.892578]  [A loss: 0.808304, acc: 0.371094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "75: [D loss: 0.693308, acc: 0.513672]  [A loss: 2.248674, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "76: [D loss: 0.462374, acc: 0.867188]  [A loss: 0.786353, acc: 0.359375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "77: [D loss: 0.695870, acc: 0.517578]  [A loss: 2.038927, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "78: [D loss: 0.454806, acc: 0.888672]  [A loss: 0.884145, acc: 0.238281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "79: [D loss: 0.616536, acc: 0.533203]  [A loss: 2.010996, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "80: [D loss: 0.458845, acc: 0.884766]  [A loss: 0.867846, acc: 0.246094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "81: [D loss: 0.677171, acc: 0.531250]  [A loss: 2.103941, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "82: [D loss: 0.491851, acc: 0.851562]  [A loss: 0.749938, acc: 0.425781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "83: [D loss: 0.726318, acc: 0.505859]  [A loss: 1.969041, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "84: [D loss: 0.512308, acc: 0.847656]  [A loss: 0.717305, acc: 0.488281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "85: [D loss: 0.710223, acc: 0.500000]  [A loss: 1.783983, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "86: [D loss: 0.484248, acc: 0.892578]  [A loss: 0.914075, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "87: [D loss: 0.642857, acc: 0.517578]  [A loss: 1.800881, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "88: [D loss: 0.501654, acc: 0.851562]  [A loss: 0.979382, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "89: [D loss: 0.641091, acc: 0.519531]  [A loss: 1.965098, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "90: [D loss: 0.517466, acc: 0.851562]  [A loss: 0.740995, acc: 0.406250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "91: [D loss: 0.766764, acc: 0.503906]  [A loss: 2.125737, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "92: [D loss: 0.532439, acc: 0.816406]  [A loss: 0.638158, acc: 0.628906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "93: [D loss: 0.751630, acc: 0.500000]  [A loss: 1.659343, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "94: [D loss: 0.536668, acc: 0.847656]  [A loss: 0.858812, acc: 0.210938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "95: [D loss: 0.661569, acc: 0.505859]  [A loss: 1.713264, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "96: [D loss: 0.530047, acc: 0.828125]  [A loss: 0.938187, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "97: [D loss: 0.674734, acc: 0.507812]  [A loss: 2.000638, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "98: [D loss: 0.529683, acc: 0.859375]  [A loss: 0.728680, acc: 0.414062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "99: [D loss: 0.770131, acc: 0.501953]  [A loss: 2.091328, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "100: [D loss: 0.576892, acc: 0.767578]  [A loss: 0.643436, acc: 0.644531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "101: [D loss: 0.761049, acc: 0.500000]  [A loss: 1.676687, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "102: [D loss: 0.558618, acc: 0.826172]  [A loss: 0.784759, acc: 0.308594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "103: [D loss: 0.695684, acc: 0.511719]  [A loss: 1.614298, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "104: [D loss: 0.566118, acc: 0.751953]  [A loss: 0.882391, acc: 0.167969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "105: [D loss: 0.674100, acc: 0.509766]  [A loss: 1.777930, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "106: [D loss: 0.565823, acc: 0.810547]  [A loss: 0.695175, acc: 0.523438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "107: [D loss: 0.739663, acc: 0.500000]  [A loss: 1.881934, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "108: [D loss: 0.566238, acc: 0.814453]  [A loss: 0.631884, acc: 0.675781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "109: [D loss: 0.767756, acc: 0.496094]  [A loss: 1.729006, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "110: [D loss: 0.580980, acc: 0.798828]  [A loss: 0.653780, acc: 0.625000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "111: [D loss: 0.730590, acc: 0.503906]  [A loss: 1.557739, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "112: [D loss: 0.581606, acc: 0.763672]  [A loss: 0.815616, acc: 0.289062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "113: [D loss: 0.694062, acc: 0.511719]  [A loss: 1.651540, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "114: [D loss: 0.581968, acc: 0.759766]  [A loss: 0.720605, acc: 0.507812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "115: [D loss: 0.719024, acc: 0.498047]  [A loss: 1.801740, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "116: [D loss: 0.575291, acc: 0.796875]  [A loss: 0.604423, acc: 0.746094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "117: [D loss: 0.754165, acc: 0.494141]  [A loss: 1.767105, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "118: [D loss: 0.592335, acc: 0.775391]  [A loss: 0.627939, acc: 0.683594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "119: [D loss: 0.746253, acc: 0.496094]  [A loss: 1.589678, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "120: [D loss: 0.588496, acc: 0.796875]  [A loss: 0.676249, acc: 0.605469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "121: [D loss: 0.700374, acc: 0.503906]  [A loss: 1.517115, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "122: [D loss: 0.585381, acc: 0.789062]  [A loss: 0.765160, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "123: [D loss: 0.712998, acc: 0.496094]  [A loss: 1.855443, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "124: [D loss: 0.591969, acc: 0.767578]  [A loss: 0.544368, acc: 0.890625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "125: [D loss: 0.796293, acc: 0.500000]  [A loss: 1.767695, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "126: [D loss: 0.617886, acc: 0.691406]  [A loss: 0.614301, acc: 0.761719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "127: [D loss: 0.729562, acc: 0.500000]  [A loss: 1.276843, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "128: [D loss: 0.601609, acc: 0.755859]  [A loss: 0.918503, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "129: [D loss: 0.648759, acc: 0.505859]  [A loss: 1.551723, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "130: [D loss: 0.577683, acc: 0.800781]  [A loss: 0.730942, acc: 0.429688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "131: [D loss: 0.723121, acc: 0.501953]  [A loss: 1.970009, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "132: [D loss: 0.635667, acc: 0.605469]  [A loss: 0.501046, acc: 0.968750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "133: [D loss: 0.787466, acc: 0.498047]  [A loss: 1.273087, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "134: [D loss: 0.604717, acc: 0.785156]  [A loss: 0.795433, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "135: [D loss: 0.658666, acc: 0.500000]  [A loss: 1.298407, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "136: [D loss: 0.587490, acc: 0.732422]  [A loss: 0.952276, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "137: [D loss: 0.644681, acc: 0.517578]  [A loss: 1.588296, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "138: [D loss: 0.590901, acc: 0.792969]  [A loss: 0.633850, acc: 0.718750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "139: [D loss: 0.756080, acc: 0.496094]  [A loss: 1.730911, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "140: [D loss: 0.628590, acc: 0.615234]  [A loss: 0.573965, acc: 0.882812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "141: [D loss: 0.761015, acc: 0.496094]  [A loss: 1.219917, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "142: [D loss: 0.604343, acc: 0.769531]  [A loss: 0.831533, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "143: [D loss: 0.682037, acc: 0.498047]  [A loss: 1.624307, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "144: [D loss: 0.613172, acc: 0.683594]  [A loss: 0.604608, acc: 0.832031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "145: [D loss: 0.710241, acc: 0.500000]  [A loss: 1.267412, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "146: [D loss: 0.616668, acc: 0.777344]  [A loss: 0.740931, acc: 0.324219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "147: [D loss: 0.670619, acc: 0.498047]  [A loss: 1.218218, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "148: [D loss: 0.590028, acc: 0.691406]  [A loss: 0.989146, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "149: [D loss: 0.633808, acc: 0.537109]  [A loss: 1.391085, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "150: [D loss: 0.579903, acc: 0.869141]  [A loss: 0.669948, acc: 0.566406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "151: [D loss: 0.750199, acc: 0.498047]  [A loss: 1.738008, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "152: [D loss: 0.647707, acc: 0.554688]  [A loss: 0.568567, acc: 0.910156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "153: [D loss: 0.748776, acc: 0.500000]  [A loss: 1.067692, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "154: [D loss: 0.601231, acc: 0.728516]  [A loss: 0.875390, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "155: [D loss: 0.629637, acc: 0.535156]  [A loss: 1.272256, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "156: [D loss: 0.568374, acc: 0.851562]  [A loss: 0.771787, acc: 0.308594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "157: [D loss: 0.658337, acc: 0.509766]  [A loss: 1.444726, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "158: [D loss: 0.606521, acc: 0.759766]  [A loss: 0.650542, acc: 0.664062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "159: [D loss: 0.714240, acc: 0.498047]  [A loss: 1.421959, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "160: [D loss: 0.582249, acc: 0.800781]  [A loss: 0.737131, acc: 0.367188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "161: [D loss: 0.713867, acc: 0.509766]  [A loss: 1.535924, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "162: [D loss: 0.633919, acc: 0.591797]  [A loss: 0.637656, acc: 0.699219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "163: [D loss: 0.735457, acc: 0.501953]  [A loss: 1.138047, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "164: [D loss: 0.588699, acc: 0.777344]  [A loss: 0.867371, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "165: [D loss: 0.651093, acc: 0.548828]  [A loss: 1.326796, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "166: [D loss: 0.606917, acc: 0.765625]  [A loss: 0.680029, acc: 0.546875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "167: [D loss: 0.685305, acc: 0.505859]  [A loss: 1.273152, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "168: [D loss: 0.605732, acc: 0.787109]  [A loss: 0.744281, acc: 0.355469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "169: [D loss: 0.694707, acc: 0.513672]  [A loss: 1.445228, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "170: [D loss: 0.619168, acc: 0.693359]  [A loss: 0.644434, acc: 0.656250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "171: [D loss: 0.686582, acc: 0.501953]  [A loss: 1.176679, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "172: [D loss: 0.602404, acc: 0.777344]  [A loss: 0.800732, acc: 0.242188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "173: [D loss: 0.682773, acc: 0.533203]  [A loss: 1.426743, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "174: [D loss: 0.617535, acc: 0.687500]  [A loss: 0.604149, acc: 0.750000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "175: [D loss: 0.750956, acc: 0.500000]  [A loss: 1.234522, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "176: [D loss: 0.614713, acc: 0.710938]  [A loss: 0.831116, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "177: [D loss: 0.666700, acc: 0.541016]  [A loss: 1.217827, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "178: [D loss: 0.626872, acc: 0.716797]  [A loss: 0.728765, acc: 0.417969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "179: [D loss: 0.690823, acc: 0.501953]  [A loss: 1.234129, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "180: [D loss: 0.611173, acc: 0.732422]  [A loss: 0.791530, acc: 0.234375]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "181: [D loss: 0.671232, acc: 0.521484]  [A loss: 1.283287, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "182: [D loss: 0.630420, acc: 0.718750]  [A loss: 0.682025, acc: 0.531250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "183: [D loss: 0.720412, acc: 0.500000]  [A loss: 1.467205, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "184: [D loss: 0.620616, acc: 0.728516]  [A loss: 0.622696, acc: 0.714844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "185: [D loss: 0.737228, acc: 0.505859]  [A loss: 1.301329, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "186: [D loss: 0.664433, acc: 0.605469]  [A loss: 0.744103, acc: 0.363281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "187: [D loss: 0.715885, acc: 0.492188]  [A loss: 1.203161, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "188: [D loss: 0.642987, acc: 0.632812]  [A loss: 0.971198, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "189: [D loss: 0.630231, acc: 0.623047]  [A loss: 0.902597, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "190: [D loss: 0.650216, acc: 0.546875]  [A loss: 1.235620, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "191: [D loss: 0.612430, acc: 0.757812]  [A loss: 0.783177, acc: 0.265625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "192: [D loss: 0.670765, acc: 0.517578]  [A loss: 1.571063, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "193: [D loss: 0.623221, acc: 0.644531]  [A loss: 0.528145, acc: 0.843750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "194: [D loss: 0.797379, acc: 0.498047]  [A loss: 1.398434, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "195: [D loss: 0.657747, acc: 0.593750]  [A loss: 0.683937, acc: 0.546875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "196: [D loss: 0.730499, acc: 0.496094]  [A loss: 1.031693, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "197: [D loss: 0.650635, acc: 0.626953]  [A loss: 0.930881, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "198: [D loss: 0.656068, acc: 0.572266]  [A loss: 1.011097, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "199: [D loss: 0.640552, acc: 0.683594]  [A loss: 0.846837, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "200: [D loss: 0.655887, acc: 0.519531]  [A loss: 1.138021, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "201: [D loss: 0.635528, acc: 0.673828]  [A loss: 0.830408, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "202: [D loss: 0.664790, acc: 0.527344]  [A loss: 1.205854, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "203: [D loss: 0.626132, acc: 0.718750]  [A loss: 0.741892, acc: 0.367188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "204: [D loss: 0.692099, acc: 0.507812]  [A loss: 1.520938, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "205: [D loss: 0.637808, acc: 0.621094]  [A loss: 0.589667, acc: 0.796875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "206: [D loss: 0.745778, acc: 0.496094]  [A loss: 1.263605, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "207: [D loss: 0.650881, acc: 0.660156]  [A loss: 0.729165, acc: 0.390625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "208: [D loss: 0.693782, acc: 0.505859]  [A loss: 1.093173, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "209: [D loss: 0.650686, acc: 0.669922]  [A loss: 0.774632, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "210: [D loss: 0.664317, acc: 0.513672]  [A loss: 1.067775, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "211: [D loss: 0.643689, acc: 0.660156]  [A loss: 0.889552, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "212: [D loss: 0.658530, acc: 0.542969]  [A loss: 1.080167, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "213: [D loss: 0.641482, acc: 0.652344]  [A loss: 0.829718, acc: 0.167969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "214: [D loss: 0.676959, acc: 0.527344]  [A loss: 1.320977, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "215: [D loss: 0.649003, acc: 0.671875]  [A loss: 0.597853, acc: 0.812500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "216: [D loss: 0.747152, acc: 0.500000]  [A loss: 1.328020, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "217: [D loss: 0.658138, acc: 0.591797]  [A loss: 0.657490, acc: 0.636719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "218: [D loss: 0.717082, acc: 0.498047]  [A loss: 1.114668, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "219: [D loss: 0.658207, acc: 0.654297]  [A loss: 0.731665, acc: 0.390625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "220: [D loss: 0.684894, acc: 0.511719]  [A loss: 1.062676, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "221: [D loss: 0.642431, acc: 0.695312]  [A loss: 0.745943, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "222: [D loss: 0.686147, acc: 0.505859]  [A loss: 1.089306, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "223: [D loss: 0.642507, acc: 0.687500]  [A loss: 0.780765, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "224: [D loss: 0.690078, acc: 0.537109]  [A loss: 1.270991, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "225: [D loss: 0.655300, acc: 0.609375]  [A loss: 0.625113, acc: 0.742188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "226: [D loss: 0.732681, acc: 0.498047]  [A loss: 1.215301, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "227: [D loss: 0.650205, acc: 0.636719]  [A loss: 0.679765, acc: 0.585938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "228: [D loss: 0.713799, acc: 0.509766]  [A loss: 1.124258, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "229: [D loss: 0.664024, acc: 0.609375]  [A loss: 0.712115, acc: 0.410156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "230: [D loss: 0.735794, acc: 0.498047]  [A loss: 1.088534, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "231: [D loss: 0.658436, acc: 0.623047]  [A loss: 0.797320, acc: 0.152344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "232: [D loss: 0.667807, acc: 0.539062]  [A loss: 0.987343, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "233: [D loss: 0.649176, acc: 0.650391]  [A loss: 0.815535, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "234: [D loss: 0.675165, acc: 0.521484]  [A loss: 1.101879, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "235: [D loss: 0.656325, acc: 0.638672]  [A loss: 0.780199, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "236: [D loss: 0.691055, acc: 0.519531]  [A loss: 1.128493, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "237: [D loss: 0.656527, acc: 0.619141]  [A loss: 0.838890, acc: 0.132812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "238: [D loss: 0.674825, acc: 0.539062]  [A loss: 1.133185, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "239: [D loss: 0.643464, acc: 0.707031]  [A loss: 0.737794, acc: 0.347656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "240: [D loss: 0.703124, acc: 0.498047]  [A loss: 1.412860, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "241: [D loss: 0.660440, acc: 0.558594]  [A loss: 0.549757, acc: 0.941406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "242: [D loss: 0.763858, acc: 0.498047]  [A loss: 1.068552, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "243: [D loss: 0.663137, acc: 0.630859]  [A loss: 0.702721, acc: 0.472656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "244: [D loss: 0.698749, acc: 0.509766]  [A loss: 1.097541, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "245: [D loss: 0.671836, acc: 0.582031]  [A loss: 0.688752, acc: 0.531250]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "246: [D loss: 0.729995, acc: 0.492188]  [A loss: 1.014424, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "247: [D loss: 0.662013, acc: 0.621094]  [A loss: 0.820705, acc: 0.117188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "248: [D loss: 0.659050, acc: 0.574219]  [A loss: 0.893677, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "249: [D loss: 0.673148, acc: 0.548828]  [A loss: 0.831781, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "250: [D loss: 0.672738, acc: 0.529297]  [A loss: 1.040353, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "251: [D loss: 0.653695, acc: 0.662109]  [A loss: 0.688720, acc: 0.539062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "252: [D loss: 0.712894, acc: 0.503906]  [A loss: 1.250998, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "253: [D loss: 0.669456, acc: 0.574219]  [A loss: 0.603404, acc: 0.828125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "254: [D loss: 0.744217, acc: 0.500000]  [A loss: 1.062256, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "255: [D loss: 0.655780, acc: 0.650391]  [A loss: 0.724388, acc: 0.398438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "256: [D loss: 0.694060, acc: 0.500000]  [A loss: 1.079271, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "257: [D loss: 0.664562, acc: 0.595703]  [A loss: 0.665709, acc: 0.628906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "258: [D loss: 0.701246, acc: 0.500000]  [A loss: 1.047318, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "259: [D loss: 0.652759, acc: 0.648438]  [A loss: 0.719334, acc: 0.437500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "260: [D loss: 0.695998, acc: 0.513672]  [A loss: 1.135171, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "261: [D loss: 0.663172, acc: 0.634766]  [A loss: 0.778056, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "262: [D loss: 0.673664, acc: 0.533203]  [A loss: 1.013296, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "263: [D loss: 0.663459, acc: 0.619141]  [A loss: 0.756500, acc: 0.308594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "264: [D loss: 0.681868, acc: 0.515625]  [A loss: 1.239031, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "265: [D loss: 0.666335, acc: 0.574219]  [A loss: 0.595794, acc: 0.828125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "266: [D loss: 0.744205, acc: 0.496094]  [A loss: 1.084178, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "267: [D loss: 0.666832, acc: 0.589844]  [A loss: 0.746764, acc: 0.320312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "268: [D loss: 0.683800, acc: 0.542969]  [A loss: 0.928138, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "269: [D loss: 0.656615, acc: 0.648438]  [A loss: 0.750492, acc: 0.273438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "270: [D loss: 0.681485, acc: 0.513672]  [A loss: 1.084821, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "271: [D loss: 0.664895, acc: 0.634766]  [A loss: 0.703108, acc: 0.464844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "272: [D loss: 0.694080, acc: 0.505859]  [A loss: 1.090002, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "273: [D loss: 0.660184, acc: 0.640625]  [A loss: 0.720287, acc: 0.402344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "274: [D loss: 0.696644, acc: 0.503906]  [A loss: 1.051895, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "275: [D loss: 0.654209, acc: 0.646484]  [A loss: 0.736567, acc: 0.363281]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "276: [D loss: 0.710851, acc: 0.517578]  [A loss: 1.143130, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "277: [D loss: 0.699335, acc: 0.501953]  [A loss: 0.746805, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "278: [D loss: 0.714645, acc: 0.486328]  [A loss: 1.048754, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "279: [D loss: 0.675770, acc: 0.601562]  [A loss: 0.813410, acc: 0.117188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "280: [D loss: 0.671432, acc: 0.542969]  [A loss: 0.869419, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "281: [D loss: 0.672715, acc: 0.541016]  [A loss: 0.950643, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "282: [D loss: 0.662658, acc: 0.589844]  [A loss: 0.830709, acc: 0.132812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "283: [D loss: 0.684143, acc: 0.539062]  [A loss: 1.197499, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "284: [D loss: 0.659546, acc: 0.605469]  [A loss: 0.602583, acc: 0.804688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "285: [D loss: 0.747732, acc: 0.501953]  [A loss: 1.186944, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "286: [D loss: 0.678160, acc: 0.560547]  [A loss: 0.719812, acc: 0.437500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "287: [D loss: 0.702570, acc: 0.513672]  [A loss: 1.101419, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "288: [D loss: 0.677885, acc: 0.548828]  [A loss: 0.679352, acc: 0.589844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "289: [D loss: 0.718186, acc: 0.503906]  [A loss: 0.996450, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "290: [D loss: 0.666379, acc: 0.595703]  [A loss: 0.781019, acc: 0.183594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "291: [D loss: 0.678241, acc: 0.535156]  [A loss: 0.949710, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "292: [D loss: 0.672009, acc: 0.626953]  [A loss: 0.782129, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "293: [D loss: 0.687168, acc: 0.527344]  [A loss: 1.021142, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "294: [D loss: 0.657044, acc: 0.650391]  [A loss: 0.765212, acc: 0.269531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "295: [D loss: 0.688697, acc: 0.505859]  [A loss: 1.085909, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "296: [D loss: 0.670827, acc: 0.615234]  [A loss: 0.711558, acc: 0.402344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "297: [D loss: 0.697245, acc: 0.498047]  [A loss: 1.101997, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "298: [D loss: 0.661472, acc: 0.621094]  [A loss: 0.681086, acc: 0.566406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "299: [D loss: 0.707087, acc: 0.500000]  [A loss: 1.060825, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "300: [D loss: 0.691210, acc: 0.513672]  [A loss: 0.787849, acc: 0.171875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "301: [D loss: 0.691722, acc: 0.523438]  [A loss: 0.936574, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "302: [D loss: 0.682666, acc: 0.560547]  [A loss: 0.892476, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "303: [D loss: 0.660571, acc: 0.617188]  [A loss: 0.806751, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "304: [D loss: 0.688529, acc: 0.519531]  [A loss: 1.032378, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "305: [D loss: 0.656686, acc: 0.658203]  [A loss: 0.744577, acc: 0.363281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "306: [D loss: 0.698215, acc: 0.509766]  [A loss: 1.221565, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "307: [D loss: 0.680513, acc: 0.560547]  [A loss: 0.603312, acc: 0.835938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "308: [D loss: 0.731932, acc: 0.498047]  [A loss: 1.086455, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "309: [D loss: 0.669405, acc: 0.570312]  [A loss: 0.656844, acc: 0.652344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "310: [D loss: 0.726176, acc: 0.505859]  [A loss: 1.062818, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "311: [D loss: 0.685252, acc: 0.523438]  [A loss: 0.717953, acc: 0.402344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "312: [D loss: 0.697974, acc: 0.498047]  [A loss: 0.959283, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "313: [D loss: 0.668758, acc: 0.605469]  [A loss: 0.772686, acc: 0.210938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "314: [D loss: 0.678723, acc: 0.525391]  [A loss: 0.911383, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "315: [D loss: 0.670246, acc: 0.613281]  [A loss: 0.835022, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "316: [D loss: 0.672339, acc: 0.552734]  [A loss: 0.947647, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "317: [D loss: 0.665939, acc: 0.599609]  [A loss: 0.803204, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "318: [D loss: 0.676797, acc: 0.541016]  [A loss: 1.023435, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "319: [D loss: 0.660803, acc: 0.640625]  [A loss: 0.758763, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "320: [D loss: 0.692600, acc: 0.517578]  [A loss: 1.180836, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "321: [D loss: 0.668638, acc: 0.585938]  [A loss: 0.592778, acc: 0.820312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "322: [D loss: 0.769569, acc: 0.500000]  [A loss: 1.164399, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "323: [D loss: 0.672257, acc: 0.566406]  [A loss: 0.638255, acc: 0.730469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "324: [D loss: 0.735389, acc: 0.498047]  [A loss: 1.060553, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "325: [D loss: 0.681139, acc: 0.558594]  [A loss: 0.695815, acc: 0.550781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "326: [D loss: 0.693179, acc: 0.531250]  [A loss: 0.869182, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "327: [D loss: 0.666289, acc: 0.595703]  [A loss: 0.836181, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "328: [D loss: 0.669672, acc: 0.562500]  [A loss: 0.847252, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "329: [D loss: 0.670696, acc: 0.535156]  [A loss: 0.899568, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "330: [D loss: 0.670263, acc: 0.566406]  [A loss: 0.872497, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "331: [D loss: 0.675450, acc: 0.560547]  [A loss: 0.891502, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "332: [D loss: 0.678492, acc: 0.550781]  [A loss: 0.964514, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "333: [D loss: 0.666576, acc: 0.628906]  [A loss: 0.790572, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "334: [D loss: 0.680452, acc: 0.533203]  [A loss: 0.977554, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "335: [D loss: 0.663044, acc: 0.611328]  [A loss: 0.763556, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "336: [D loss: 0.690334, acc: 0.529297]  [A loss: 1.203148, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "337: [D loss: 0.669388, acc: 0.572266]  [A loss: 0.589521, acc: 0.804688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "338: [D loss: 0.755191, acc: 0.503906]  [A loss: 1.171806, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "339: [D loss: 0.675286, acc: 0.560547]  [A loss: 0.666426, acc: 0.593750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "340: [D loss: 0.702338, acc: 0.507812]  [A loss: 0.983042, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "341: [D loss: 0.667731, acc: 0.615234]  [A loss: 0.705810, acc: 0.417969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "342: [D loss: 0.685420, acc: 0.513672]  [A loss: 0.919011, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "343: [D loss: 0.667487, acc: 0.587891]  [A loss: 0.787442, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "344: [D loss: 0.687874, acc: 0.531250]  [A loss: 0.959719, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "345: [D loss: 0.668226, acc: 0.574219]  [A loss: 0.811887, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "346: [D loss: 0.707366, acc: 0.511719]  [A loss: 0.951176, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "347: [D loss: 0.674293, acc: 0.582031]  [A loss: 0.894558, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "348: [D loss: 0.673133, acc: 0.564453]  [A loss: 0.894497, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "349: [D loss: 0.682593, acc: 0.513672]  [A loss: 1.006898, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "350: [D loss: 0.665149, acc: 0.634766]  [A loss: 0.759138, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "351: [D loss: 0.687157, acc: 0.517578]  [A loss: 1.004990, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "352: [D loss: 0.666639, acc: 0.619141]  [A loss: 0.797425, acc: 0.195312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "353: [D loss: 0.679237, acc: 0.531250]  [A loss: 1.059032, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "354: [D loss: 0.671760, acc: 0.601562]  [A loss: 0.654045, acc: 0.609375]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "355: [D loss: 0.721246, acc: 0.501953]  [A loss: 1.196730, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "356: [D loss: 0.679665, acc: 0.550781]  [A loss: 0.604167, acc: 0.820312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "357: [D loss: 0.758952, acc: 0.498047]  [A loss: 1.004954, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "358: [D loss: 0.678349, acc: 0.556641]  [A loss: 0.775475, acc: 0.218750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "359: [D loss: 0.678919, acc: 0.541016]  [A loss: 0.888720, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "360: [D loss: 0.674180, acc: 0.585938]  [A loss: 0.737001, acc: 0.343750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "361: [D loss: 0.695013, acc: 0.525391]  [A loss: 1.028417, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "362: [D loss: 0.674369, acc: 0.578125]  [A loss: 0.683161, acc: 0.558594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "363: [D loss: 0.697491, acc: 0.507812]  [A loss: 1.000096, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "364: [D loss: 0.670593, acc: 0.634766]  [A loss: 0.731566, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "365: [D loss: 0.695888, acc: 0.521484]  [A loss: 0.982706, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "366: [D loss: 0.657378, acc: 0.667969]  [A loss: 0.752863, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "367: [D loss: 0.695249, acc: 0.525391]  [A loss: 1.009377, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "368: [D loss: 0.661642, acc: 0.617188]  [A loss: 0.783749, acc: 0.195312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "369: [D loss: 0.691853, acc: 0.533203]  [A loss: 1.030810, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "370: [D loss: 0.672444, acc: 0.605469]  [A loss: 0.677622, acc: 0.562500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "371: [D loss: 0.691271, acc: 0.503906]  [A loss: 1.080335, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "372: [D loss: 0.667727, acc: 0.582031]  [A loss: 0.706126, acc: 0.480469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "373: [D loss: 0.719507, acc: 0.519531]  [A loss: 1.063499, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "374: [D loss: 0.699814, acc: 0.535156]  [A loss: 0.733252, acc: 0.367188]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "375: [D loss: 0.713769, acc: 0.515625]  [A loss: 0.985116, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "376: [D loss: 0.681256, acc: 0.533203]  [A loss: 0.752060, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "377: [D loss: 0.684541, acc: 0.541016]  [A loss: 0.902119, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "378: [D loss: 0.673969, acc: 0.580078]  [A loss: 0.803277, acc: 0.140625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "379: [D loss: 0.679813, acc: 0.546875]  [A loss: 0.957809, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "380: [D loss: 0.680786, acc: 0.572266]  [A loss: 0.750056, acc: 0.312500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "381: [D loss: 0.684328, acc: 0.533203]  [A loss: 0.999343, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "382: [D loss: 0.671207, acc: 0.595703]  [A loss: 0.757095, acc: 0.285156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "383: [D loss: 0.687958, acc: 0.529297]  [A loss: 1.026044, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "384: [D loss: 0.667871, acc: 0.640625]  [A loss: 0.680786, acc: 0.578125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "385: [D loss: 0.720489, acc: 0.515625]  [A loss: 1.118257, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "386: [D loss: 0.682304, acc: 0.568359]  [A loss: 0.660784, acc: 0.640625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "387: [D loss: 0.705250, acc: 0.517578]  [A loss: 1.048951, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "388: [D loss: 0.672143, acc: 0.583984]  [A loss: 0.694024, acc: 0.511719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "389: [D loss: 0.705924, acc: 0.517578]  [A loss: 0.952591, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "390: [D loss: 0.671142, acc: 0.601562]  [A loss: 0.778468, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "391: [D loss: 0.681149, acc: 0.548828]  [A loss: 0.935371, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "392: [D loss: 0.664787, acc: 0.617188]  [A loss: 0.816808, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "393: [D loss: 0.680970, acc: 0.560547]  [A loss: 0.924881, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "394: [D loss: 0.666196, acc: 0.630859]  [A loss: 0.846658, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "395: [D loss: 0.681657, acc: 0.564453]  [A loss: 0.933951, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "396: [D loss: 0.674491, acc: 0.589844]  [A loss: 0.823042, acc: 0.164062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "397: [D loss: 0.676726, acc: 0.554688]  [A loss: 0.965222, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "398: [D loss: 0.670896, acc: 0.599609]  [A loss: 0.805036, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "399: [D loss: 0.678948, acc: 0.542969]  [A loss: 1.109236, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "400: [D loss: 0.677019, acc: 0.568359]  [A loss: 0.649717, acc: 0.656250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "401: [D loss: 0.715697, acc: 0.505859]  [A loss: 1.155200, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "402: [D loss: 0.676267, acc: 0.552734]  [A loss: 0.614092, acc: 0.789062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "403: [D loss: 0.718363, acc: 0.505859]  [A loss: 1.045218, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "404: [D loss: 0.681574, acc: 0.541016]  [A loss: 0.711342, acc: 0.445312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "405: [D loss: 0.705134, acc: 0.505859]  [A loss: 0.932551, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "406: [D loss: 0.667092, acc: 0.609375]  [A loss: 0.779444, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "407: [D loss: 0.676206, acc: 0.542969]  [A loss: 0.943226, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "408: [D loss: 0.659935, acc: 0.650391]  [A loss: 0.755736, acc: 0.347656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "409: [D loss: 0.730206, acc: 0.505859]  [A loss: 1.051510, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "410: [D loss: 0.672060, acc: 0.595703]  [A loss: 0.701379, acc: 0.484375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "411: [D loss: 0.685084, acc: 0.515625]  [A loss: 0.936272, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "412: [D loss: 0.663485, acc: 0.626953]  [A loss: 0.731533, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "413: [D loss: 0.683247, acc: 0.515625]  [A loss: 1.032363, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "414: [D loss: 0.675447, acc: 0.572266]  [A loss: 0.723774, acc: 0.445312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "415: [D loss: 0.699413, acc: 0.537109]  [A loss: 0.963127, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "416: [D loss: 0.661859, acc: 0.626953]  [A loss: 0.770479, acc: 0.281250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "417: [D loss: 0.704220, acc: 0.527344]  [A loss: 1.050775, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "418: [D loss: 0.674274, acc: 0.595703]  [A loss: 0.701302, acc: 0.476562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "419: [D loss: 0.707644, acc: 0.507812]  [A loss: 1.073925, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "420: [D loss: 0.678003, acc: 0.578125]  [A loss: 0.695186, acc: 0.496094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "421: [D loss: 0.689526, acc: 0.525391]  [A loss: 0.933425, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "422: [D loss: 0.661135, acc: 0.619141]  [A loss: 0.754818, acc: 0.289062]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "423: [D loss: 0.701851, acc: 0.529297]  [A loss: 0.974223, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "424: [D loss: 0.669053, acc: 0.593750]  [A loss: 0.766249, acc: 0.289062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "425: [D loss: 0.689138, acc: 0.523438]  [A loss: 0.993521, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "426: [D loss: 0.663412, acc: 0.636719]  [A loss: 0.749619, acc: 0.316406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "427: [D loss: 0.687458, acc: 0.535156]  [A loss: 0.972317, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "428: [D loss: 0.676509, acc: 0.544922]  [A loss: 0.782372, acc: 0.214844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "429: [D loss: 0.683391, acc: 0.556641]  [A loss: 0.976214, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "430: [D loss: 0.669488, acc: 0.560547]  [A loss: 0.884822, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "431: [D loss: 0.667065, acc: 0.613281]  [A loss: 0.856821, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "432: [D loss: 0.678032, acc: 0.562500]  [A loss: 0.968261, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "433: [D loss: 0.674081, acc: 0.576172]  [A loss: 0.797473, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "434: [D loss: 0.693286, acc: 0.525391]  [A loss: 1.044677, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "435: [D loss: 0.681527, acc: 0.566406]  [A loss: 0.721249, acc: 0.406250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "436: [D loss: 0.692413, acc: 0.515625]  [A loss: 1.112050, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "437: [D loss: 0.672616, acc: 0.562500]  [A loss: 0.628491, acc: 0.707031]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "438: [D loss: 0.735865, acc: 0.507812]  [A loss: 1.144105, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "439: [D loss: 0.676527, acc: 0.537109]  [A loss: 0.663852, acc: 0.648438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "440: [D loss: 0.715792, acc: 0.501953]  [A loss: 0.970441, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "441: [D loss: 0.682095, acc: 0.562500]  [A loss: 0.788270, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "442: [D loss: 0.669308, acc: 0.570312]  [A loss: 0.846927, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "443: [D loss: 0.681225, acc: 0.552734]  [A loss: 0.863247, acc: 0.132812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "444: [D loss: 0.667036, acc: 0.599609]  [A loss: 0.863279, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "445: [D loss: 0.696989, acc: 0.509766]  [A loss: 0.871950, acc: 0.113281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "446: [D loss: 0.680330, acc: 0.580078]  [A loss: 0.873391, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "447: [D loss: 0.684291, acc: 0.523438]  [A loss: 0.884476, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "448: [D loss: 0.670482, acc: 0.591797]  [A loss: 0.889663, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "449: [D loss: 0.669983, acc: 0.587891]  [A loss: 0.874144, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "450: [D loss: 0.671943, acc: 0.583984]  [A loss: 0.881704, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "451: [D loss: 0.673832, acc: 0.568359]  [A loss: 0.892023, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "452: [D loss: 0.680202, acc: 0.583984]  [A loss: 0.938731, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "453: [D loss: 0.685517, acc: 0.546875]  [A loss: 0.898694, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "454: [D loss: 0.673730, acc: 0.572266]  [A loss: 0.929125, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "455: [D loss: 0.673156, acc: 0.589844]  [A loss: 0.836562, acc: 0.128906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "456: [D loss: 0.695337, acc: 0.541016]  [A loss: 1.058549, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "457: [D loss: 0.660264, acc: 0.646484]  [A loss: 0.679240, acc: 0.578125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "458: [D loss: 0.706657, acc: 0.523438]  [A loss: 1.215239, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "459: [D loss: 0.678514, acc: 0.542969]  [A loss: 0.590389, acc: 0.816406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "460: [D loss: 0.734683, acc: 0.501953]  [A loss: 1.089649, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "461: [D loss: 0.675714, acc: 0.564453]  [A loss: 0.670466, acc: 0.550781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "462: [D loss: 0.730052, acc: 0.511719]  [A loss: 1.029374, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "463: [D loss: 0.670863, acc: 0.599609]  [A loss: 0.739425, acc: 0.355469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "464: [D loss: 0.692784, acc: 0.505859]  [A loss: 0.917196, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "465: [D loss: 0.672267, acc: 0.603516]  [A loss: 0.746130, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "466: [D loss: 0.682198, acc: 0.544922]  [A loss: 0.884341, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "467: [D loss: 0.681659, acc: 0.542969]  [A loss: 0.915925, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "468: [D loss: 0.669538, acc: 0.623047]  [A loss: 0.810309, acc: 0.199219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "469: [D loss: 0.687539, acc: 0.539062]  [A loss: 0.983914, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "470: [D loss: 0.673788, acc: 0.576172]  [A loss: 0.731771, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "471: [D loss: 0.692595, acc: 0.525391]  [A loss: 0.963246, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "472: [D loss: 0.662717, acc: 0.619141]  [A loss: 0.780110, acc: 0.261719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "473: [D loss: 0.673614, acc: 0.566406]  [A loss: 0.935339, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "474: [D loss: 0.677014, acc: 0.560547]  [A loss: 0.874834, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "475: [D loss: 0.673134, acc: 0.564453]  [A loss: 0.907322, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "476: [D loss: 0.677284, acc: 0.558594]  [A loss: 0.850864, acc: 0.121094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "477: [D loss: 0.680534, acc: 0.568359]  [A loss: 0.969586, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "478: [D loss: 0.674319, acc: 0.585938]  [A loss: 0.838901, acc: 0.160156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "479: [D loss: 0.683512, acc: 0.556641]  [A loss: 1.009033, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "480: [D loss: 0.676185, acc: 0.566406]  [A loss: 0.732005, acc: 0.421875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "481: [D loss: 0.697706, acc: 0.523438]  [A loss: 1.079279, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "482: [D loss: 0.669204, acc: 0.587891]  [A loss: 0.655581, acc: 0.656250]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "483: [D loss: 0.718341, acc: 0.503906]  [A loss: 1.115455, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "484: [D loss: 0.668636, acc: 0.572266]  [A loss: 0.663250, acc: 0.585938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "485: [D loss: 0.730611, acc: 0.513672]  [A loss: 1.098850, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "486: [D loss: 0.679264, acc: 0.546875]  [A loss: 0.658847, acc: 0.648438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "487: [D loss: 0.719500, acc: 0.501953]  [A loss: 0.994260, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "488: [D loss: 0.666616, acc: 0.599609]  [A loss: 0.723082, acc: 0.417969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "489: [D loss: 0.689975, acc: 0.529297]  [A loss: 0.871408, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "490: [D loss: 0.680657, acc: 0.552734]  [A loss: 0.864430, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "491: [D loss: 0.682205, acc: 0.554688]  [A loss: 0.871447, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "492: [D loss: 0.662713, acc: 0.607422]  [A loss: 0.863583, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "493: [D loss: 0.670850, acc: 0.599609]  [A loss: 0.837313, acc: 0.218750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "494: [D loss: 0.671687, acc: 0.595703]  [A loss: 0.872261, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "495: [D loss: 0.669687, acc: 0.591797]  [A loss: 0.875339, acc: 0.121094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "496: [D loss: 0.677280, acc: 0.574219]  [A loss: 0.886270, acc: 0.117188]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "497: [D loss: 0.673940, acc: 0.589844]  [A loss: 0.879854, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "498: [D loss: 0.666997, acc: 0.599609]  [A loss: 0.868068, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "499: [D loss: 0.673927, acc: 0.580078]  [A loss: 0.935343, acc: 0.058594]\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "500: [D loss: 0.673466, acc: 0.578125]  [A loss: 0.805069, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "501: [D loss: 0.680528, acc: 0.546875]  [A loss: 1.013235, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "502: [D loss: 0.659300, acc: 0.613281]  [A loss: 0.754378, acc: 0.316406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "503: [D loss: 0.684599, acc: 0.537109]  [A loss: 1.068302, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "504: [D loss: 0.681667, acc: 0.535156]  [A loss: 0.824184, acc: 0.246094]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "505: [D loss: 0.691602, acc: 0.550781]  [A loss: 1.042747, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "506: [D loss: 0.668826, acc: 0.585938]  [A loss: 0.807843, acc: 0.218750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "507: [D loss: 0.687014, acc: 0.523438]  [A loss: 1.056759, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "508: [D loss: 0.679753, acc: 0.578125]  [A loss: 0.707288, acc: 0.449219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "509: [D loss: 0.700203, acc: 0.521484]  [A loss: 1.120698, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "510: [D loss: 0.678249, acc: 0.568359]  [A loss: 0.656926, acc: 0.593750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "511: [D loss: 0.707088, acc: 0.505859]  [A loss: 1.084769, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "512: [D loss: 0.682387, acc: 0.556641]  [A loss: 0.721598, acc: 0.375000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "513: [D loss: 0.691831, acc: 0.529297]  [A loss: 0.923462, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "514: [D loss: 0.679045, acc: 0.566406]  [A loss: 0.803282, acc: 0.218750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "515: [D loss: 0.689557, acc: 0.558594]  [A loss: 0.937608, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "516: [D loss: 0.667442, acc: 0.591797]  [A loss: 0.771814, acc: 0.312500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "517: [D loss: 0.708243, acc: 0.503906]  [A loss: 1.041984, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "518: [D loss: 0.682050, acc: 0.566406]  [A loss: 0.733066, acc: 0.390625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "519: [D loss: 0.681098, acc: 0.539062]  [A loss: 0.921463, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "520: [D loss: 0.658465, acc: 0.628906]  [A loss: 0.794340, acc: 0.257812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "521: [D loss: 0.693625, acc: 0.546875]  [A loss: 1.014742, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "522: [D loss: 0.675739, acc: 0.572266]  [A loss: 0.779771, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "523: [D loss: 0.687120, acc: 0.533203]  [A loss: 1.005579, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "524: [D loss: 0.678642, acc: 0.564453]  [A loss: 0.792838, acc: 0.269531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "525: [D loss: 0.699912, acc: 0.537109]  [A loss: 0.976130, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "526: [D loss: 0.671716, acc: 0.607422]  [A loss: 0.761508, acc: 0.277344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "527: [D loss: 0.696011, acc: 0.533203]  [A loss: 1.047027, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "528: [D loss: 0.670464, acc: 0.572266]  [A loss: 0.694315, acc: 0.511719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "529: [D loss: 0.712647, acc: 0.511719]  [A loss: 1.030203, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "530: [D loss: 0.659434, acc: 0.626953]  [A loss: 0.726851, acc: 0.441406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "531: [D loss: 0.700940, acc: 0.517578]  [A loss: 1.004101, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "532: [D loss: 0.662934, acc: 0.583984]  [A loss: 0.733243, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "533: [D loss: 0.687567, acc: 0.525391]  [A loss: 0.901746, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "534: [D loss: 0.672140, acc: 0.560547]  [A loss: 0.812381, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "535: [D loss: 0.695482, acc: 0.500000]  [A loss: 0.921491, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "536: [D loss: 0.676441, acc: 0.582031]  [A loss: 0.842948, acc: 0.183594]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "537: [D loss: 0.687283, acc: 0.519531]  [A loss: 0.910700, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "538: [D loss: 0.667267, acc: 0.607422]  [A loss: 0.823941, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "539: [D loss: 0.689795, acc: 0.539062]  [A loss: 0.989747, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "540: [D loss: 0.670751, acc: 0.582031]  [A loss: 0.776647, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "541: [D loss: 0.678538, acc: 0.546875]  [A loss: 1.010662, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "542: [D loss: 0.674473, acc: 0.599609]  [A loss: 0.837187, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "543: [D loss: 0.692243, acc: 0.544922]  [A loss: 1.028195, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "544: [D loss: 0.667812, acc: 0.619141]  [A loss: 0.743872, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "545: [D loss: 0.704105, acc: 0.535156]  [A loss: 1.046639, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "546: [D loss: 0.680041, acc: 0.544922]  [A loss: 0.740064, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "547: [D loss: 0.697135, acc: 0.519531]  [A loss: 0.994467, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "548: [D loss: 0.673217, acc: 0.572266]  [A loss: 0.771499, acc: 0.285156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "549: [D loss: 0.692685, acc: 0.533203]  [A loss: 0.984582, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "550: [D loss: 0.668487, acc: 0.605469]  [A loss: 0.804292, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "551: [D loss: 0.666050, acc: 0.595703]  [A loss: 0.952305, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "552: [D loss: 0.692387, acc: 0.527344]  [A loss: 0.796263, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "553: [D loss: 0.670380, acc: 0.587891]  [A loss: 0.949997, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "554: [D loss: 0.685159, acc: 0.558594]  [A loss: 0.867463, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "555: [D loss: 0.680156, acc: 0.546875]  [A loss: 0.931323, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "556: [D loss: 0.676513, acc: 0.576172]  [A loss: 0.831682, acc: 0.171875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "557: [D loss: 0.681863, acc: 0.548828]  [A loss: 0.978911, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "558: [D loss: 0.669839, acc: 0.603516]  [A loss: 0.822542, acc: 0.210938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "559: [D loss: 0.679458, acc: 0.554688]  [A loss: 0.983150, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "560: [D loss: 0.653202, acc: 0.673828]  [A loss: 0.757096, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "561: [D loss: 0.718354, acc: 0.517578]  [A loss: 1.140980, acc: 0.000000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "562: [D loss: 0.684348, acc: 0.562500]  [A loss: 0.646145, acc: 0.671875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "563: [D loss: 0.741730, acc: 0.503906]  [A loss: 1.113914, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "564: [D loss: 0.675751, acc: 0.564453]  [A loss: 0.700934, acc: 0.457031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "565: [D loss: 0.732297, acc: 0.507812]  [A loss: 0.974946, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "566: [D loss: 0.679718, acc: 0.564453]  [A loss: 0.789435, acc: 0.261719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "567: [D loss: 0.696099, acc: 0.527344]  [A loss: 0.937124, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "568: [D loss: 0.668012, acc: 0.580078]  [A loss: 0.765025, acc: 0.328125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "569: [D loss: 0.696042, acc: 0.519531]  [A loss: 0.966218, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "570: [D loss: 0.671382, acc: 0.591797]  [A loss: 0.781847, acc: 0.261719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "571: [D loss: 0.684241, acc: 0.544922]  [A loss: 0.976221, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "572: [D loss: 0.668015, acc: 0.593750]  [A loss: 0.793842, acc: 0.242188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "573: [D loss: 0.690254, acc: 0.542969]  [A loss: 0.937979, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "574: [D loss: 0.673456, acc: 0.578125]  [A loss: 0.828538, acc: 0.199219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "575: [D loss: 0.681803, acc: 0.546875]  [A loss: 0.957693, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "576: [D loss: 0.666803, acc: 0.613281]  [A loss: 0.778432, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "577: [D loss: 0.682064, acc: 0.568359]  [A loss: 1.004990, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "578: [D loss: 0.659193, acc: 0.630859]  [A loss: 0.753122, acc: 0.347656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "579: [D loss: 0.707287, acc: 0.521484]  [A loss: 1.043913, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "580: [D loss: 0.679530, acc: 0.550781]  [A loss: 0.718987, acc: 0.484375]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "581: [D loss: 0.707708, acc: 0.537109]  [A loss: 1.054039, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "582: [D loss: 0.671557, acc: 0.558594]  [A loss: 0.764505, acc: 0.324219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "583: [D loss: 0.691969, acc: 0.531250]  [A loss: 0.979428, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "584: [D loss: 0.680148, acc: 0.542969]  [A loss: 0.724263, acc: 0.417969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "585: [D loss: 0.699550, acc: 0.542969]  [A loss: 0.987935, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "586: [D loss: 0.680794, acc: 0.560547]  [A loss: 0.720026, acc: 0.437500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "587: [D loss: 0.690691, acc: 0.541016]  [A loss: 0.939276, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "588: [D loss: 0.669204, acc: 0.585938]  [A loss: 0.783013, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "589: [D loss: 0.690840, acc: 0.535156]  [A loss: 0.961325, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "590: [D loss: 0.685895, acc: 0.560547]  [A loss: 0.806423, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "591: [D loss: 0.684342, acc: 0.578125]  [A loss: 0.918921, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "592: [D loss: 0.663124, acc: 0.595703]  [A loss: 0.826102, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "593: [D loss: 0.694776, acc: 0.541016]  [A loss: 0.985943, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "594: [D loss: 0.679012, acc: 0.574219]  [A loss: 0.790136, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "595: [D loss: 0.691587, acc: 0.527344]  [A loss: 0.959741, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "596: [D loss: 0.679389, acc: 0.562500]  [A loss: 0.794562, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "597: [D loss: 0.688308, acc: 0.548828]  [A loss: 0.972567, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "598: [D loss: 0.666671, acc: 0.615234]  [A loss: 0.797686, acc: 0.195312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "599: [D loss: 0.682812, acc: 0.550781]  [A loss: 1.038338, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "600: [D loss: 0.684493, acc: 0.560547]  [A loss: 0.692281, acc: 0.519531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "601: [D loss: 0.708516, acc: 0.507812]  [A loss: 1.077264, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "602: [D loss: 0.684623, acc: 0.544922]  [A loss: 0.750746, acc: 0.343750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "603: [D loss: 0.696840, acc: 0.527344]  [A loss: 1.010727, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "604: [D loss: 0.694124, acc: 0.525391]  [A loss: 0.786477, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "605: [D loss: 0.679567, acc: 0.562500]  [A loss: 0.937585, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "606: [D loss: 0.677039, acc: 0.578125]  [A loss: 0.808635, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "607: [D loss: 0.668654, acc: 0.580078]  [A loss: 0.930070, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "608: [D loss: 0.667269, acc: 0.603516]  [A loss: 0.808925, acc: 0.234375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "609: [D loss: 0.689137, acc: 0.531250]  [A loss: 1.022340, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "610: [D loss: 0.669959, acc: 0.595703]  [A loss: 0.786070, acc: 0.261719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "611: [D loss: 0.691430, acc: 0.527344]  [A loss: 0.946684, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "612: [D loss: 0.677257, acc: 0.580078]  [A loss: 0.800223, acc: 0.238281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "613: [D loss: 0.697485, acc: 0.527344]  [A loss: 1.013366, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "614: [D loss: 0.677510, acc: 0.552734]  [A loss: 0.859012, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "615: [D loss: 0.679943, acc: 0.578125]  [A loss: 0.913111, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "616: [D loss: 0.687809, acc: 0.500000]  [A loss: 0.867736, acc: 0.140625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "617: [D loss: 0.679417, acc: 0.564453]  [A loss: 0.945972, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "618: [D loss: 0.692903, acc: 0.527344]  [A loss: 0.894986, acc: 0.121094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "619: [D loss: 0.669824, acc: 0.597656]  [A loss: 0.902928, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "620: [D loss: 0.659814, acc: 0.632812]  [A loss: 0.870846, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "621: [D loss: 0.668792, acc: 0.570312]  [A loss: 0.983392, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "622: [D loss: 0.672322, acc: 0.580078]  [A loss: 0.889902, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "623: [D loss: 0.674768, acc: 0.583984]  [A loss: 0.921592, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "624: [D loss: 0.671763, acc: 0.574219]  [A loss: 0.928384, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "625: [D loss: 0.667842, acc: 0.589844]  [A loss: 0.952927, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "626: [D loss: 0.667828, acc: 0.597656]  [A loss: 0.879480, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "627: [D loss: 0.688429, acc: 0.552734]  [A loss: 1.086664, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "628: [D loss: 0.671641, acc: 0.597656]  [A loss: 0.681886, acc: 0.519531]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "629: [D loss: 0.747726, acc: 0.529297]  [A loss: 1.229759, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "630: [D loss: 0.685035, acc: 0.558594]  [A loss: 0.613251, acc: 0.742188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "631: [D loss: 0.753870, acc: 0.500000]  [A loss: 1.222470, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "632: [D loss: 0.698320, acc: 0.539062]  [A loss: 0.694309, acc: 0.519531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "633: [D loss: 0.717621, acc: 0.525391]  [A loss: 0.896325, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "634: [D loss: 0.681564, acc: 0.546875]  [A loss: 0.814173, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "635: [D loss: 0.671990, acc: 0.564453]  [A loss: 0.843746, acc: 0.187500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "636: [D loss: 0.668230, acc: 0.580078]  [A loss: 0.820542, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "637: [D loss: 0.683056, acc: 0.572266]  [A loss: 0.857490, acc: 0.179688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "638: [D loss: 0.684303, acc: 0.550781]  [A loss: 0.871227, acc: 0.128906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "639: [D loss: 0.670097, acc: 0.578125]  [A loss: 0.873525, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "640: [D loss: 0.671190, acc: 0.564453]  [A loss: 0.868670, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "641: [D loss: 0.678145, acc: 0.576172]  [A loss: 0.877578, acc: 0.128906]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "642: [D loss: 0.677079, acc: 0.583984]  [A loss: 0.893040, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "643: [D loss: 0.689295, acc: 0.544922]  [A loss: 0.973857, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "644: [D loss: 0.682218, acc: 0.560547]  [A loss: 0.791903, acc: 0.214844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "645: [D loss: 0.697148, acc: 0.542969]  [A loss: 0.975502, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "646: [D loss: 0.665942, acc: 0.603516]  [A loss: 0.784055, acc: 0.265625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "647: [D loss: 0.701650, acc: 0.550781]  [A loss: 1.046851, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "648: [D loss: 0.671147, acc: 0.587891]  [A loss: 0.769072, acc: 0.335938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "649: [D loss: 0.701607, acc: 0.537109]  [A loss: 1.038568, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "650: [D loss: 0.669962, acc: 0.582031]  [A loss: 0.751273, acc: 0.339844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "651: [D loss: 0.690070, acc: 0.544922]  [A loss: 1.049645, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "652: [D loss: 0.669025, acc: 0.591797]  [A loss: 0.833360, acc: 0.179688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "653: [D loss: 0.679729, acc: 0.544922]  [A loss: 0.951951, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "654: [D loss: 0.673956, acc: 0.564453]  [A loss: 0.840711, acc: 0.199219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "655: [D loss: 0.679057, acc: 0.576172]  [A loss: 0.931594, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "656: [D loss: 0.677129, acc: 0.566406]  [A loss: 0.908960, acc: 0.113281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "657: [D loss: 0.654008, acc: 0.583984]  [A loss: 0.951891, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "658: [D loss: 0.676797, acc: 0.548828]  [A loss: 0.958804, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "659: [D loss: 0.665896, acc: 0.595703]  [A loss: 0.863180, acc: 0.195312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "660: [D loss: 0.722416, acc: 0.537109]  [A loss: 1.109089, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "661: [D loss: 0.691435, acc: 0.546875]  [A loss: 0.782487, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "662: [D loss: 0.713927, acc: 0.515625]  [A loss: 1.117870, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "663: [D loss: 0.659253, acc: 0.605469]  [A loss: 0.670579, acc: 0.570312]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "664: [D loss: 0.732276, acc: 0.505859]  [A loss: 1.099780, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "665: [D loss: 0.676659, acc: 0.572266]  [A loss: 0.703230, acc: 0.464844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "666: [D loss: 0.706381, acc: 0.519531]  [A loss: 1.040886, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "667: [D loss: 0.684329, acc: 0.566406]  [A loss: 0.793173, acc: 0.289062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "668: [D loss: 0.685358, acc: 0.554688]  [A loss: 0.938291, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "669: [D loss: 0.671116, acc: 0.626953]  [A loss: 0.800665, acc: 0.246094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "670: [D loss: 0.689360, acc: 0.568359]  [A loss: 0.948026, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "671: [D loss: 0.688278, acc: 0.570312]  [A loss: 0.818298, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "672: [D loss: 0.689385, acc: 0.539062]  [A loss: 0.942234, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "673: [D loss: 0.684656, acc: 0.562500]  [A loss: 0.864300, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "674: [D loss: 0.672829, acc: 0.582031]  [A loss: 0.909288, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "675: [D loss: 0.667159, acc: 0.593750]  [A loss: 0.895751, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "676: [D loss: 0.679386, acc: 0.537109]  [A loss: 0.915655, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "677: [D loss: 0.670950, acc: 0.566406]  [A loss: 0.929363, acc: 0.121094]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "678: [D loss: 0.666554, acc: 0.574219]  [A loss: 0.870148, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "679: [D loss: 0.665734, acc: 0.582031]  [A loss: 0.925616, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "680: [D loss: 0.675632, acc: 0.570312]  [A loss: 0.864916, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "681: [D loss: 0.672779, acc: 0.558594]  [A loss: 1.000309, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "682: [D loss: 0.655098, acc: 0.640625]  [A loss: 0.864681, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "683: [D loss: 0.679709, acc: 0.582031]  [A loss: 1.033471, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "684: [D loss: 0.670879, acc: 0.585938]  [A loss: 0.803907, acc: 0.281250]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "685: [D loss: 0.699877, acc: 0.535156]  [A loss: 1.218821, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "686: [D loss: 0.676924, acc: 0.564453]  [A loss: 0.654006, acc: 0.574219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "687: [D loss: 0.737180, acc: 0.511719]  [A loss: 1.156708, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "688: [D loss: 0.685772, acc: 0.548828]  [A loss: 0.676169, acc: 0.582031]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "689: [D loss: 0.724872, acc: 0.525391]  [A loss: 1.087602, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "690: [D loss: 0.679676, acc: 0.570312]  [A loss: 0.810232, acc: 0.246094]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "691: [D loss: 0.681993, acc: 0.566406]  [A loss: 0.897132, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "692: [D loss: 0.674444, acc: 0.593750]  [A loss: 0.861617, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "693: [D loss: 0.671481, acc: 0.580078]  [A loss: 0.902687, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "694: [D loss: 0.685462, acc: 0.539062]  [A loss: 0.850135, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "695: [D loss: 0.680969, acc: 0.556641]  [A loss: 0.899753, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "696: [D loss: 0.673471, acc: 0.578125]  [A loss: 0.849412, acc: 0.160156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "697: [D loss: 0.683023, acc: 0.558594]  [A loss: 0.990553, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "698: [D loss: 0.682077, acc: 0.560547]  [A loss: 0.816139, acc: 0.226562]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "699: [D loss: 0.670039, acc: 0.548828]  [A loss: 0.993350, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "700: [D loss: 0.676285, acc: 0.599609]  [A loss: 0.808863, acc: 0.253906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "701: [D loss: 0.716401, acc: 0.531250]  [A loss: 1.009280, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "702: [D loss: 0.668175, acc: 0.580078]  [A loss: 0.795056, acc: 0.289062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "703: [D loss: 0.699235, acc: 0.541016]  [A loss: 0.988595, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "704: [D loss: 0.684730, acc: 0.525391]  [A loss: 0.818639, acc: 0.246094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "705: [D loss: 0.702966, acc: 0.556641]  [A loss: 1.029306, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "706: [D loss: 0.681681, acc: 0.552734]  [A loss: 0.789213, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "707: [D loss: 0.693159, acc: 0.541016]  [A loss: 1.029719, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "708: [D loss: 0.667596, acc: 0.574219]  [A loss: 0.762967, acc: 0.324219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "709: [D loss: 0.697114, acc: 0.531250]  [A loss: 1.009261, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "710: [D loss: 0.680323, acc: 0.556641]  [A loss: 0.818493, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "711: [D loss: 0.681160, acc: 0.591797]  [A loss: 0.976738, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "712: [D loss: 0.672828, acc: 0.587891]  [A loss: 0.834422, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "713: [D loss: 0.700105, acc: 0.544922]  [A loss: 1.072989, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "714: [D loss: 0.682633, acc: 0.550781]  [A loss: 0.752443, acc: 0.382812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "715: [D loss: 0.704908, acc: 0.515625]  [A loss: 1.148520, acc: 0.007812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "716: [D loss: 0.678861, acc: 0.546875]  [A loss: 0.686074, acc: 0.539062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "717: [D loss: 0.710594, acc: 0.521484]  [A loss: 1.115636, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "718: [D loss: 0.682713, acc: 0.541016]  [A loss: 0.715956, acc: 0.472656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "719: [D loss: 0.701936, acc: 0.519531]  [A loss: 1.055935, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "720: [D loss: 0.685135, acc: 0.572266]  [A loss: 0.732816, acc: 0.386719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "721: [D loss: 0.705429, acc: 0.500000]  [A loss: 1.001966, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "722: [D loss: 0.710155, acc: 0.494141]  [A loss: 0.921766, acc: 0.128906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "723: [D loss: 0.688729, acc: 0.578125]  [A loss: 0.917668, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "724: [D loss: 0.671684, acc: 0.603516]  [A loss: 0.844291, acc: 0.187500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "725: [D loss: 0.672508, acc: 0.578125]  [A loss: 0.897422, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "726: [D loss: 0.678368, acc: 0.587891]  [A loss: 0.935182, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "727: [D loss: 0.674482, acc: 0.580078]  [A loss: 0.836556, acc: 0.234375]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "728: [D loss: 0.708185, acc: 0.523438]  [A loss: 0.992611, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "729: [D loss: 0.690657, acc: 0.535156]  [A loss: 0.842776, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "730: [D loss: 0.670730, acc: 0.597656]  [A loss: 0.923537, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "731: [D loss: 0.679839, acc: 0.560547]  [A loss: 0.956567, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "732: [D loss: 0.664516, acc: 0.583984]  [A loss: 0.868713, acc: 0.171875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "733: [D loss: 0.677911, acc: 0.570312]  [A loss: 1.038265, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "734: [D loss: 0.669907, acc: 0.587891]  [A loss: 0.847872, acc: 0.187500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "735: [D loss: 0.704377, acc: 0.533203]  [A loss: 1.064834, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "736: [D loss: 0.680994, acc: 0.580078]  [A loss: 0.791222, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "737: [D loss: 0.702628, acc: 0.539062]  [A loss: 1.103543, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "738: [D loss: 0.682273, acc: 0.568359]  [A loss: 0.704928, acc: 0.460938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "739: [D loss: 0.731436, acc: 0.500000]  [A loss: 1.154445, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "740: [D loss: 0.680168, acc: 0.550781]  [A loss: 0.684247, acc: 0.539062]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "741: [D loss: 0.711755, acc: 0.527344]  [A loss: 1.071435, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "742: [D loss: 0.693671, acc: 0.550781]  [A loss: 0.728598, acc: 0.441406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "743: [D loss: 0.700416, acc: 0.533203]  [A loss: 1.034247, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "744: [D loss: 0.674949, acc: 0.574219]  [A loss: 0.777174, acc: 0.332031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "745: [D loss: 0.693796, acc: 0.541016]  [A loss: 0.969716, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "746: [D loss: 0.668390, acc: 0.570312]  [A loss: 0.795552, acc: 0.257812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "747: [D loss: 0.695950, acc: 0.544922]  [A loss: 0.970597, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "748: [D loss: 0.669640, acc: 0.580078]  [A loss: 0.800876, acc: 0.312500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "749: [D loss: 0.687760, acc: 0.570312]  [A loss: 1.011719, acc: 0.031250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "750: [D loss: 0.679628, acc: 0.560547]  [A loss: 0.754059, acc: 0.328125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "751: [D loss: 0.686509, acc: 0.537109]  [A loss: 1.017040, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "752: [D loss: 0.667813, acc: 0.574219]  [A loss: 0.744241, acc: 0.371094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "753: [D loss: 0.686650, acc: 0.556641]  [A loss: 1.010330, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "754: [D loss: 0.666267, acc: 0.576172]  [A loss: 0.810959, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "755: [D loss: 0.709448, acc: 0.537109]  [A loss: 1.073523, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "756: [D loss: 0.684352, acc: 0.562500]  [A loss: 0.762783, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "757: [D loss: 0.697806, acc: 0.550781]  [A loss: 1.062444, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "758: [D loss: 0.675851, acc: 0.585938]  [A loss: 0.756050, acc: 0.335938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "759: [D loss: 0.726297, acc: 0.509766]  [A loss: 0.974191, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "760: [D loss: 0.695157, acc: 0.546875]  [A loss: 0.975833, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "761: [D loss: 0.683480, acc: 0.546875]  [A loss: 0.753343, acc: 0.386719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "762: [D loss: 0.691750, acc: 0.533203]  [A loss: 1.038126, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "763: [D loss: 0.678104, acc: 0.562500]  [A loss: 0.750069, acc: 0.382812]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "764: [D loss: 0.712433, acc: 0.517578]  [A loss: 1.019616, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "765: [D loss: 0.690080, acc: 0.541016]  [A loss: 0.793558, acc: 0.257812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "766: [D loss: 0.682595, acc: 0.556641]  [A loss: 0.944803, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "767: [D loss: 0.679717, acc: 0.568359]  [A loss: 0.850584, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "768: [D loss: 0.681833, acc: 0.560547]  [A loss: 0.984351, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "769: [D loss: 0.672326, acc: 0.568359]  [A loss: 0.811557, acc: 0.269531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "770: [D loss: 0.678914, acc: 0.585938]  [A loss: 1.014451, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "771: [D loss: 0.674003, acc: 0.568359]  [A loss: 0.777188, acc: 0.320312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "772: [D loss: 0.700568, acc: 0.527344]  [A loss: 1.024998, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "773: [D loss: 0.701248, acc: 0.527344]  [A loss: 0.879687, acc: 0.125000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "774: [D loss: 0.662976, acc: 0.599609]  [A loss: 0.899818, acc: 0.140625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "775: [D loss: 0.684877, acc: 0.529297]  [A loss: 0.869242, acc: 0.171875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "776: [D loss: 0.669457, acc: 0.564453]  [A loss: 0.910693, acc: 0.128906]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "777: [D loss: 0.666873, acc: 0.587891]  [A loss: 0.826428, acc: 0.257812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "778: [D loss: 0.712892, acc: 0.537109]  [A loss: 1.014641, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "779: [D loss: 0.674360, acc: 0.589844]  [A loss: 0.743124, acc: 0.382812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "780: [D loss: 0.706826, acc: 0.529297]  [A loss: 1.148854, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "781: [D loss: 0.656992, acc: 0.623047]  [A loss: 0.690130, acc: 0.531250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "782: [D loss: 0.741173, acc: 0.505859]  [A loss: 1.146218, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "783: [D loss: 0.698802, acc: 0.542969]  [A loss: 0.693577, acc: 0.535156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "784: [D loss: 0.732315, acc: 0.505859]  [A loss: 1.136784, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "785: [D loss: 0.679043, acc: 0.585938]  [A loss: 0.686594, acc: 0.546875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "786: [D loss: 0.711219, acc: 0.513672]  [A loss: 0.984552, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "787: [D loss: 0.690661, acc: 0.535156]  [A loss: 0.806624, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "788: [D loss: 0.690697, acc: 0.546875]  [A loss: 0.886725, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "789: [D loss: 0.671765, acc: 0.597656]  [A loss: 0.805777, acc: 0.281250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "790: [D loss: 0.686423, acc: 0.542969]  [A loss: 0.874797, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "791: [D loss: 0.669500, acc: 0.587891]  [A loss: 0.860083, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "792: [D loss: 0.674760, acc: 0.556641]  [A loss: 0.893233, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "793: [D loss: 0.684933, acc: 0.556641]  [A loss: 0.875298, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "794: [D loss: 0.680563, acc: 0.566406]  [A loss: 0.888382, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "795: [D loss: 0.670675, acc: 0.580078]  [A loss: 0.864315, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "796: [D loss: 0.693310, acc: 0.539062]  [A loss: 0.876900, acc: 0.171875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "797: [D loss: 0.686687, acc: 0.523438]  [A loss: 0.949123, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "798: [D loss: 0.676075, acc: 0.560547]  [A loss: 0.829317, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "799: [D loss: 0.679312, acc: 0.578125]  [A loss: 0.956178, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "800: [D loss: 0.678415, acc: 0.570312]  [A loss: 0.847864, acc: 0.203125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "801: [D loss: 0.688480, acc: 0.539062]  [A loss: 0.973881, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "802: [D loss: 0.671765, acc: 0.576172]  [A loss: 0.808807, acc: 0.277344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "803: [D loss: 0.717007, acc: 0.521484]  [A loss: 1.085487, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "804: [D loss: 0.695740, acc: 0.529297]  [A loss: 0.785318, acc: 0.253906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "805: [D loss: 0.709618, acc: 0.509766]  [A loss: 1.131253, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "806: [D loss: 0.679502, acc: 0.570312]  [A loss: 0.694495, acc: 0.511719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "807: [D loss: 0.733157, acc: 0.515625]  [A loss: 1.148813, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "808: [D loss: 0.682570, acc: 0.566406]  [A loss: 0.664555, acc: 0.589844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "809: [D loss: 0.728685, acc: 0.521484]  [A loss: 1.037919, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "810: [D loss: 0.678596, acc: 0.560547]  [A loss: 0.780255, acc: 0.332031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "811: [D loss: 0.702166, acc: 0.531250]  [A loss: 0.897778, acc: 0.140625]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "812: [D loss: 0.687676, acc: 0.552734]  [A loss: 0.838543, acc: 0.214844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "813: [D loss: 0.677929, acc: 0.564453]  [A loss: 0.883982, acc: 0.148438]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "814: [D loss: 0.675548, acc: 0.578125]  [A loss: 0.801395, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "815: [D loss: 0.704486, acc: 0.525391]  [A loss: 0.999777, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "816: [D loss: 0.671128, acc: 0.580078]  [A loss: 0.808735, acc: 0.277344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "817: [D loss: 0.696355, acc: 0.542969]  [A loss: 0.946546, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "818: [D loss: 0.673057, acc: 0.583984]  [A loss: 0.852449, acc: 0.195312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "819: [D loss: 0.679983, acc: 0.558594]  [A loss: 0.873045, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "820: [D loss: 0.662802, acc: 0.617188]  [A loss: 0.874972, acc: 0.160156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "821: [D loss: 0.689041, acc: 0.542969]  [A loss: 0.957744, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "822: [D loss: 0.671006, acc: 0.580078]  [A loss: 0.845922, acc: 0.242188]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "823: [D loss: 0.692792, acc: 0.556641]  [A loss: 0.958098, acc: 0.058594]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "824: [D loss: 0.673605, acc: 0.582031]  [A loss: 0.776795, acc: 0.332031]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "825: [D loss: 0.691922, acc: 0.568359]  [A loss: 1.043692, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "826: [D loss: 0.678356, acc: 0.568359]  [A loss: 0.738725, acc: 0.445312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "827: [D loss: 0.727975, acc: 0.488281]  [A loss: 1.086823, acc: 0.019531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "828: [D loss: 0.679899, acc: 0.560547]  [A loss: 0.681596, acc: 0.558594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "829: [D loss: 0.727768, acc: 0.535156]  [A loss: 1.165892, acc: 0.023438]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "830: [D loss: 0.681045, acc: 0.552734]  [A loss: 0.711940, acc: 0.468750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "831: [D loss: 0.706204, acc: 0.523438]  [A loss: 1.011897, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "832: [D loss: 0.693609, acc: 0.544922]  [A loss: 0.788028, acc: 0.328125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "833: [D loss: 0.683580, acc: 0.550781]  [A loss: 0.959724, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "834: [D loss: 0.676840, acc: 0.542969]  [A loss: 0.776214, acc: 0.324219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "835: [D loss: 0.684997, acc: 0.564453]  [A loss: 0.939871, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "836: [D loss: 0.668250, acc: 0.597656]  [A loss: 0.829640, acc: 0.238281]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "837: [D loss: 0.688648, acc: 0.541016]  [A loss: 1.000106, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "838: [D loss: 0.673579, acc: 0.591797]  [A loss: 0.802047, acc: 0.253906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "839: [D loss: 0.685225, acc: 0.541016]  [A loss: 1.006119, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "840: [D loss: 0.676712, acc: 0.578125]  [A loss: 0.766287, acc: 0.375000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "841: [D loss: 0.693058, acc: 0.546875]  [A loss: 1.047251, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "842: [D loss: 0.667822, acc: 0.572266]  [A loss: 0.756530, acc: 0.378906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "843: [D loss: 0.700781, acc: 0.531250]  [A loss: 1.053493, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "844: [D loss: 0.680283, acc: 0.556641]  [A loss: 0.706865, acc: 0.527344]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "845: [D loss: 0.722967, acc: 0.521484]  [A loss: 1.046157, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "846: [D loss: 0.689807, acc: 0.546875]  [A loss: 0.779041, acc: 0.367188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "847: [D loss: 0.695566, acc: 0.533203]  [A loss: 0.928515, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "848: [D loss: 0.674412, acc: 0.593750]  [A loss: 0.795466, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "849: [D loss: 0.679198, acc: 0.531250]  [A loss: 0.883750, acc: 0.132812]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "850: [D loss: 0.686209, acc: 0.546875]  [A loss: 0.907089, acc: 0.121094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "851: [D loss: 0.680623, acc: 0.583984]  [A loss: 0.857322, acc: 0.191406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "852: [D loss: 0.676757, acc: 0.558594]  [A loss: 0.919191, acc: 0.109375]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "853: [D loss: 0.706181, acc: 0.541016]  [A loss: 0.875793, acc: 0.179688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "854: [D loss: 0.693766, acc: 0.535156]  [A loss: 1.109626, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "855: [D loss: 0.665789, acc: 0.582031]  [A loss: 0.703057, acc: 0.480469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "856: [D loss: 0.717738, acc: 0.511719]  [A loss: 1.095834, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "857: [D loss: 0.681188, acc: 0.572266]  [A loss: 0.732104, acc: 0.453125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "858: [D loss: 0.722951, acc: 0.523438]  [A loss: 0.986932, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "859: [D loss: 0.684338, acc: 0.552734]  [A loss: 0.799220, acc: 0.300781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "860: [D loss: 0.679779, acc: 0.556641]  [A loss: 0.919165, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "861: [D loss: 0.675780, acc: 0.599609]  [A loss: 0.801859, acc: 0.269531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "862: [D loss: 0.693710, acc: 0.546875]  [A loss: 0.933596, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "863: [D loss: 0.690103, acc: 0.539062]  [A loss: 0.892343, acc: 0.101562]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "864: [D loss: 0.688727, acc: 0.552734]  [A loss: 0.972548, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "865: [D loss: 0.669366, acc: 0.611328]  [A loss: 0.840230, acc: 0.214844]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "866: [D loss: 0.673546, acc: 0.570312]  [A loss: 0.893607, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "867: [D loss: 0.674893, acc: 0.578125]  [A loss: 0.821296, acc: 0.238281]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "868: [D loss: 0.676608, acc: 0.583984]  [A loss: 0.954109, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "869: [D loss: 0.669815, acc: 0.617188]  [A loss: 0.751676, acc: 0.394531]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "870: [D loss: 0.713997, acc: 0.546875]  [A loss: 1.040689, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "871: [D loss: 0.687271, acc: 0.550781]  [A loss: 0.806129, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "872: [D loss: 0.702563, acc: 0.546875]  [A loss: 1.101770, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "873: [D loss: 0.675874, acc: 0.570312]  [A loss: 0.679556, acc: 0.582031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "874: [D loss: 0.711930, acc: 0.511719]  [A loss: 1.114119, acc: 0.015625]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "875: [D loss: 0.677826, acc: 0.570312]  [A loss: 0.774489, acc: 0.296875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "876: [D loss: 0.692583, acc: 0.533203]  [A loss: 0.980310, acc: 0.062500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "877: [D loss: 0.680665, acc: 0.560547]  [A loss: 0.789369, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "878: [D loss: 0.693666, acc: 0.539062]  [A loss: 0.983242, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "879: [D loss: 0.658666, acc: 0.625000]  [A loss: 0.787776, acc: 0.308594]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "880: [D loss: 0.697264, acc: 0.539062]  [A loss: 1.055025, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "881: [D loss: 0.678086, acc: 0.560547]  [A loss: 0.771393, acc: 0.347656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "882: [D loss: 0.691680, acc: 0.544922]  [A loss: 0.956104, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "883: [D loss: 0.668748, acc: 0.611328]  [A loss: 0.791211, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "884: [D loss: 0.679579, acc: 0.572266]  [A loss: 0.969814, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "885: [D loss: 0.678993, acc: 0.560547]  [A loss: 0.861544, acc: 0.199219]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "886: [D loss: 0.695027, acc: 0.527344]  [A loss: 0.905949, acc: 0.144531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "887: [D loss: 0.676345, acc: 0.556641]  [A loss: 0.836089, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "888: [D loss: 0.683169, acc: 0.582031]  [A loss: 0.912666, acc: 0.136719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "889: [D loss: 0.683683, acc: 0.548828]  [A loss: 0.856715, acc: 0.167969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "890: [D loss: 0.688596, acc: 0.552734]  [A loss: 0.979877, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "891: [D loss: 0.675643, acc: 0.554688]  [A loss: 0.814575, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "892: [D loss: 0.682715, acc: 0.595703]  [A loss: 0.953472, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "893: [D loss: 0.675972, acc: 0.578125]  [A loss: 0.833360, acc: 0.242188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "894: [D loss: 0.691490, acc: 0.556641]  [A loss: 1.043873, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "895: [D loss: 0.688008, acc: 0.566406]  [A loss: 0.767360, acc: 0.351562]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "896: [D loss: 0.679881, acc: 0.558594]  [A loss: 1.048756, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "897: [D loss: 0.703098, acc: 0.521484]  [A loss: 0.851180, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "898: [D loss: 0.689084, acc: 0.556641]  [A loss: 1.035484, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "899: [D loss: 0.681356, acc: 0.566406]  [A loss: 0.760388, acc: 0.386719]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "900: [D loss: 0.714571, acc: 0.521484]  [A loss: 1.169938, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "901: [D loss: 0.679350, acc: 0.566406]  [A loss: 0.664513, acc: 0.585938]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "902: [D loss: 0.766871, acc: 0.498047]  [A loss: 1.097689, acc: 0.003906]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "903: [D loss: 0.686276, acc: 0.533203]  [A loss: 0.667893, acc: 0.617188]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "904: [D loss: 0.751307, acc: 0.503906]  [A loss: 1.086298, acc: 0.039062]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "905: [D loss: 0.695149, acc: 0.531250]  [A loss: 0.746684, acc: 0.406250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "906: [D loss: 0.704932, acc: 0.531250]  [A loss: 0.886667, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "907: [D loss: 0.683560, acc: 0.564453]  [A loss: 0.851602, acc: 0.218750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "908: [D loss: 0.683040, acc: 0.568359]  [A loss: 0.834800, acc: 0.179688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "909: [D loss: 0.687099, acc: 0.560547]  [A loss: 0.920425, acc: 0.093750]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "910: [D loss: 0.687820, acc: 0.552734]  [A loss: 0.822914, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "911: [D loss: 0.688091, acc: 0.539062]  [A loss: 0.953994, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "912: [D loss: 0.671417, acc: 0.597656]  [A loss: 0.793300, acc: 0.261719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "913: [D loss: 0.692762, acc: 0.560547]  [A loss: 0.996805, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "914: [D loss: 0.683758, acc: 0.535156]  [A loss: 0.777166, acc: 0.332031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "915: [D loss: 0.690553, acc: 0.542969]  [A loss: 0.991721, acc: 0.070312]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "916: [D loss: 0.684373, acc: 0.558594]  [A loss: 0.802010, acc: 0.328125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "917: [D loss: 0.690992, acc: 0.541016]  [A loss: 0.947141, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "918: [D loss: 0.680033, acc: 0.554688]  [A loss: 0.929275, acc: 0.117188]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "919: [D loss: 0.685230, acc: 0.562500]  [A loss: 0.888784, acc: 0.167969]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "920: [D loss: 0.687463, acc: 0.523438]  [A loss: 0.870560, acc: 0.187500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "921: [D loss: 0.690528, acc: 0.531250]  [A loss: 1.086560, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "922: [D loss: 0.685953, acc: 0.554688]  [A loss: 0.717504, acc: 0.480469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "923: [D loss: 0.696876, acc: 0.554688]  [A loss: 1.065044, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "924: [D loss: 0.670588, acc: 0.583984]  [A loss: 0.725327, acc: 0.449219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "925: [D loss: 0.740341, acc: 0.498047]  [A loss: 1.069779, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "926: [D loss: 0.666721, acc: 0.603516]  [A loss: 0.718486, acc: 0.472656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "927: [D loss: 0.724004, acc: 0.521484]  [A loss: 1.134227, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "928: [D loss: 0.675212, acc: 0.595703]  [A loss: 0.681262, acc: 0.574219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "929: [D loss: 0.749115, acc: 0.486328]  [A loss: 1.004501, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "930: [D loss: 0.691340, acc: 0.527344]  [A loss: 0.792731, acc: 0.281250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "931: [D loss: 0.698047, acc: 0.554688]  [A loss: 0.983436, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "932: [D loss: 0.678020, acc: 0.578125]  [A loss: 0.754400, acc: 0.359375]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "933: [D loss: 0.702710, acc: 0.521484]  [A loss: 0.938428, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "934: [D loss: 0.690686, acc: 0.523438]  [A loss: 0.815590, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "935: [D loss: 0.683743, acc: 0.552734]  [A loss: 0.958721, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "936: [D loss: 0.672391, acc: 0.626953]  [A loss: 0.780899, acc: 0.328125]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "937: [D loss: 0.689169, acc: 0.525391]  [A loss: 0.963439, acc: 0.085938]\n",
            "8/8 [==============================] - 0s 6ms/step\n",
            "938: [D loss: 0.680934, acc: 0.570312]  [A loss: 0.855307, acc: 0.214844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "939: [D loss: 0.677221, acc: 0.566406]  [A loss: 0.971125, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "940: [D loss: 0.674059, acc: 0.572266]  [A loss: 0.801412, acc: 0.292969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "941: [D loss: 0.687423, acc: 0.562500]  [A loss: 1.002343, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "942: [D loss: 0.675082, acc: 0.597656]  [A loss: 0.739213, acc: 0.394531]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "943: [D loss: 0.708749, acc: 0.513672]  [A loss: 1.128787, acc: 0.011719]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "944: [D loss: 0.677883, acc: 0.578125]  [A loss: 0.677089, acc: 0.582031]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "945: [D loss: 0.694266, acc: 0.541016]  [A loss: 1.009367, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "946: [D loss: 0.666949, acc: 0.593750]  [A loss: 0.742568, acc: 0.394531]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "947: [D loss: 0.714385, acc: 0.548828]  [A loss: 1.010771, acc: 0.046875]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "948: [D loss: 0.681741, acc: 0.558594]  [A loss: 0.771682, acc: 0.367188]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "949: [D loss: 0.706834, acc: 0.517578]  [A loss: 0.928896, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "950: [D loss: 0.663931, acc: 0.597656]  [A loss: 0.732997, acc: 0.472656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "951: [D loss: 0.717546, acc: 0.519531]  [A loss: 1.066149, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "952: [D loss: 0.685656, acc: 0.566406]  [A loss: 0.706368, acc: 0.507812]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "953: [D loss: 0.713598, acc: 0.527344]  [A loss: 1.028585, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "954: [D loss: 0.671221, acc: 0.591797]  [A loss: 0.728967, acc: 0.433594]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "955: [D loss: 0.717765, acc: 0.527344]  [A loss: 1.016455, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "956: [D loss: 0.670814, acc: 0.611328]  [A loss: 0.748558, acc: 0.371094]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "957: [D loss: 0.684640, acc: 0.582031]  [A loss: 0.988093, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "958: [D loss: 0.677593, acc: 0.558594]  [A loss: 0.856064, acc: 0.175781]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "959: [D loss: 0.683648, acc: 0.568359]  [A loss: 0.890944, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "960: [D loss: 0.681768, acc: 0.560547]  [A loss: 0.872927, acc: 0.160156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "961: [D loss: 0.678328, acc: 0.599609]  [A loss: 0.848004, acc: 0.207031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "962: [D loss: 0.693824, acc: 0.521484]  [A loss: 0.948122, acc: 0.097656]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "963: [D loss: 0.688895, acc: 0.548828]  [A loss: 0.790903, acc: 0.316406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "964: [D loss: 0.705316, acc: 0.535156]  [A loss: 0.966346, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "965: [D loss: 0.679335, acc: 0.580078]  [A loss: 0.875556, acc: 0.113281]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "966: [D loss: 0.697602, acc: 0.544922]  [A loss: 1.046715, acc: 0.054688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "967: [D loss: 0.677090, acc: 0.593750]  [A loss: 0.799418, acc: 0.304688]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "968: [D loss: 0.698623, acc: 0.558594]  [A loss: 1.003983, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "969: [D loss: 0.685902, acc: 0.533203]  [A loss: 0.820971, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "970: [D loss: 0.691680, acc: 0.537109]  [A loss: 1.000906, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "971: [D loss: 0.685420, acc: 0.556641]  [A loss: 0.827814, acc: 0.230469]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "972: [D loss: 0.690914, acc: 0.550781]  [A loss: 1.001515, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "973: [D loss: 0.683640, acc: 0.566406]  [A loss: 0.728819, acc: 0.429688]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "974: [D loss: 0.722529, acc: 0.527344]  [A loss: 1.140775, acc: 0.027344]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "975: [D loss: 0.686439, acc: 0.550781]  [A loss: 0.631668, acc: 0.687500]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "976: [D loss: 0.724883, acc: 0.525391]  [A loss: 1.094224, acc: 0.035156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "977: [D loss: 0.658775, acc: 0.601562]  [A loss: 0.779174, acc: 0.347656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "978: [D loss: 0.699672, acc: 0.519531]  [A loss: 0.949108, acc: 0.105469]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "979: [D loss: 0.685624, acc: 0.578125]  [A loss: 0.850304, acc: 0.222656]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "980: [D loss: 0.692584, acc: 0.533203]  [A loss: 0.871025, acc: 0.160156]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "981: [D loss: 0.682474, acc: 0.566406]  [A loss: 0.901837, acc: 0.156250]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "982: [D loss: 0.671688, acc: 0.591797]  [A loss: 0.835021, acc: 0.238281]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "983: [D loss: 0.701414, acc: 0.523438]  [A loss: 0.994386, acc: 0.074219]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "984: [D loss: 0.677021, acc: 0.585938]  [A loss: 0.791256, acc: 0.316406]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "985: [D loss: 0.690128, acc: 0.535156]  [A loss: 0.993289, acc: 0.089844]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "986: [D loss: 0.679057, acc: 0.564453]  [A loss: 0.732954, acc: 0.496094]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "987: [D loss: 0.697045, acc: 0.542969]  [A loss: 1.061113, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "988: [D loss: 0.693715, acc: 0.535156]  [A loss: 0.799681, acc: 0.320312]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "989: [D loss: 0.713934, acc: 0.539062]  [A loss: 1.060667, acc: 0.042969]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "990: [D loss: 0.697725, acc: 0.521484]  [A loss: 0.806640, acc: 0.250000]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "991: [D loss: 0.699395, acc: 0.558594]  [A loss: 0.985049, acc: 0.078125]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "992: [D loss: 0.669347, acc: 0.609375]  [A loss: 0.770438, acc: 0.382812]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "993: [D loss: 0.713817, acc: 0.548828]  [A loss: 1.037884, acc: 0.050781]\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "994: [D loss: 0.678920, acc: 0.587891]  [A loss: 0.807716, acc: 0.277344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "995: [D loss: 0.696525, acc: 0.544922]  [A loss: 1.053278, acc: 0.066406]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "996: [D loss: 0.683146, acc: 0.546875]  [A loss: 0.757695, acc: 0.382812]\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "997: [D loss: 0.699606, acc: 0.535156]  [A loss: 0.980264, acc: 0.082031]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "998: [D loss: 0.687288, acc: 0.552734]  [A loss: 0.803830, acc: 0.277344]\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "999: [D loss: 0.690738, acc: 0.541016]  [A loss: 1.028960, acc: 0.062500]\n",
            "1/1 [==============================] - 0s 16ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 합성 MNIST 이미지 배치 생성"
      ],
      "metadata": {
        "id": "yxNoUgSb1L3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timer.elapsed_time()\n",
        "mnist_dcgan.plot_images(fake=True)\n",
        "mnist_dcgan.plot_images(fake=False, save2file=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "w9z6CRev1FoB",
        "outputId": "afa71feb-7320-424c-bd06-9ba43d7ad195"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed: 4.846509742736816 min \n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}