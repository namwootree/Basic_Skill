# 비지도 학습 개요

#### 비지도 학습

* 비지도 학습에서는 레이블을 사용할 수 없음
* AI 에이전트의 작업이 명확히 정의되지 않으므로 모델의 성능을 명확히 측정할 수 없다
* 데이터에서 새로운 패턴을 찾는다는 관점에서 이러한 비지도 학습 기반 시스템은 지도 학습 기반 시스템보다 더 우수하다

---

# 비지도 학습의 강점과 약점

#### 강점

* 패턴을 알 수 없거나 끊임없이 변화하거나 레이블 데이터가 충분히 확보되지 않은 문제 영역에서는 비지도 학습이 우월
  - 데이터의 레이블이 아닌 데이터 자체의 내재된 구조를 학습해 작동
  - 데이터 셋이 보유한 데이터 레코드 수보다 훨씬 작은 수의 매개변수 집합으로 훈련해 데이터를 표현하려는 시도를 함
  - 비지도 학습은 이러한 표현 학습을 수행합으로써 데이터 셋의 고유 패턴을 식별

#### 약점

* 비지도 학습 AI 자체가 스스로 '의자' 또는 '개'로 표시할 수 없음
  - 하지만 비슷한 이미지가 함께 그룹화됐기 때문에 이후 우리는 훨씬 더 간단히 레이블을 지정할 수 있음
  - 비지도 학습 AI가 레이블이 지정된 어떤 그룹에도 속하지 않는 이미지를 찾는 경우, 
    AI는 분류되지 않은 이미지를 레이블이 지정되지 않은 별도의 새 이미지 그룹으로 만들어 사람이 레이블을 직접 지정하도록 유도
    
---
 
# 비지도 학습을 사용해 머신러닝 솔루션 개선하기

## 레이블 데이터는 언제나 부족하다

* 모든 데이터가 평등하게 생성되는 것은 아니다
* 비지도 학습을 통해 레이블이 지정되지 않은 데이터에 자동으로 레이블을 지정할 수 있다

## 과대 적합

* 훈련 데이터에서 지나치게 복잡한 함수를 학습하면 홀드아웃 데이터 셋에 존재하는 새로운 인스턴스에서 제대로 수행되지 않을 수 있음
* 비지도 학습을 정규화기로 사용하면 이러한 문제를 해결할 수 있음
* 원본 입력 데이터를 지도 학습 알고리즘에 직접 공급하는 대신 비지도 사전 훈련을 통해 생성된 원본 입력 데이터의 새로운 형태 표현을 제공할 수 있음
* 이 새로운 표현은 원본 데이터의 본질인 내재된 구조를 포착하는 한편, 내재된 구조 외 부가적인 노이즈를 데이터는 제거함

## 차원의 저주

* 피처가 많을수록 훈련은 더 어려워짐, 어떤 경우에는 최적해를 빨리 찾는 것이 불가능할 수 있음
* 차원 감소를 통해 원래의 피처 집합에서 가장 핵심적인 피처를 찾고, 중요한 정보를 보존하면서 차원 수를 적용 가능한 개수로 줄인 후 지도 학습 알고리즘을 적용해 효율적으로 최적의 함수 근사를 찾을 수 있음

## 피처 엔지니어링

* 적절한 피처가 없으면 머신러닝 알고리즘은 새로운 데이터에서 좋은 결정을 내릴 수 있을 만큼 공간에서 포인트를 분리할 수 없음
* 비지도 학습 알고리즘의 표현 학습을 사용해 적절한 유형의 피처 표현을 자동 학습하는 것으로 문제를 해결할 수 있음

## 이상치

* 머선리닝 알고리즘이 희귀하고 왜곡된 이상치를 학습하는 경우 이상치를 별개로 두거나 무시하는 경우보다 일반화 오차가 더 작다
* 비지도 학습을 통해 차원 감소를 사용해 이상치 탐지를 수행하고, 이상치를 위한 솔루션과 및 정상 데이터를 위한 솔루션을 별도로 생성할 수 있습니다

## 데이터 드리프트
    
* 데이터 드리프트 : 시간이 지남에 따라 모델의 성능 저하를 발생시키는 입력 데이터의 변경 내용
* 비지도 학습을 사용해 확률 분포를 구축함으로써 현재 데이터가 훈련 셋과 얼마나 다른지 평가할 수 있음

---

# 비지도 학습 알고리즘 자세히 살펴보기

## 차원 축소

* 원래의 고차원 입력 데이터를 저차원 공간에 투영해 관련성이 없는 피처를 필터링하고, 관심 있는 피처를 가능한 많이 유지
* 차원 축소를 통해 비지도 학습 AI가 패턴을 효과적으로 식별하고, 컴퓨팅 비용이 많이 드는 대용량 문제를 효율적으로 해결

### 1. 선형 투영

* #### PCA (주성분 분석)

  - 데이터의 내재된 구조를 학습하는 한 가지 방법은 데이터의 인스턴스 간 변동성을 설명할 때 전체 피처 중 어떤 피처가 가장 중요한지 식별하는 것
  - 최대한 다양성을 유지하면서 데이터의 저차원 표현을 찾음
  - PCA 수행 후 남은 차원의 수는 전체 데이터 셋의 차원 수 보다 상당히 적음
  - 저차원 공간으로 이동함으로써 분산 중 일부는 손실되지만 데이터의 내재된 구조를 식별하기가 더 쉽기 때문에 클러스터링 같은 작업을 효율적으로 수행할 수 있음
  - PCA의 여러 가지 변형 방법
    * Incremental PCA : 미니 배치 변형
    * Kernel PCA : 비성형 변형
    * Sparse PCA : 희소 변형

* #### SVD (특잇값 분해)
  
  - 데이터의 내재된 구조를 학습하는 또 다른 방법은 원래 행렬의 차원을 작은 차원으로 줄여 더 작은 차원의 행렬에서 일부 벡터의 성형 결합을 사용해 원래 행렬을 다시 만드는 다시 만들 수 있도록 하는 것
  - 더 작은 차원의 행렬을 생성하기 위해 가장 많은 정보를 가진 원본 행렬의 백터 (가장 높은 특이값)를 유지함
  - 더 작은 차원의 행렬은 원래 피처 공간의 가장 중요한 요소를 포착함

* #### 랜던 투영

  - 점 사이 거리의 배율이 유지되도록 고차원 공간에서 훨씬 낮은 차원의 공간으로 점을 투영하는 것을 포함
  - 이를 위해 랜덤 가우시안 행렬 또는 랜덤 희소 행렬 중 하나를 사용할 수 있음

### 매니폴드 학습

 * 선형 투영보다는 매니폴드 학습 또는 비성형 차원 축소와 같이 데이터의 비성형 변환을 수행하는 것이 좋음

* #### isomap
  
  - 유클리드 거리가 아니라 각 점과 인접한 점 사이의 지오데식 또는 곡선 거리를 추정해 데이터 매니폴드의 본질적인 지오메트리를 학습
  - isomap은 이를 사용해 원래의 고차원 공간을 저차원 공간에 투영함

* #### t - SNE (t - 분포 확률적 임베딩)
  
  - 고차원 데이터를 2 ~ 3 차원 공간에 투영해 변환된 데이터를 시각화할 수 있도록 함
  - 이 2차원 또는 3차원 공간에서 비슷한 인스턴스들을 더 가까이, 비슷하지 않은 인스턴스들은 멀리 떨어지도록 모델링함

* #### 사전 학습

  - 데이터에 내저된 희소 표현을 학습하는 방법
  - 이러한 표현 요소는 단순 이진 벡터 (0과 1)이며, 데이터 셋의 각 인스턴스는 표현 요소의 가중 합계로 재구성될 수 있음
  - 이 비지도 학습이 생성하는 행렬 (dictionary : 사전)은 대부분 0 아닌 몇 개의 가중치를 가진 0으로 채움
  - 이러한 사전을 생성함으로써 이 알고리즘은 원래 피처 공간의 가장 핵심적인 표현 요소를 효율적으로 식별할 수 있음
    (이들은 대부분 0이 아닌 가중치를 가진 요소이다)
  - 덜 중요한 표현 요소는 가중치가 대부분 0이다

### ICA (독립 성분 분석)
  * 레이블이 없는 데이터의 공통 문제 중 하나는 우리가 제공한 피처에 함께 포함된 수많은 독립적인 신호가 있다는 것이다
  * ICA를 사용해 이러한 혼합 신호를 개별 성분으로 분리할 수 있다
  * 분리가 완료되면 생성된 개별 성분의 일부 조합을 추가해 원래 피처를 재구성할 수 있다
  * ICA는 일반적으로 신호 처리 작업에 사용됨

### LDA (잠재 디리클레 할당)

  * 비지도 학습은 데이터 셋의 일부가 서로 유사한 이유를 학습해 데이터 셋을 설명할 수 있음
  * 이를 위해서는 데이터 셋에서 관측되지 않은 요소를 학습해야 함


## 클러스터링

* 원래의 피처 집합을 더 작고 관리하기 쉬운 집합으로 줄인 후 유사한 데이터 인스턴스를 그룹화해 흥미로운 패턴을 찾을 수 있음

### K - Means Clustering

* 클러스터링을 잘하기 위해서는 그룹 내 인스턴스는 서로 유사하지만 다른 그룹의 인스턴스와는 차이가 나도록 별개의 그룹을 식별해야 함
* 우리가 원하는 클러스터 k의 수를 지정하면 각 인스턴스를 k개의 클러스터 중 정확히 하나에 할당합니다
* 또한, 군집 내 분산을 초소화, 즉 모든 k개 군집의 군집 내 분산의 합이 가능한 한 작게 그룹화를 최적화합니다
* K - 평균은 이러한 클러스터링 수행 속도를 높이기 위해 각 관측치를 k 개의 군집 중 하나에 무작위로 할당한 다음 각 관측치와 해당 군집의 중심점 또는 중심 사이의 유클리드 거리를 최소화하기 위해 이러한 관측치를 재지정하기 시작합니다

### Hierarhical Clustering

* 군집 수를 사전에 지정하지 않고 클러스터링하는 방식
* 계층적 클러스터링의 한 버전인 Agglomerative Clustering
  - 트리 기반 클러스터링 방법을 사용해 Dendrogram을 생성함
  - Dendrogram은 거꾸로 된 트리 형태의 그래픽으로 표현됨
  - 맨 아래에 있는 잎은 데이터 셋의 개별 인스턴스를 의미
  - 그런 다음 계층적 클러스터링은 거꾸로 된 트리를 수직으로 위로 이동시키면서 서로 얼마나 유사한지에 따라 잎을 함께 결합시킴
  - 서로 유사한 인스턴스일수록 더 빨리 결합하고 유사히지 않은 인스턴스일수록 나종에 결합됨
  - 이러한 과정을 반복해 모든 인스턴스는 결국 트리의 단일 줄기를 형성하며 서로 연결됨
* 계층적 군집 알고리즘이 실행을 마치면 Dendrogram을 보고 트리를 잘라낼 위치를 결정할 수 있음
* 트리를 덜 잘라낼수록 개별 분기가 많이 남음 (더 많은 군집이 남음)
* 더 적은 수의 군집을 원하면 Dendrogram 상에서 더 높은 지점을 자르면 됨

### DBSCAN (노이즈 응용 밀도 기반 공간 클러스터링)

* 데이터 포인트의 밀도 기반으로 하는 더욱 강력한 클러스터링 알고리즘
* 모든 인스턴스가 주어지면 DBSCAN은 밀접하게 묶인 인스턴스를 그룹화함
* 여기서 밀접한 관계는 특정 거리 내에 존재해야 하는 최소 인스턴스 수로 정의됨
* 필요한 최소 인스턴스 수와 거리를 모두 지정해야 함
* 인스턴스가 여러 군집과 지정된 거리 내에 있으면 해당 인스턴스와 가장 밀집된 위치에 있는 군집과 그룹화됨
* 어떤 군집과도 지정된 거리 외에 있는 인스턴스는 이상치로 레이블링 됨
* K-평균과 달리, DBSCAN은 군집의 개수를 미리 지정할 필요가 없음
* 임의로 형성된 군집을 만들 수 있으며, 일반적으로 데이터의 이상치 때문에 발생하는 왜곡이 훨씬 적음

## 피처 추출

* 비지도 학습을 사용해 데이터의 원본 피처에 대한 새로운 표현을 배울 수 있다
* 피처 추출을 사용하면 원본 피처 수를 더 작은 하위 집합으로 줄여 차원 감소를 효과적으로 수행할 수 있음
* 새로운 피처를 생성해 지도 학습 문제의 성능을 향상시키는데도 도움을 줄 수 있음

### 오토인코더

* 새로운 피처를 생성하기 위해 피드 포워드 신경망, 비순환 신경망을 사용해 표현 학습을 수행할 수 있음
* 여기서 출력층의 노드 수는 입력층의 노드 수와 일치함
* 원본 피처를 효과적으로 재구성하고 은닉층을 사용해 새로운 표현을 학습함
* 오토인코더의 각 은닉층은 원본 피처의 표현을 학습하고 후속 층은 이전 층에서 학습한 표현을 기반으로 구축됨
* 오토인코더는 각 층별 단계적으로 단순한 표현에서 점점 더 복잡한 표현을 학습함

### 피드 포워드 신경망의 지도 훈련을 사용한 피처 추출

* 레이블이 있는 경우 대체할 수 있는 피처 추출 방법은 출력층이 올바른 레이블을 에측하도록 시도하는 피드 포워드 신경망이나 비순환 신경망을 사용하는 것
* 오토인코더와 마찬가지로 각 은닉층은 원본 피처의 표현을 학습함
* 그러나 새로운 피처를 생성할 때 이 네트워크는 레이블에 의해 명시적으로 안내됨
* 이 네트워크에서 새로 학습한 원본 피처의 최종 표현을 추출하기 위해 끝에서 두 번째 층을 추출함
* 이 끝에서 두번쨰 층을 지도 학습 모델의 입력으로 사용할 수 있음

## 비지도 딥러닝

* 심층 신경망 훈련에서 가중치 업데이트 시 발생하는 문제
  - 그레이디언트 소실 문제
  - 그레이디언트 폭주 문제

### 비지도 사전 훈련

* 매우 깊고 층이 여러 개인 신경망을 훈련시키는 데 발생하는 이러한 어려움을 해결하기 위해 각 단계가 얕은 신경망을 포함하는 여러 개의 연속적인 단계로 신경망을 학습시킴
* 하나의 얕은 네트워크의 출력은 다음 신경망의 입력으로 사용됨
* 일반적으로 이 파이프라인의 첫 번째 얕은 신경망은 비지도 신경망을 포함하지만 이후 신경망은 지도 학습을 수행함
* 이 비지도 부분을 탐욕적 계층별 비지도 사전 훈련으로 알려져 있음
* 비지도 사전 훈련을 사용하면 AI가 원본 입력 데이터의 향상된 표현을 포착할 수 있음
* 비지도 사전 훈련은 지도 학습 문제를 쉽게 해결할 수 있을 뿐만 아니라 전이 학습을 용이하게 함

* #### RBM (제한된 볼츠만 머신)
  - 비지도 사전 훈련의 한 예로 얕은 2개의 층으로 구성된 신경망
  - 첫 번째 층은 입력층이고 두 번째 층은 은닉층ㅇ다
  - 각 노드는 다른 층의 노드에 연결되지만 동일한 층의 노드들은 서로 연결되지 않음
  - RBM은 차원 감소 및 피처 추출과 같은 비지도 작업을 수행할 수 있으며 지도 학습 솔루션의 일부로 유용한 비지도 사전 훈련을 제공함

* #### DBN (심층 신뢰 신경망)

  - RBM은 DBN으로 알려진 다단계 신경망 파이프 라인을 형성하기 위해 함께 연결할 수 있음
  - 각 RBM의 은닉층은 다음 RBM의 입력으로 사용됨
  - 즉, 각 RBM은 다음 RBM이 사용하는 데이터 표현을 생성함
  - 이러한 유형의 표현 학습을 연속적으로 연결함으로써 DBN은 종종 피처 추출기로 사용되는 더 복잡한 표현을 배울 수 있음

* #### GAN (생성적 적대 신경망)
  - 


