# 비지도 학습 개요

#### 비지도 학습

* 비지도 학습에서는 레이블을 사용할 수 없음
* AI 에이전트의 작업이 명확히 정의되지 않으므로 모델의 성능을 명확히 측정할 수 없다
* 데이터에서 새로운 패턴을 찾는다는 관점에서 이러한 비지도 학습 기반 시스템은 지도 학습 기반 시스템보다 더 우수하다

---

# 비지도 학습의 강점과 약점

#### 강점

* 패턴을 알 수 없거나 끊임없이 변화하거나 레이블 데이터가 충분히 확보되지 않은 문제 영역에서는 비지도 학습이 우월
  - 데이터의 레이블이 아닌 데이터 자체의 내재된 구조를 학습해 작동
  - 데이터 셋이 보유한 데이터 레코드 수보다 훨씬 작은 수의 매개변수 집합으로 훈련해 데이터를 표현하려는 시도를 함
  - 비지도 학습은 이러한 표현 학습을 수행합으로써 데이터 셋의 고유 패턴을 식별

#### 약점

* 비지도 학습 AI 자체가 스스로 '의자' 또는 '개'로 표시할 수 없음
  - 하지만 비슷한 이미지가 함께 그룹화됐기 때문에 이후 우리는 훨씬 더 간단히 레이블을 지정할 수 있음
  - 비지도 학습 AI가 레이블이 지정된 어떤 그룹에도 속하지 않는 이미지를 찾는 경우, 
    AI는 분류되지 않은 이미지를 레이블이 지정되지 않은 별도의 새 이미지 그룹으로 만들어 사람이 레이블을 직접 지정하도록 유도
    
---
 
# 비지도 학습을 사용해 머신러닝 솔루션 개선하기

## 레이블 데이터는 언제나 부족하다

* 모든 데이터가 평등하게 생성되는 것은 아니다
* 비지도 학습을 통해 레이블이 지정되지 않은 데이터에 자동으로 레이블을 지정할 수 있다

## 과대 적합

* 훈련 데이터에서 지나치게 복잡한 함수를 학습하면 홀드아웃 데이터 셋에 존재하는 새로운 인스턴스에서 제대로 수행되지 않을 수 있음
* 비지도 학습을 정규화기로 사용하면 이러한 문제를 해결할 수 있음
* 원본 입력 데이터를 지도 학습 알고리즘에 직접 공급하는 대신 비지도 사전 훈련을 통해 생성된 원본 입력 데이터의 새로운 형태 표현을 제공할 수 있음
* 이 새로운 표현은 원본 데이터의 본질인 내재된 구조를 포착하는 한편, 내재된 구조 외 부가적인 노이즈를 데이터는 제거함

## 차원의 저주

* 피처가 많을수록 훈련은 더 어려워짐, 어떤 경우에는 최적해를 빨리 찾는 것이 불가능할 수 있음
* 차원 감소를 통해 원래의 피처 집합에서 가장 핵심적인 피처를 찾고, 중요한 정보를 보존하면서 차원 수를 적용 가능한 개수로 줄인 후 지도 학습 알고리즘을 적용해 효율적으로 최적의 함수 근사를 찾을 수 있음

## 피처 엔지니어링

* 적절한 피처가 없으면 머신러닝 알고리즘은 새로운 데이터에서 좋은 결정을 내릴 수 있을 만큼 공간에서 포인트를 분리할 수 없음
* 비지도 학습 알고리즘의 표현 학습을 사용해 적절한 유형의 피처 표현을 자동 학습하는 것으로 문제를 해결할 수 있음

## 이상치

* 머선리닝 알고리즘이 희귀하고 왜곡된 이상치를 학습하는 경우 이상치를 별개로 두거나 무시하는 경우보다 일반화 오차가 더 작다
* 비지도 학습을 통해 차원 감소를 사용해 이상치 탐지를 수행하고, 이상치를 위한 솔루션과 및 정상 데이터를 위한 솔루션을 별도로 생성할 수 있습니다

## 데이터 드리프트
    
* 데이터 드리프트 : 시간이 지남에 따라 모델의 성능 저하를 발생시키는 입력 데이터의 변경 내용
* 비지도 학습을 사용해 확률 분포를 구축함으로써 현재 데이터가 훈련 셋과 얼마나 다른지 평가할 수 있음

---

# 비지도 학습 알고리즘 자세히 살펴보기

## 차원 축소

* 원래의 고차원 입력 데이터를 저차원 공간에 투영해 관련성이 없는 피처를 필터링하고, 관심 있는 피처를 가능한 많이 유지
* 차원 축소를 통해 비지도 학습 AI가 패턴을 효과적으로 식별하고, 컴퓨팅 비용이 많이 드는 대용량 문제를 효율적으로 해결

### 1. 선형 투영

* #### PCA (주성분 분석)

  - 데이터의 내재된 구조를 학습하는 한 가지 방법은 데이터의 인스턴스 간 변동성을 설명할 때 전체 피처 중 어떤 피처가 가장 중요한지 식별하는 것
  - 최대한 다양성을 유지하면서 데이터의 저차원 표현을 찾음
  - PCA 수행 후 남은 차원의 수는 전체 데이터 셋의 차원 수 보다 상당히 적음
  - 저차원 공간으로 이동함으로써 분산 중 일부는 손실되지만 데이터의 내재된 구조를 식별하기가 더 쉽기 때문에 클러스터링 같은 작업을 효율적으로 수행할 수 있음
  - PCA의 여러 가지 변형 방법
    * Incremental PCA : 미니 배치 변형
    * Kernel PCA : 비성형 변형
    * Sparse PCA : 희소 변형

* #### SVD (특잇값 분해)
  
  - 데이터의 내재된 구조를 학습하는 또 다른 방법은 원래 행렬의 차원을 작은 차원으로 줄여 더 작은 차원의 행렬에서 일부 벡터의 성형 결합을 사용해 원래 행렬을 다시 만드는 다시 만들 수 있도록 하는 것
  - 더 작은 차원의 행렬을 생성하기 위해 가장 많은 정보를 가진 원본 행렬의 백터 (가장 높은 특이값)를 유지함
  - 더 작은 차원의 행렬은 원래 피처 공간의 가장 중요한 요소를 포착함

* #### 랜던 투영

  - 점 사이 거리의 배율이 유지되도록 고차원 공간에서 훨씬 낮은 차원의 공간으로 점을 투영하는 것을 포함
  - 이를 위해 랜덤 가우시안 행렬 또는 랜덤 희소 행렬 중 하나를 사용할 수 있음

## 매니폴드 학습

 * 선형 투영보다는 매니폴드 학습 또는 비성형 차원 축소와 같이 데이터의 비성형 변환을 수행하는 것이 좋음

* #### isomap
  
  - 유클리드 
